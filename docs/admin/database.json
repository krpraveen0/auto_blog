{
  "version": "1.0",
  "exported_at": "2026-01-23T19:59:40.935640+00:00",
  "stats": {
    "total_papers": 55,
    "total_content": 5,
    "content_by_type": {
      "blog": 2,
      "linkedin": 3
    },
    "content_by_status": {
      "drafted": 4,
      "published": 1
    },
    "papers_by_source": {
      "arxiv": 54,
      "github": 1
    },
    "top_languages": {
      "Python": 1
    },
    "last_updated": "2026-01-23T19:59:40.931853+00:00"
  },
  "papers": [
    {
      "id": "github_test_12345",
      "title": "test-user/awesome-ml-project",
      "url": "https://github.com/test-user/awesome-ml-project",
      "pdf_url": null,
      "summary": "An amazing machine learning project that makes AI accessible to everyone",
      "authors": [],
      "published": "2025-12-01T00:00:00Z",
      "updated": "2026-01-18T00:00:00Z",
      "category": null,
      "categories": [],
      "primary_category": null,
      "source": "github",
      "source_priority": "medium",
      "score": 0.0,
      "fetched_at": "2026-01-19T16:00:00Z",
      "created_at": "2026-01-19 16:41:46",
      "stars": 5432,
      "forks": 876,
      "watchers": 234,
      "open_issues": 45,
      "language": "Python",
      "topics": [
        "machine-learning",
        "deep-learning",
        "neural-networks"
      ],
      "license": "MIT",
      "languages": {
        "Python": 85000,
        "JavaScript": 12000,
        "HTML": 3000
      },
      "contributors_count": 42,
      "owner_type": "Organization",
      "stars_per_day": 110.24,
      "forks_per_day": 17.76,
      "activity_score": 1191.2,
      "days_since_creation": 49,
      "days_since_update": 1,
      "is_recently_active": 1,
      "has_wiki": 1,
      "has_pages": 1,
      "has_discussions": 0
    },
    {
      "id": "2601.10712v1",
      "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
      "url": "http://arxiv.org/abs/2601.10712v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10712v1",
      "summary": "Tool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by interleaving reasoning steps with external tool interactions. However, existing reinforcement learning methods typically rely on outcome- or trajectory-level rewards, assigning uniform advantages to all steps within a trajectory. This coarse-grained credit assignment fails to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios. To address this, we propose MatchTIR, a framework that introduces fine-grained supervision via bipartite matching-based turn-level reward assignment and dual-level advantage estimation. Specifically, we formulate credit assignment as a bipartite matching problem between predicted and ground-truth traces, utilizing two assignment strategies to derive dense turn-level rewards. Furthermore, to balance local step precision with global task success, we introduce a dual-level advantage estimation scheme that integrates turn-level and trajectory-level signals, assigning distinct advantage values to individual interaction turns. Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model surpasses the majority of 8B competitors, particularly in long-horizon and multi-turn tasks. Our codes are available at https://github.com/quchangle1/MatchTIR.",
      "authors": [
        "Changle Qu",
        "Sunhao Dai",
        "Hengyi Cai",
        "Jun Xu",
        "Shuaiqiang Wang",
        "Dawei Yin"
      ],
      "published": "2026-01-15T18:59:23+00:00",
      "updated": "2026-01-15T18:59:23+00:00",
      "category": "cs.AI",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "source": "arxiv",
      "source_priority": "high",
      "score": 9.0,
      "fetched_at": "2026-01-18T15:37:08.020359",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10681v1",
      "title": "Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems",
      "url": "http://arxiv.org/abs/2601.10681v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10681v1",
      "summary": "Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.",
      "authors": [
        "Amir Khurshid",
        "Abhishek Sehgal"
      ],
      "published": "2026-01-15T18:43:19+00:00",
      "updated": "2026-01-15T18:43:19+00:00",
      "category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "source": "arxiv",
      "source_priority": "high",
      "score": 9.0,
      "fetched_at": "2026-01-18T15:37:08.020489",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10581v1",
      "title": "From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA",
      "url": "http://arxiv.org/abs/2601.10581v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10581v1",
      "summary": "Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.",
      "authors": [
        "Kimia Abedini",
        "Farzad Shami",
        "Gianmaria Silvello"
      ],
      "published": "2026-01-15T16:54:11+00:00",
      "updated": "2026-01-15T16:54:11+00:00",
      "category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "source": "arxiv",
      "source_priority": "high",
      "score": 9.0,
      "fetched_at": "2026-01-18T15:37:08.020647",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10567v1",
      "title": "Generative AI collective behavior needs an interactionist paradigm",
      "url": "http://arxiv.org/abs/2601.10567v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10567v1",
      "summary": "In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.",
      "authors": [
        "Laura Ferrarotti",
        "Gian Maria Campedelli",
        "Roberto Dess\u00ec",
        "Andrea Baronchelli",
        "Giovanni Iacca",
        "Kathleen M. Carley",
        "Alex Pentland",
        "Joel Z. Leibo",
        "James Evans",
        "Bruno Lepri"
      ],
      "published": "2026-01-15T16:29:23+00:00",
      "updated": "2026-01-15T16:29:23+00:00",
      "category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "source": "arxiv",
      "source_priority": "high",
      "score": 9.0,
      "fetched_at": "2026-01-18T15:37:08.020678",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10543v1",
      "title": "Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing",
      "url": "http://arxiv.org/abs/2601.10543v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10543v1",
      "summary": "Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: https://github.com/zyz13590/SafeProbing.",
      "authors": [
        "Yinzhi Zhao",
        "Ming Wang",
        "Shi Feng",
        "Xiaocui Yang",
        "Daling Wang",
        "Yifei Zhang"
      ],
      "published": "2026-01-15T16:09:10+00:00",
      "updated": "2026-01-15T16:09:10+00:00",
      "category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "source": "arxiv",
      "source_priority": "high",
      "score": 9.0,
      "fetched_at": "2026-01-18T15:37:08.020738",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10527v1",
      "title": "A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5",
      "url": "http://arxiv.org/abs/2601.10527v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10527v1",
      "summary": "The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.",
      "authors": [
        "Xingjun Ma",
        "Yixu Wang",
        "Hengyuan Xu",
        "Yutao Wu",
        "Yifan Ding",
        "Yunhan Zhao",
        "Zilong Wang",
        "Jiabin Hua",
        "Ming Wen",
        "Jianan Liu",
        "Ranjie Duan",
        "Yifeng Gao",
        "Yingshui Tan",
        "Yunhao Chen",
        "Hui Xue",
        "Xin Wang",
        "Wei Cheng",
        "Jingjing Chen",
        "Zuxuan Wu",
        "Bo Li",
        "Yu-Gang Jiang"
      ],
      "published": "2026-01-15T15:52:52+00:00",
      "updated": "2026-01-15T15:52:52+00:00",
      "category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "source": "arxiv",
      "source_priority": "high",
      "score": 9.0,
      "fetched_at": "2026-01-18T15:37:08.020769",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10524v1",
      "title": "Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection",
      "url": "http://arxiv.org/abs/2601.10524v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10524v1",
      "summary": "The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover the root causes of their generalization failures. Our investigation reveals three critical findings: (1) Generalization is driven by a powerful synergy between architecture and data diversity. The Gemma 2 9B model achieves state-of-the-art performance (>91\\% F1), but only when trained on a stylistically diverse ``generalist'' dataset. (2) Generalization is highly architecture-dependent. We diagnose a specific failure mode in Llama 3.1 8B, which performs well on a narrow domain but cannot integrate diverse data, leading to a significant performance drop. (3) Some architectures are inherently more generalizable. The Mistral model proves to be a consistent and resilient performer across multiple training paradigms. By pinpointing the flawed heuristics responsible for these failures, our work provides a concrete methodology for diagnosing and understanding generalization failures, underscoring that reliable AI requires deep validation of the interplay between architecture, data, and training strategy.",
      "authors": [
        "Frank Bobe",
        "Gregory D. Vetaw",
        "Chase Pavlick",
        "Darshan Bryner",
        "Matthew Cook",
        "Jose Salas-Vernis"
      ],
      "published": "2026-01-15T15:51:24+00:00",
      "updated": "2026-01-15T15:51:24+00:00",
      "category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "source": "arxiv",
      "source_priority": "high",
      "score": 9.0,
      "fetched_at": "2026-01-18T15:37:08.020790",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10702v1",
      "title": "Grounding Agent Memory in Contextual Intent",
      "url": "http://arxiv.org/abs/2601.10702v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10702v1",
      "summary": "Deploying large language models in long-horizon, goal-oriented interactions remains challenging because similar entities and facts recur under different latent goals and constraints, causing memory systems to retrieve context-mismatched evidence. We propose STITCH (Structured Intent Tracking in Contextual History), an agentic memory system that indexes each trajectory step with a structured retrieval cue, contextual intent, and retrieves history by matching the current step's intent. Contextual intent provides compact signals that disambiguate repeated mentions and reduce interference: (1) the current latent goal defining a thematic segment, (2) the action type, and (3) the salient entity types anchoring which attributes matter. During inference, STITCH filters and prioritizes memory snippets by intent compatibility, suppressing semantically similar but context-incompatible history.\n  For evaluation, we introduce CAME-Bench, a benchmark for context-aware retrieval in realistic, dynamic, goal-oriented trajectories. Across CAME-Bench and LongMemEval, STITCH achieves state-of-the-art performance, outperforming the strongest baseline by 35.6%, with the largest gains as trajectory length increases. Our analysis shows that intent indexing substantially reduces retrieval noise, supporting intent-aware memory for robust long-horizon reasoning.",
      "authors": [
        "Ruozhen Yang",
        "Yucheng Jiang",
        "Yueqi Jiang",
        "Priyanka Kargupta",
        "Yunyi Zhang",
        "Jiawei Han"
      ],
      "published": "2026-01-15T18:55:13+00:00",
      "updated": "2026-01-15T18:55:13+00:00",
      "category": "cs.AI",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "source": "arxiv",
      "source_priority": "high",
      "score": 8.6,
      "fetched_at": "2026-01-18T15:37:08.020399",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10700v1",
      "title": "LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals",
      "url": "http://arxiv.org/abs/2601.10700v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10700v1",
      "summary": "Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods.",
      "authors": [
        "Gilat Toker",
        "Nitay Calderon",
        "Ohad Amosy",
        "Roi Reichart"
      ],
      "published": "2026-01-15T18:54:50+00:00",
      "updated": "2026-01-15T18:54:50+00:00",
      "category": "cs.AI",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "source": "arxiv",
      "source_priority": "high",
      "score": 8.6,
      "fetched_at": "2026-01-18T15:37:08.020423",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10684v1",
      "title": "On the origin of neural scaling laws: from random graphs to natural language",
      "url": "http://arxiv.org/abs/2601.10684v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10684v1",
      "summary": "Scaling laws have played a major role in the modern AI revolution, providing practitioners predictive power over how the model performance will improve with increasing data, compute, and number of model parameters. This has spurred an intense interest in the origin of neural scaling laws, with a common suggestion being that they arise from power law structure already present in the data. In this paper we study scaling laws for transformers trained to predict random walks (bigrams) on graphs with tunable complexity. We demonstrate that this simplified setting already gives rise to neural scaling laws even in the absence of power law structure in the data correlations. We further consider dialing down the complexity of natural language systematically, by training on sequences sampled from increasingly simplified generative language models, from 4,2,1-layer transformer language models down to language bigrams, revealing a monotonic evolution of the scaling exponents. Our results also include scaling laws obtained from training on random walks on random graphs drawn from Erd\u00f6s-Renyi and scale-free Barab\u00e1si-Albert ensembles. Finally, we revisit conventional scaling laws for language modeling, demonstrating that several essential results can be reproduced using 2 layer transformers with context length of 50, provide a critical analysis of various fits used in prior literature, demonstrate an alternative method for obtaining compute optimal curves as compared with current practice in published literature, and provide preliminary evidence that maximal update parameterization may be more parameter efficient than standard parameterization.",
      "authors": [
        "Maissam Barkeshli",
        "Alberto Alfarano",
        "Andrey Gromov"
      ],
      "published": "2026-01-15T18:46:09+00:00",
      "updated": "2026-01-15T18:46:09+00:00",
      "category": "cs.AI",
      "categories": [
        "cs.LG",
        "cond-mat.dis-nn",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "source": "arxiv",
      "source_priority": "high",
      "score": 8.6,
      "fetched_at": "2026-01-18T15:37:08.020468",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10679v1",
      "title": "Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models",
      "url": "http://arxiv.org/abs/2601.10679v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10679v1",
      "summary": "Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) \"Grokking\" dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM \"guesses\" the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be \"guessing\" instead of \"reasoning\". Leveraging this \"guessing\" picture, we propose three strategies to scale HRM's guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models \"reason\".",
      "authors": [
        "Zirui Ren",
        "Ziming Liu"
      ],
      "published": "2026-01-15T18:42:50+00:00",
      "updated": "2026-01-15T18:42:50+00:00",
      "category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "source": "arxiv",
      "source_priority": "high",
      "score": 8.6,
      "fetched_at": "2026-01-18T15:37:08.020510",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10587v1",
      "title": "Adversarial Evasion Attacks on Computer Vision using SHAP Values",
      "url": "http://arxiv.org/abs/2601.10587v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10587v1",
      "summary": "The paper introduces a white-box attack on computer vision models using SHAP values. It demonstrates how adversarial evasion attacks can compromise the performance of deep learning models by reducing output confidence or inducing misclassifications. Such attacks are particularly insidious as they can deceive the perception of an algorithm while eluding human perception due to their imperceptibility to the human eye. The proposed attack leverages SHAP values to quantify the significance of individual inputs to the output at the inference stage. A comparison is drawn between the SHAP attack and the well-known Fast Gradient Sign Method. We find evidence that SHAP attacks are more robust in generating misclassifications particularly in gradient hiding scenarios.",
      "authors": [
        "Frank Mollard",
        "Marcus Becker",
        "Florian Roehrbein"
      ],
      "published": "2026-01-15T16:58:55+00:00",
      "updated": "2026-01-15T16:58:55+00:00",
      "category": "cs.AI",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "source": "arxiv",
      "source_priority": "high",
      "score": 8.6,
      "fetched_at": "2026-01-18T15:37:08.020625",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10520v1",
      "title": "Breaking Up with Normatively Monolithic Agency with GRACE: A Reason-Based Neuro-Symbolic Architecture for Safe and Ethical AI Alignment",
      "url": "http://arxiv.org/abs/2601.10520v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10520v1",
      "summary": "As AI agents become increasingly autonomous, widely deployed in consequential contexts, and efficacious in bringing about real-world impacts, ensuring that their decisions are not only instrumentally effective but also normatively aligned has become critical. We introduce a neuro-symbolic reason-based containment architecture, Governor for Reason-Aligned ContainmEnt (GRACE), that decouples normative reasoning from instrumental decision-making and can contain AI agents of virtually any design. GRACE restructures decision-making into three modules: a Moral Module (MM) that determines permissible macro actions via deontic logic-based reasoning; a Decision-Making Module (DMM) that encapsulates the target agent while selecting instrumentally optimal primitive actions in accordance with derived macro actions; and a Guard that monitors and enforces moral compliance. The MM uses a reason-based formalism providing a semantic foundation for deontic logic, enabling interpretability, contestability, and justifiability. Its symbolic representation enriches the DMM's informational context and supports formal verification and statistical guarantees of alignment enforced by the Guard. We demonstrate GRACE on an example of a LLM therapy assistant, showing how it enables stakeholders to understand, contest, and refine agent behavior.",
      "authors": [
        "Felix Jahn",
        "Yannic Muskalla",
        "Lisa Dargasz",
        "Patrick Schramowski",
        "Kevin Baum"
      ],
      "published": "2026-01-15T15:47:38+00:00",
      "updated": "2026-01-15T15:47:38+00:00",
      "category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "source": "arxiv",
      "source_priority": "high",
      "score": 8.6,
      "fetched_at": "2026-01-18T15:37:08.020814",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10673v1",
      "title": "Single-Stage Huffman Encoder for ML Compression",
      "url": "http://arxiv.org/abs/2601.10673v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10673v1",
      "summary": "Training and serving Large Language Models (LLMs) require partitioning data across multiple accelerators, where collective operations are frequently bottlenecked by network bandwidth. Lossless compression using Huffman codes is an effective way to alleviate the issue, however, its three-stage design requiring on-the-fly frequency analysis, codebook generation and transmission of codebook along with data introduces computational, latency and data overheads which are prohibitive for latency-sensitive scenarios such as die-to-die communication. This paper proposes a single-stage Huffman encoder that eliminates these overheads by using fixed codebooks derived from the average probability distribution of previous data batches. Through our analysis of the Gemma 2B model, we demonstrate that tensors exhibit high statistical similarity across layers and shards. Using this approach we achieve compression within 0.5% of per-shard Huffman coding and within 1% of the ideal Shannon compressibility, enabling efficient on-the-fly compression.",
      "authors": [
        "Aditya Agrawal",
        "Albert Magyar",
        "Hiteshwar Eswaraiah",
        "Patrick Sheridan",
        "Pradeep Janedula",
        "Ravi Krishnan Venkatesan",
        "Krishna Nair",
        "Ravi Iyer"
      ],
      "published": "2026-01-15T18:37:56+00:00",
      "updated": "2026-01-15T18:37:56+00:00",
      "category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "source": "arxiv",
      "source_priority": "high",
      "score": 8.6,
      "fetched_at": "2026-01-18T15:37:11.929793",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10657v1",
      "title": "PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution",
      "url": "http://arxiv.org/abs/2601.10657v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10657v1",
      "summary": "Large Language Models (LLMs) have emerged as powerful operators for evolutionary search, yet the design of efficient search scaffolds remains ad hoc. While promising, current LLM-in-the-loop systems lack a systematic approach to managing the evolutionary process. We identify three distinct failure modes: Context Pollution, where experiment history biases future candidate generation; Mode Collapse, where agents stagnate in local minima due to poor exploration-exploitation balance; and Weak Collaboration, where rigid crossover strategies fail to leverage parallel search trajectories effectively. We introduce Progress-Aware Consistent Evolution (PACEvolve), a framework designed to robustly govern the agent's context and search dynamics, to address these challenges. PACEvolve combines hierarchical context management (HCM) with pruning to address context pollution; momentum-based backtracking (MBB) to escape local minima; and a self-adaptive sampling policy that unifies backtracking and crossover for dynamic search coordination (CE), allowing agents to balance internal refinement with cross-trajectory collaboration. We demonstrate that PACEvolve provides a systematic path to consistent, long-horizon self-improvement, achieving state-of-the-art results on LLM-SR and KernelBench, while discovering solutions surpassing the record on Modded NanoGPT.",
      "authors": [
        "Minghao Yan",
        "Bo Peng",
        "Benjamin Coleman",
        "Ziqi Chen",
        "Zhouhang Xie",
        "Zhankui He",
        "Noveen Sachdeva",
        "Isabella Ye",
        "Weili Wang",
        "Chi Wang",
        "Ed H. Chi",
        "Wang-Cheng Kang",
        "Derek Zhiyuan Cheng",
        "Beidou Wang"
      ],
      "published": "2026-01-15T18:25:23+00:00",
      "updated": "2026-01-15T18:25:23+00:00",
      "category": "cs.LG",
      "categories": [
        "cs.NE",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "source": "arxiv",
      "source_priority": "high",
      "score": 8.6,
      "fetched_at": "2026-01-18T15:37:11.929822",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10639v1",
      "title": "STEM: Scaling Transformers with Embedding Modules",
      "url": "http://arxiv.org/abs/2601.10639v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10639v1",
      "summary": "Fine-grained sparsity promises higher parametric capacity without proportional per-token compute, but often suffers from training instability, load balancing, and communication overhead. We introduce STEM (Scaling Transformers with Embedding Modules), a static, token-indexed approach that replaces the FFN up-projection with a layer-local embedding lookup while keeping the gate and down-projection dense. This removes runtime routing, enables CPU offload with asynchronous prefetch, and decouples capacity from both per-token FLOPs and cross-device communication. Empirically, STEM trains stably despite extreme sparsity. It improves downstream performance over dense baselines while reducing per-token FLOPs and parameter accesses (eliminating roughly one-third of FFN parameters). STEM learns embedding spaces with large angular spread which enhances its knowledge storage capacity. More interestingly, this enhanced knowledge capacity comes with better interpretability. The token-indexed nature of STEM embeddings allows simple ways to perform knowledge editing and knowledge injection in an interpretable manner without any intervention in the input text or additional computation. In addition, STEM strengthens long-context performance: as sequence length grows, more distinct parameters are activated, yielding practical test-time capacity scaling. Across 350M and 1B model scales, STEM delivers up to ~3--4% accuracy improvements overall, with notable gains on knowledge and reasoning-heavy benchmarks (ARC-Challenge, OpenBookQA, GSM8K, MMLU). Overall, STEM is an effective way of scaling parametric memory while providing better interpretability, better training stability and improved efficiency.",
      "authors": [
        "Ranajoy Sadhukhan",
        "Sheng Cao",
        "Harry Dong",
        "Changsheng Zhao",
        "Attiano Purpura-Pontoniere",
        "Yuandong Tian",
        "Zechun Liu",
        "Beidi Chen"
      ],
      "published": "2026-01-15T18:00:27+00:00",
      "updated": "2026-01-15T18:00:27+00:00",
      "category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "source": "arxiv",
      "source_priority": "high",
      "score": 8.6,
      "fetched_at": "2026-01-18T15:37:11.929871",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10660v1",
      "title": "Detecting Winning Arguments with Large Language Models and Persuasion Strategies",
      "url": "http://arxiv.org/abs/2601.10660v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10660v1",
      "summary": "Detecting persuasion in argumentative text is a challenging task with important implications for understanding human communication. This work investigates the role of persuasion strategies - such as Attack on reputation, Distraction, and Manipulative wording - in determining the persuasiveness of a text. We conduct experiments on three annotated argument datasets: Winning Arguments (built from the Change My View subreddit), Anthropic/Persuasion, and Persuasion for Good. Our approach leverages large language models (LLMs) with a Multi-Strategy Persuasion Scoring approach that guides reasoning over six persuasion strategies. Results show that strategy-guided reasoning improves the prediction of persuasiveness. To better understand the influence of content, we organize the Winning Argument dataset into broad discussion topics and analyze performance across them. We publicly release this topic-annotated version of the dataset to facilitate future research. Overall, our methodology demonstrates the value of structured, strategy-aware prompting for enhancing interpretability and robustness in argument quality assessment.",
      "authors": [
        "Tiziano Labruna",
        "Arkadiusz Modzelewski",
        "Giorgio Satta",
        "Giovanni Da San Martino"
      ],
      "published": "2026-01-15T18:30:15+00:00",
      "updated": "2026-01-15T18:30:15+00:00",
      "category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "source": "arxiv",
      "source_priority": "high",
      "score": 8.6,
      "fetched_at": "2026-01-18T15:37:15.265322",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10589v1",
      "title": "Be Your Own Red Teamer: Safety Alignment via Self-Play and Reflective Experience Replay",
      "url": "http://arxiv.org/abs/2601.10589v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10589v1",
      "summary": "Large Language Models (LLMs) have achieved remarkable capabilities but remain vulnerable to adversarial ``jailbreak'' attacks designed to bypass safety guardrails. Current safety alignment methods depend heavily on static external red teaming, utilizing fixed defense prompts or pre-collected adversarial datasets. This leads to a rigid defense that overfits known patterns and fails to generalize to novel, sophisticated threats. To address this critical limitation, we propose empowering the model to be its own red teamer, capable of achieving autonomous and evolving adversarial attacks. Specifically, we introduce Safety Self- Play (SSP), a system that utilizes a single LLM to act concurrently as both the Attacker (generating jailbreaks) and the Defender (refusing harmful requests) within a unified Reinforcement Learning (RL) loop, dynamically evolving attack strategies to uncover vulnerabilities while simultaneously strengthening defense mechanisms. To ensure the Defender effectively addresses critical safety issues during the self-play, we introduce an advanced Reflective Experience Replay Mechanism, which uses an experience pool accumulated throughout the process. The mechanism employs a Upper Confidence Bound (UCB) sampling strategy to focus on failure cases with low rewards, helping the model learn from past hard mistakes while balancing exploration and exploitation. Extensive experiments demonstrate that our SSP approach autonomously evolves robust defense capabilities, significantly outperforming baselines trained on static adversarial datasets and establishing a new benchmark for proactive safety alignment.",
      "authors": [
        "Hao Wang",
        "Yanting Wang",
        "Hao Li",
        "Rui Li",
        "Lei Sha"
      ],
      "published": "2026-01-15T17:00:16+00:00",
      "updated": "2026-01-15T17:00:16+00:00",
      "category": "cs.CL",
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "source": "arxiv",
      "source_priority": "high",
      "score": 8.6,
      "fetched_at": "2026-01-18T15:37:15.265370",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10532v1",
      "title": "PERM: Psychology-grounded Empathetic Reward Modeling for Large Language Models",
      "url": "http://arxiv.org/abs/2601.10532v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10532v1",
      "summary": "Large Language Models (LLMs) are increasingly deployed in human-centric applications, yet they often fail to provide substantive emotional support. While Reinforcement Learning (RL) has been utilized to enhance empathy of LLMs, existing reward models typically evaluate empathy from a single perspective, overlooking the inherently bidirectional interaction nature of empathy between the supporter and seeker as defined by Empathy Cycle theory. To address this limitation, we propose Psychology-grounded Empathetic Reward Modeling (PERM). PERM operationalizes empathy evaluation through a bidirectional decomposition: 1) Supporter perspective, assessing internal resonation and communicative expression; 2) Seeker perspective, evaluating emotional reception. Additionally, it incorporates a bystander perspective to monitor overall interaction quality. Extensive experiments on a widely-used emotional intelligence benchmark and an industrial daily conversation dataset demonstrate that PERM outperforms state-of-the-art baselines by over 10\\%. Furthermore, a blinded user study reveals a 70\\% preference for our approach, highlighting its efficacy in generating more empathetic responses. Our code, dataset, and models are available at https://github.com/ZhengWwwq/PERM.",
      "authors": [
        "Chengbing Wang",
        "Wuqiang Zheng",
        "Yang Zhang",
        "Fengbin Zhu",
        "Junyi Cheng",
        "Yi Xie",
        "Wenjie Wang",
        "Fuli Feng"
      ],
      "published": "2026-01-15T15:56:55+00:00",
      "updated": "2026-01-15T15:56:55+00:00",
      "category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "source": "arxiv",
      "source_priority": "high",
      "score": 8.6,
      "fetched_at": "2026-01-18T15:37:15.265489",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10710v1",
      "title": "From One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion",
      "url": "http://arxiv.org/abs/2601.10710v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10710v1",
      "summary": "Vision-Language Models (VLMs) create a severe visual feature bottleneck by using a crude, asymmetric connection that links only the output of the vision encoder to the input of the large language model (LLM). This static architecture fundamentally limits the ability of LLMs to achieve comprehensive alignment with hierarchical visual knowledge, compromising their capacity to accurately integrate local details with global semantics into coherent reasoning. To resolve this, we introduce Cross-Layer Injection (CLI), a novel and lightweight framework that forges a dynamic many-to-many bridge between the two modalities. CLI consists of two synergistic, parameter-efficient components: an Adaptive Multi-Projection (AMP) module that harmonizes features from diverse vision layers, and an Adaptive Gating Fusion (AGF) mechanism that empowers the LLM to selectively inject the most relevant visual information based on its real-time decoding context. We validate the effectiveness and versatility of CLI by integrating it into LLaVA-OneVision and LLaVA-1.5. Extensive experiments on 18 diverse benchmarks demonstrate significant performance improvements, establishing CLI as a scalable paradigm that unlocks deeper multimodal understanding by granting LLMs on-demand access to the full visual hierarchy.",
      "authors": [
        "Cheng Chen",
        "Yuyu Guo",
        "Pengpeng Zeng",
        "Jingkuan Song",
        "Peng Di",
        "Hang Yu",
        "Lianli Gao"
      ],
      "published": "2026-01-15T18:59:10+00:00",
      "updated": "2026-01-15T18:59:10+00:00",
      "category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "source": "arxiv",
      "source_priority": "high",
      "score": 8.6,
      "fetched_at": "2026-01-18T15:37:18.614184",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10551v1",
      "title": "Unleashing the Capabilities of Large Vision-Language Models for Intelligent Perception of Roadside Infrastructure",
      "url": "http://arxiv.org/abs/2601.10551v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10551v1",
      "summary": "Automated perception of urban roadside infrastructure is crucial for smart city management, yet general-purpose models often struggle to capture the necessary fine-grained attributes and domain rules. While Large Vision Language Models (VLMs) excel at open-world recognition, they often struggle to accurately interpret complex facility states in compliance with engineering standards, leading to unreliable performance in real-world applications. To address this, we propose a domain-adapted framework that transforms VLMs into specialized agents for intelligent infrastructure analysis. Our approach integrates a data-efficient fine-tuning strategy with a knowledge-grounded reasoning mechanism. Specifically, we leverage open-vocabulary fine-tuning on Grounding DINO to robustly localize diverse assets with minimal supervision, followed by LoRA-based adaptation on Qwen-VL for deep semantic attribute reasoning. To mitigate hallucinations and enforce professional compliance, we introduce a dual-modality Retrieval-Augmented Generation (RAG) module that dynamically retrieves authoritative industry standards and visual exemplars during inference. Evaluated on a comprehensive new dataset of urban roadside scenes, our framework achieves a detection performance of 58.9 mAP and an attribute recognition accuracy of 95.5%, demonstrating a robust solution for intelligent infrastructure monitoring.",
      "authors": [
        "Luxuan Fu",
        "Chong Liu",
        "Bisheng Yang",
        "Zhen Dong"
      ],
      "published": "2026-01-15T16:16:34+00:00",
      "updated": "2026-01-15T16:16:34+00:00",
      "category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "source": "arxiv",
      "source_priority": "high",
      "score": 8.6,
      "fetched_at": "2026-01-18T15:37:18.614509",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10504v1",
      "title": "DR-Arena: an Automated Evaluation Framework for Deep Research Agents",
      "url": "http://arxiv.org/abs/2601.10504v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10504v1",
      "summary": "As Large Language Models (LLMs) increasingly operate as Deep Research (DR) Agents capable of autonomous investigation and information synthesis, reliable evaluation of their task performance has become a critical bottleneck. Current benchmarks predominantly rely on static datasets, which suffer from several limitations: limited task generality, temporal misalignment, and data contamination. To address these, we introduce DR-Arena, a fully automated evaluation framework that pushes DR agents to their capability limits through dynamic investigation. DR-Arena constructs real-time Information Trees from fresh web trends to ensure the evaluation rubric is synchronized with the live world state, and employs an automated Examiner to generate structured tasks testing two orthogonal capabilities: Deep reasoning and Wide coverage. DR-Arena further adopts Adaptive Evolvement Loop, a state-machine controller that dynamically escalates task complexity based on real-time performance, demanding deeper deduction or wider aggregation until a decisive capability boundary emerges. Experiments with six advanced DR agents demonstrate that DR-Arena achieves a Spearman correlation of 0.94 with the LMSYS Search Arena leaderboard. This represents the state-of-the-art alignment with human preferences without any manual efforts, validating DR-Arena as a reliable alternative for costly human adjudication.",
      "authors": [
        "Yiwen Gao",
        "Ruochen Zhao",
        "Yang Deng",
        "Wenxuan Zhang"
      ],
      "published": "2026-01-15T15:28:21+00:00",
      "updated": "2026-01-15T15:28:21+00:00",
      "category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "source": "arxiv",
      "source_priority": "high",
      "score": 8.3,
      "fetched_at": "2026-01-18T15:37:15.265573",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10410v1",
      "title": "TF3-RO-50M: Training Compact Romanian Language Models from Scratch on Synthetic Moral Microfiction",
      "url": "http://arxiv.org/abs/2601.10410v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10410v1",
      "summary": "Recent advances in synthetic data generation have shown that compact language models can be trained effectively when the underlying corpus is structurally controlled and linguistically coherent. However, for morphologically rich and computationally under-resourced languages such as Romanian, there is still no openly documented, end-to-end pipeline that unifies tokenizer design, preprocessing, pretraining, compression, evaluation, and large-scale synthetic data generation in a reproducible framework. Building on TF1, a three-million-story English fable dataset, and TF2, which extends TF1 through high-quality Romanian translations, we introduce TF3-RO, a Romanian-centric language modeling pipeline spanning tokenizer training, from-scratch model development, and Romanian-native dataset generation. TF3-RO constructs Romanian-specific BPE and Unigram tokenizers from a linguistically informed corpus to mitigate token inflation induced by Romanian morphology. Using long-sequence packed training, we pretrain a 51.65M-parameter LLaMA-style Transformer entirely from scratch. The model is subsequently optimized through quantization, structured pruning, and logit-based knowledge distillation, yielding a compact 26.45M-parameter student model with tied embeddings and strong deployment characteristics. Using this distilled model, TF3-RO generates three million Romanian-native synthetic fables via a controlled combinatorial prompting framework. Across all stages, the pipeline integrates a comprehensive evaluation suite combining intrinsic metrics, Romanian agreement probes, entity coherence, rule-based grammar checking, and LLM-based assessment. TF3-RO provides a reproducible and linguistically grounded framework for training compact Romanian language models and producing large-scale synthetic narrative corpora.",
      "authors": [
        "Mihai Dan Nadas",
        "Laura Diosan",
        "Andreea Tomescu",
        "Andrei Piscoran"
      ],
      "published": "2026-01-15T14:02:00+00:00",
      "updated": "2026-01-15T14:02:00+00:00",
      "category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "source": "arxiv",
      "source_priority": "high",
      "score": 8.3,
      "fetched_at": "2026-01-18T15:37:15.265665",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10388v1",
      "title": "INDIC DIALECT: A Multi Task Benchmark to Evaluate and Translate in Indian Language Dialects",
      "url": "http://arxiv.org/abs/2601.10388v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10388v1",
      "summary": "Recent NLP advances focus primarily on standardized languages, leaving most low-resource dialects under-served especially in Indian scenarios. In India, the issue is particularly important: despite Hindi being the third most spoken language globally (over 600 million speakers), its numerous dialects remain underrepresented. The situation is similar for Odia, which has around 45 million speakers. While some datasets exist which contain standard Hindi and Odia languages, their regional dialects have almost no web presence. We introduce INDIC-DIALECT, a human-curated parallel corpus of 13k sentence pairs spanning 11 dialects and 2 languages: Hindi and Odia. Using this corpus, we construct a multi-task benchmark with three tasks: dialect classification, multiple-choice question (MCQ) answering, and machine translation (MT). Our experiments show that LLMs like GPT-4o and Gemini 2.5 perform poorly on the classification task. While fine-tuned transformer based models pretrained on Indian languages substantially improve performance e.g., improving F1 from 19.6\\% to 89.8\\% on dialect classification. For dialect to language translation, we find that hybrid AI model achieves highest BLEU score of 61.32 compared to the baseline score of 23.36. Interestingly, due to complexity in generating dialect sentences, we observe that for language to dialect translation the ``rule-based followed by AI\" approach achieves best BLEU score of 48.44 compared to the baseline score of 27.59. INDIC-DIALECT thus is a new benchmark for dialect-aware Indic NLP, and we plan to release it as open source to support further work on low-resource Indian dialects.",
      "authors": [
        "Tarun Sharma",
        "Manikandan Ravikiran",
        "Sourava Kumar Behera",
        "Pramit Bhattacharya",
        "Arnab Bhattacharya",
        "Rohit Saluja"
      ],
      "published": "2026-01-15T13:40:27+00:00",
      "updated": "2026-01-15T13:40:27+00:00",
      "category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "source": "arxiv",
      "source_priority": "high",
      "score": 8.3,
      "fetched_at": "2026-01-18T15:37:15.265689",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10562v1",
      "title": "Process-Guided Concept Bottleneck Model",
      "url": "http://arxiv.org/abs/2601.10562v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10562v1",
      "summary": "Concept Bottleneck Models (CBMs) improve the explainability of black-box Deep Learning (DL) by introducing intermediate semantic concepts. However, standard CBMs often overlook domain-specific relationships and causal mechanisms, and their dependence on complete concept labels limits applicability in scientific domains where supervision is sparse but processes are well defined. To address this, we propose the Process-Guided Concept Bottleneck Model (PG-CBM), an extension of CBMs which constrains learning to follow domain-defined causal mechanisms through biophysically meaningful intermediate concepts. Using above ground biomass density estimation from Earth Observation data as a case study, we show that PG-CBM reduces error and bias compared to multiple benchmarks, whilst leveraging multi-source heterogeneous training data and producing interpretable intermediate outputs. Beyond improved accuracy, PG-CBM enhances transparency, enables detection of spurious learning, and provides scientific insights, representing a step toward more trustworthy AI systems in scientific applications.",
      "authors": [
        "Reza M. Asiyabi",
        "SEOSAW Partnership",
        "Steven Hancock",
        "Casey Ryan"
      ],
      "published": "2026-01-15T16:25:55+00:00",
      "updated": "2026-01-15T16:25:55+00:00",
      "category": "cs.AI",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "source": "arxiv",
      "source_priority": "high",
      "score": 8.2,
      "fetched_at": "2026-01-18T15:37:08.020697",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10645v1",
      "title": "Influential Training Data Retrieval for Explaining Verbalized Confidence of LLMs",
      "url": "http://arxiv.org/abs/2601.10645v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10645v1",
      "summary": "Large language models (LLMs) can increase users' perceived trust by verbalizing confidence in their outputs. However, prior work has shown that LLMs are often overconfident, making their stated confidence unreliable since it does not consistently align with factual accuracy. To better understand the sources of this verbalized confidence, we introduce TracVC (\\textbf{Trac}ing \\textbf{V}erbalized \\textbf{C}onfidence), a method that builds on information retrieval and influence estimation to trace generated confidence expressions back to the training data. We evaluate TracVC on OLMo and Llama models in a question answering setting, proposing a new metric, content groundness, which measures the extent to which an LLM grounds its confidence in content-related training examples (relevant to the question and answer) versus in generic examples of confidence verbalization. Our analysis reveals that OLMo2-13B is frequently influenced by confidence-related data that is lexically unrelated to the query, suggesting that it may mimic superficial linguistic expressions of certainty rather than rely on genuine content grounding. These findings point to a fundamental limitation in current training regimes: LLMs may learn how to sound confident without learning when confidence is justified. Our analysis provides a foundation for improving LLMs' trustworthiness in expressing more reliable confidence.",
      "authors": [
        "Yuxi Xia",
        "Loris Schoenegger",
        "Benjamin Roth"
      ],
      "published": "2026-01-15T18:05:42+00:00",
      "updated": "2026-01-15T18:05:42+00:00",
      "category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "source": "arxiv",
      "source_priority": "high",
      "score": 8.2,
      "fetched_at": "2026-01-18T15:37:15.265345",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10649v1",
      "title": "CURVE: A Benchmark for Cultural and Multilingual Long Video Reasoning",
      "url": "http://arxiv.org/abs/2601.10649v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10649v1",
      "summary": "Recent advancements in video models have shown tremendous progress, particularly in long video understanding. However, current benchmarks predominantly feature western-centric data and English as the dominant language, introducing significant biases in evaluation. To address this, we introduce CURVE (Cultural Understanding and Reasoning in Video Evaluation), a challenging benchmark for multicultural and multilingual video reasoning. CURVE comprises high-quality, entirely human-generated annotations from diverse, region-specific cultural videos across 18 global locales. Unlike prior work that relies on automatic translations, CURVE provides complex questions, answers, and multi-step reasoning steps, all crafted in native languages. Making progress on CURVE requires a deeply situated understanding of visual cultural context. Furthermore, we leverage CURVE's reasoning traces to construct evidence-based graphs and propose a novel iterative strategy using these graphs to identify fine-grained errors in reasoning. Our evaluations reveal that SoTA Video-LLMs struggle significantly, performing substantially below human-level accuracy, with errors primarily stemming from the visual perception of cultural elements. CURVE will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file\\#minerva-cultural",
      "authors": [
        "Darshan Singh",
        "Arsha Nagrani",
        "Kawshik Manikantan",
        "Harman Singh",
        "Dinesh Tewari",
        "Tobias Weyand",
        "Cordelia Schmid",
        "Anelia Angelova",
        "Shachi Dave"
      ],
      "published": "2026-01-15T18:15:06+00:00",
      "updated": "2026-01-15T18:15:06+00:00",
      "category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "source": "arxiv",
      "source_priority": "high",
      "score": 8.2,
      "fetched_at": "2026-01-18T15:37:18.614235",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10535v1",
      "title": "SVII-3D: Advancing Roadside Infrastructure Inventory with Decimeter-level 3D Localization and Comprehension from Sparse Street Imagery",
      "url": "http://arxiv.org/abs/2601.10535v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10535v1",
      "summary": "The automated creation of digital twins and precise asset inventories is a critical task in smart city construction and facility lifecycle management. However, utilizing cost-effective sparse imagery remains challenging due to limited robustness, inaccurate localization, and a lack of fine-grained state understanding. To address these limitations, SVII-3D, a unified framework for holistic asset digitization, is proposed. First, LoRA fine-tuned open-set detection is fused with a spatial-attention matching network to robustly associate observations across sparse views. Second, a geometry-guided refinement mechanism is introduced to resolve structural errors, achieving precise decimeter-level 3D localization. Third, transcending static geometric mapping, a Vision-Language Model agent leveraging multi-modal prompting is incorporated to automatically diagnose fine-grained operational states. Experiments demonstrate that SVII-3D significantly improves identification accuracy and minimizes localization errors. Consequently, this framework offers a scalable, cost-effective solution for high-fidelity infrastructure digitization, effectively bridging the gap between sparse perception and automated intelligent maintenance.",
      "authors": [
        "Chong Liu",
        "Luxuan Fu",
        "Yang Jia",
        "Zhen Dong",
        "Bisheng Yang"
      ],
      "published": "2026-01-15T15:57:18+00:00",
      "updated": "2026-01-15T15:57:18+00:00",
      "category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "source": "arxiv",
      "source_priority": "high",
      "score": 8.2,
      "fetched_at": "2026-01-18T15:37:18.614558",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10521v1",
      "title": "BikeActions: An Open Platform and Benchmark for Cyclist-Centric VRU Action Recognition",
      "url": "http://arxiv.org/abs/2601.10521v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10521v1",
      "summary": "Anticipating the intentions of Vulnerable Road Users (VRUs) is a critical challenge for safe autonomous driving (AD) and mobile robotics. While current research predominantly focuses on pedestrian crossing behaviors from a vehicle's perspective, interactions within dense shared spaces remain underexplored. To bridge this gap, we introduce FUSE-Bike, the first fully open perception platform of its kind. Equipped with two LiDARs, a camera, and GNSS, it facilitates high-fidelity, close-range data capture directly from a cyclist's viewpoint. Leveraging this platform, we present BikeActions, a novel multi-modal dataset comprising 852 annotated samples across 5 distinct action classes, specifically tailored to improve VRU behavior modeling. We establish a rigorous benchmark by evaluating state-of-the-art graph convolution and transformer-based models on our publicly released data splits, establishing the first performance baselines for this challenging task. We release the full dataset together with data curation tools, the open hardware design, and the benchmark code to foster future research in VRU action understanding under https://iv.ee.hm.edu/bikeactions/.",
      "authors": [
        "Max A. Buettner",
        "Kanak Mazumder",
        "Luca Koecher",
        "Mario Finkbeiner",
        "Sebastian Niebler",
        "Fabian B. Flohr"
      ],
      "published": "2026-01-15T15:47:46+00:00",
      "updated": "2026-01-15T15:47:46+00:00",
      "category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "source": "arxiv",
      "source_priority": "high",
      "score": 8.2,
      "fetched_at": "2026-01-18T15:37:18.614613",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10611v1",
      "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
      "url": "http://arxiv.org/abs/2601.10611v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10611v1",
      "summary": "Today's strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&F on video tracking).",
      "authors": [
        "Christopher Clark",
        "Jieyu Zhang",
        "Zixian Ma",
        "Jae Sung Park",
        "Mohammadreza Salehi",
        "Rohun Tripathi",
        "Sangho Lee",
        "Zhongzheng Ren",
        "Chris Dongjoo Kim",
        "Yinuo Yang",
        "Vincent Shao",
        "Yue Yang",
        "Weikai Huang",
        "Ziqi Gao",
        "Taira Anderson",
        "Jianrui Zhang",
        "Jitesh Jain",
        "George Stoica",
        "Winson Han",
        "Ali Farhadi",
        "Ranjay Krishna"
      ],
      "published": "2026-01-15T17:27:44+00:00",
      "updated": "2026-01-15T17:27:44+00:00",
      "category": "cs.AI",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.8,
      "fetched_at": "2026-01-18T15:37:08.020560",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10600v1",
      "title": "Procedural Fairness in Multi-Agent Bandits",
      "url": "http://arxiv.org/abs/2601.10600v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10600v1",
      "summary": "In the context of multi-agent multi-armed bandits (MA-MAB), fairness is often reduced to outcomes: maximizing welfare, reducing inequality, or balancing utilities. However, evidence in psychology, economics, and Rawlsian theory suggests that fairness is also about process and who gets a say in the decisions being made. We introduce a new fairness objective, procedural fairness, which provides equal decision-making power for all agents, lies in the core, and provides for proportionality in outcomes. Empirical results confirm that fairness notions based on optimizing for outcomes sacrifice equal voice and representation, while the sacrifice in outcome-based fairness objectives (like equality and utilitarianism) is minimal under procedurally fair policies. We further prove that different fairness notions prioritize fundamentally different and incompatible values, highlighting that fairness requires explicit normative choices. This paper argues that procedural legitimacy deserves greater focus as a fairness objective, and provides a framework for putting procedural fairness into practice.",
      "authors": [
        "Joshua Caiata",
        "Carter Blair",
        "Kate Larson"
      ],
      "published": "2026-01-15T17:11:51+00:00",
      "updated": "2026-01-15T17:11:51+00:00",
      "category": "cs.AI",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.GT",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.8,
      "fetched_at": "2026-01-18T15:37:08.020581",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10560v1",
      "title": "Learning Latency-Aware Orchestration for Parallel Multi-Agent Systems",
      "url": "http://arxiv.org/abs/2601.10560v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10560v1",
      "summary": "Multi-agent systems (MAS) enable complex reasoning by coordinating multiple agents, but often incur high inference latency due to multi-step execution and repeated model invocations, severely limiting their scalability and usability in time-sensitive scenarios. Most existing approaches primarily optimize task performance and inference cost, and explicitly or implicitly assume sequential execution, making them less optimal for controlling latency under parallel execution. In this work, we investigate learning-based orchestration of multi-agent systems with explicit latency supervision under parallel execution. We propose Latency-Aware Multi-agent System (LAMaS), a latency-aware multi-agent orchestration framework that enables parallel execution and explicitly optimizes the critical execution path, allowing the controller to construct execution topology graphs with lower latency under parallel execution. Our experiments show that our approach reduces critical path length by 38-46% compared to the state-of-the-art baseline for multi-agent architecture search across multiple benchmarks, while maintaining or even improving task performance. These results highlight the importance of explicitly optimizing latency under parallel execution when designing efficient multi-agent systems. The code is available at https://github.com/xishi404/LAMaS",
      "authors": [
        "Xi Shi",
        "Mengxin Zheng",
        "Qian Lou"
      ],
      "published": "2026-01-15T16:23:53+00:00",
      "updated": "2026-01-15T16:23:53+00:00",
      "category": "cs.AI",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.MA",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.8,
      "fetched_at": "2026-01-18T15:37:08.020716",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10707v1",
      "title": "See Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection",
      "url": "http://arxiv.org/abs/2601.10707v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10707v1",
      "summary": "Recent advances in end-to-end autonomous driving show that policies trained on patch-aligned features extracted from foundation models generalize better to Out-of-Distribution (OOD). We hypothesize that due to the self-attention mechanism, each patch feature implicitly embeds/contains information from all other patches, represented in a different way and intensity, making these descriptors highly redundant. We quantify redundancy in such (BLIP2) features via PCA and cross-patch similarity: $90$% of variance is captured by $17/64$ principal components, and strong inter-token correlations are pervasive. Training on such overlapping information leads the policy to overfit spurious correlations, hurting OOD robustness. We present Stochastic-Patch-Selection (SPS), a simple yet effective approach for learning policies that are more robust, generalizable, and efficient. For every frame, SPS randomly masks a fraction of patch descriptors, not feeding them to the policy model, while preserving the spatial layout of the remaining patches. Thus, the policy is provided with different stochastic but complete views of the (same) scene: every random subset of patches acts like a different, yet still sensible, coherent projection of the world. The policy thus bases its decisions on features that are invariant to which specific tokens survive. Extensive experiments confirm that across all OOD scenarios, our method outperforms the state of the art (SOTA), achieving a $6.2$% average improvement and up to $20.4$% in closed-loop simulations, while being $2.4\\times$ faster. We conduct ablations over masking rates and patch-feature reorganization, training and evaluating 9 systems, with 8 of them surpassing prior SOTA. Finally, we show that the same learned policy transfers to a physical, real-world car without any tuning.",
      "authors": [
        "Amir Mallak",
        "Erfan Aasi",
        "Shiva Sreeram",
        "Tsun-Hsuan Wang",
        "Daniela Rus",
        "Alaa Maalouf"
      ],
      "published": "2026-01-15T18:58:33+00:00",
      "updated": "2026-01-15T18:58:33+00:00",
      "category": "cs.LG",
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.8,
      "fetched_at": "2026-01-18T15:37:11.929651",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10583v1",
      "title": "Combinatorial Optimization Augmented Machine Learning",
      "url": "http://arxiv.org/abs/2601.10583v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10583v1",
      "summary": "Combinatorial optimization augmented machine learning (COAML) has recently emerged as a powerful paradigm for integrating predictive models with combinatorial decision-making. By embedding combinatorial optimization oracles into learning pipelines, COAML enables the construction of policies that are both data-driven and feasibility-preserving, bridging the traditions of machine learning, operations research, and stochastic optimization. This paper provides a comprehensive overview of the state of the art in COAML. We introduce a unifying framework for COAML pipelines, describe their methodological building blocks, and formalize their connection to empirical cost minimization. We then develop a taxonomy of problem settings based on the form of uncertainty and decision structure. Using this taxonomy, we review algorithmic approaches for static and dynamic problems, survey applications across domains such as scheduling, vehicle routing, stochastic programming, and reinforcement learning, and synthesize methodological contributions in terms of empirical cost minimization, imitation learning, and reinforcement learning. Finally, we identify key research frontiers. This survey aims to serve both as a tutorial introduction to the field and as a roadmap for future research at the interface of combinatorial optimization and machine learning.",
      "authors": [
        "Maximilian Schiffer",
        "Heiko Hoppe",
        "Yue Su",
        "Louis Bouvier",
        "Axel Parmentier"
      ],
      "published": "2026-01-15T16:55:19+00:00",
      "updated": "2026-01-15T16:55:19+00:00",
      "category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.8,
      "fetched_at": "2026-01-18T15:37:11.930020",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10606v1",
      "title": "RSATalker: Realistic Socially-Aware Talking Head Generation for Multi-Turn Conversation",
      "url": "http://arxiv.org/abs/2601.10606v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10606v1",
      "summary": "Talking head generation is increasingly important in virtual reality (VR), especially for social scenarios involving multi-turn conversation. Existing approaches face notable limitations: mesh-based 3D methods can model dual-person dialogue but lack realistic textures, while large-model-based 2D methods produce natural appearances but incur prohibitive computational costs. Recently, 3D Gaussian Splatting (3DGS) based methods achieve efficient and realistic rendering but remain speaker-only and ignore social relationships. We introduce RSATalker, the first framework that leverages 3DGS for realistic and socially-aware talking head generation with support for multi-turn conversation. Our method first drives mesh-based 3D facial motion from speech, then binds 3D Gaussians to mesh facets to render high-fidelity 2D avatar videos. To capture interpersonal dynamics, we propose a socially-aware module that encodes social relationships, including blood and non-blood as well as equal and unequal, into high-level embeddings through a learnable query mechanism. We design a three-stage training paradigm and construct the RSATalker dataset with speech-mesh-image triplets annotated with social relationships. Extensive experiments demonstrate that RSATalker achieves state-of-the-art performance in both realism and social awareness. The code and dataset will be released.",
      "authors": [
        "Peng Chen",
        "Xiaobao Wei",
        "Yi Yang",
        "Naiming Yao",
        "Hui Chen",
        "Feng Tian"
      ],
      "published": "2026-01-15T17:23:19+00:00",
      "updated": "2026-01-15T17:23:19+00:00",
      "category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.8,
      "fetched_at": "2026-01-18T15:37:18.614348",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10592v1",
      "title": "Action100M: A Large-scale Video Action Dataset",
      "url": "http://arxiv.org/abs/2601.10592v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10592v1",
      "summary": "Inferring physical actions from visual observations is a fundamental capability for advancing machine intelligence in the physical world. Achieving this requires large-scale, open-vocabulary video action datasets that span broad domains. We introduce Action100M, a large-scale dataset constructed from 1.2M Internet instructional videos (14.6 years of duration), yielding O(100 million) temporally localized segments with open-vocabulary action supervision and rich captions. Action100M is generated by a fully automated pipeline that (i) performs hierarchical temporal segmentation using V-JEPA 2 embeddings, (ii) produces multi-level frame and segment captions organized as a Tree-of-Captions, and (iii) aggregates evidence with a reasoning model (GPT-OSS-120B) under a multi-round Self-Refine procedure to output structured annotations (brief/detailed action, actor, brief/detailed caption). Training VL-JEPA on Action100M demonstrates consistent data-scaling improvements and strong zero-shot performance across diverse action recognition benchmarks, establishing Action100M as a new foundation for scalable research in video understanding and world modeling.",
      "authors": [
        "Delong Chen",
        "Tejaswi Kasarla",
        "Yejin Bang",
        "Mustafa Shukor",
        "Willy Chung",
        "Jade Yu",
        "Allen Bolourchi",
        "Theo Moutakanni",
        "Pascale Fung"
      ],
      "published": "2026-01-15T17:02:27+00:00",
      "updated": "2026-01-15T17:02:27+00:00",
      "category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.8,
      "fetched_at": "2026-01-18T15:37:18.614371",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10577v1",
      "title": "Jordan-Segmentable Masks: A Topology-Aware definition for characterizing Binary Image Segmentation",
      "url": "http://arxiv.org/abs/2601.10577v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10577v1",
      "summary": "Image segmentation plays a central role in computer vision. However, widely used evaluation metrics, whether pixel-wise, region-based, or boundary-focused, often struggle to capture the structural and topological coherence of a segmentation. In many practical scenarios, such as medical imaging or object delineation, small inaccuracies in boundary, holes, or fragmented predictions can result in high metric scores, despite the fact that the resulting masks fail to preserve the object global shape or connectivity. This highlights a limitation of conventional metrics: they are unable to assess whether a predicted segmentation partitions the image into meaningful interior and exterior regions.\n  In this work, we introduce a topology-aware notion of segmentation based on the Jordan Curve Theorem, and adapted for use in digital planes. We define the concept of a \\emph{Jordan-segmentatable mask}, which is a binary segmentation whose structure ensures a topological separation of the image domain into two connected components. We analyze segmentation masks through the lens of digital topology and homology theory, extracting a $4$-curve candidate from the mask, verifying its topological validity using Betti numbers. A mask is considered Jordan-segmentatable when this candidate forms a digital 4-curve with $\u03b2_0 = \u03b2_1 = 1$, or equivalently when its complement splits into exactly two $8$-connected components.\n  This framework provides a mathematically rigorous, unsupervised criterion with which to assess the structural coherence of segmentation masks. By combining digital Jordan theory and homological invariants, our approach provides a valuable alternative to standard evaluation metrics, especially in applications where topological correctness must be preserved.",
      "authors": [
        "Serena Grazia De Benedictis",
        "Amedeo Altavilla",
        "Nicoletta Del Buono"
      ],
      "published": "2026-01-15T16:47:53+00:00",
      "updated": "2026-01-15T16:47:53+00:00",
      "category": "cs.CV",
      "categories": [
        "cs.CV",
        "math.AT",
        "math.NA"
      ],
      "primary_category": "cs.CV",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.8,
      "fetched_at": "2026-01-18T15:37:18.614418",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10696v1",
      "title": "The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load",
      "url": "http://arxiv.org/abs/2601.10696v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10696v1",
      "summary": "Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users' prior expertise and interaction strategies through prompting.",
      "authors": [
        "Han Jiang",
        "Yao Xiao",
        "Rachel Hurley",
        "Shichao Liu"
      ],
      "published": "2026-01-15T18:52:59+00:00",
      "updated": "2026-01-15T18:52:59+00:00",
      "category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.4,
      "fetched_at": "2026-01-18T15:37:08.020445",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10708v1",
      "title": "High-accuracy and dimension-free sampling with diffusions",
      "url": "http://arxiv.org/abs/2601.10708v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10708v1",
      "summary": "Diffusion models have shown remarkable empirical success in sampling from rich multi-modal distributions. Their inference relies on numerically solving a certain differential equation. This differential equation cannot be solved in closed form, and its resolution via discretization typically requires many small iterations to produce \\emph{high-quality} samples.\n  More precisely, prior works have shown that the iteration complexity of discretization methods for diffusion models scales polynomially in the ambient dimension and the inverse accuracy $1/\\varepsilon$. In this work, we propose a new solver for diffusion models relying on a subtle interplay between low-degree approximation and the collocation method (Lee, Song, Vempala 2018), and we prove that its iteration complexity scales \\emph{polylogarithmically} in $1/\\varepsilon$, yielding the first ``high-accuracy'' guarantee for a diffusion-based sampler that only uses (approximate) access to the scores of the data distribution. In addition, our bound does not depend explicitly on the ambient dimension; more precisely, the dimension affects the complexity of our solver through the \\emph{effective radius} of the support of the target distribution only.",
      "authors": [
        "Khashayar Gatmiry",
        "Sitan Chen",
        "Adil Salim"
      ],
      "published": "2026-01-15T18:58:50+00:00",
      "updated": "2026-01-15T18:58:50+00:00",
      "category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.ST"
      ],
      "primary_category": "cs.LG",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.4,
      "fetched_at": "2026-01-18T15:37:11.929624",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10705v1",
      "title": "Distributed Perceptron under Bounded Staleness, Partial Participation, and Noisy Communication",
      "url": "http://arxiv.org/abs/2601.10705v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10705v1",
      "summary": "We study a semi-asynchronous client-server perceptron trained via iterative parameter mixing (IPM-style averaging): clients run local perceptron updates and a server forms a global model by aggregating the updates that arrive in each communication round. The setting captures three system effects in federated and distributed deployments: (i) stale updates due to delayed model delivery and delayed application of client computations (two-sided version lag), (ii) partial participation (intermittent client availability), and (iii) imperfect communication on both downlink and uplink, modeled as effective zero-mean additive noise with bounded second moment. We introduce a server-side aggregation rule called staleness-bucket aggregation with padding that deterministically enforces a prescribed staleness profile over update ages without assuming any stochastic model for delays or participation. Under margin separability and bounded data radius, we prove a finite-horizon expected bound on the cumulative weighted number of perceptron mistakes over a given number of server rounds: the impact of delay appears only through the mean enforced staleness, whereas communication noise contributes an additional term that grows on the order of the square root of the horizon with the total noise energy. In the noiseless case, we show how a finite expected mistake budget yields an explicit finite-round stabilization bound under a mild fresh-participation condition.",
      "authors": [
        "Keval Jain",
        "Anant Raj",
        "Saurav Prakash",
        "Girish Varma"
      ],
      "published": "2026-01-15T18:56:54+00:00",
      "updated": "2026-01-15T18:56:54+00:00",
      "category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.4,
      "fetched_at": "2026-01-18T15:37:11.929675",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10701v1",
      "title": "Communication-Efficient and Privacy-Adaptable Mechanism -- a Federated Learning Scheme with Convergence Analysis",
      "url": "http://arxiv.org/abs/2601.10701v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10701v1",
      "summary": "Federated learning enables multiple parties to jointly train learning models without sharing their own underlying data, offering a practical pathway to privacy-preserving collaboration under data-governance constraints. Continued study of federated learning is essential to address key challenges in it, including communication efficiency and privacy protection between parties. A recent line of work introduced a novel approach called the Communication-Efficient and Privacy-Adaptable Mechanism (CEPAM), which achieves both objectives simultaneously. CEPAM leverages the rejection-sampled universal quantizer (RSUQ), a randomized vector quantizer whose quantization error is equivalent to a prescribed noise, which can be tuned to customize privacy protection between parties. In this work, we theoretically analyze the privacy guarantees and convergence properties of CEPAM. Moreover, we assess CEPAM's utility performance through experimental evaluations, including convergence profiles compared with other baselines, and accuracy-privacy trade-offs between different parties.",
      "authors": [
        "Chun Hei Michael Shiu",
        "Chih Wei Ling"
      ],
      "published": "2026-01-15T18:55:00+00:00",
      "updated": "2026-01-15T18:55:00+00:00",
      "category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.4,
      "fetched_at": "2026-01-18T15:37:11.929697",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10690v1",
      "title": "Data-driven stochastic reduced-order modeling of parametrized dynamical systems",
      "url": "http://arxiv.org/abs/2601.10690v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10690v1",
      "summary": "Modeling complex dynamical systems under varying conditions is computationally intensive, often rendering high-fidelity simulations intractable. Although reduced-order models (ROMs) offer a promising solution, current methods often struggle with stochastic dynamics and fail to quantify prediction uncertainty, limiting their utility in robust decision-making contexts. To address these challenges, we introduce a data-driven framework for learning continuous-time stochastic ROMs that generalize across parameter spaces and forcing conditions. Our approach, based on amortized stochastic variational inference, leverages a reparametrization trick for Markov Gaussian processes to eliminate the need for computationally expensive forward solvers during training. This enables us to jointly learn a probabilistic autoencoder and stochastic differential equations governing the latent dynamics, at a computational cost that is independent of the dataset size and system stiffness. Additionally, our approach offers the flexibility of incorporating physics-informed priors if available. Numerical studies are presented for three challenging test problems, where we demonstrate excellent generalization to unseen parameter combinations and forcings, and significant efficiency gains compared to existing approaches.",
      "authors": [
        "Andrew F. Ilersich",
        "Kevin Course",
        "Prasanth B. Nair"
      ],
      "published": "2026-01-15T18:50:18+00:00",
      "updated": "2026-01-15T18:50:18+00:00",
      "category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.4,
      "fetched_at": "2026-01-18T15:37:11.929720",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10628v1",
      "title": "Parametric RDT approach to computational gap of symmetric binary perceptron",
      "url": "http://arxiv.org/abs/2601.10628v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10628v1",
      "summary": "We study potential presence of statistical-computational gaps (SCG) in symmetric binary perceptrons (SBP) via a parametric utilization of \\emph{fully lifted random duality theory} (fl-RDT) [96]. A structural change from decreasingly to arbitrarily ordered $c$-sequence (a key fl-RDT parametric component) is observed on the second lifting level and associated with \\emph{satisfiability} ($\u03b1_c$) -- \\emph{algorithmic} ($\u03b1_a$) constraints density threshold change thereby suggesting a potential existence of a nonzero computational gap $SCG=\u03b1_c-\u03b1_a$. The second level estimate is shown to match the theoretical $\u03b1_c$ whereas the $r\\rightarrow \\infty$ level one is proposed to correspond to $\u03b1_a$. For example, for the canonical SBP ($\u03ba=1$ margin) we obtain $\u03b1_c\\approx 1.8159$ on the second and $\u03b1_a\\approx 1.6021$ (with converging tendency towards $\\sim 1.59$ range) on the seventh level. Our propositions remarkably well concur with recent literature: (i) in [20] local entropy replica approach predicts $\u03b1_{LE}\\approx 1.58$ as the onset of clustering defragmentation (presumed driving force behind locally improving algorithms failures); (ii) in $\u03b1\\rightarrow 0$ regime we obtain on the third lifting level $\u03ba\\approx 1.2385\\sqrt{\\frac{\u03b1_a}{-\\log\\left ( \u03b1_a \\right ) }}$ which qualitatively matches overlap gap property (OGP) based predictions of [43] and identically matches local entropy based predictions of [24]; (iii) $c$-sequence ordering change phenomenology mirrors the one observed in asymmetric binary perceptron (ABP) in [98] and the negative Hopfield model in [100]; and (iv) as in [98,100], we here design a CLuP based algorithm whose practical performance closely matches proposed theoretical predictions.",
      "authors": [
        "Mihailo Stojnic"
      ],
      "published": "2026-01-15T17:48:58+00:00",
      "updated": "2026-01-15T17:48:58+00:00",
      "category": "cs.LG",
      "categories": [
        "stat.ML",
        "cond-mat.dis-nn",
        "cs.IT",
        "cs.LG",
        "math.PR"
      ],
      "primary_category": "stat.ML",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.4,
      "fetched_at": "2026-01-18T15:37:11.929922",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10566v1",
      "title": "Representation-Aware Unlearning via Activation Signatures: From Suppression to Knowledge-Signature Erasure",
      "url": "http://arxiv.org/abs/2601.10566v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10566v1",
      "summary": "Selective knowledge erasure from LLMs is critical for GDPR compliance and model safety, yet current unlearning methods conflate behavioral suppression with true knowledge removal, allowing latent capabilities to persist beneath surface-level refusals. In this work, we address this challenge by introducing Knowledge Immunization Framework (KIF), a representation-aware architecture that distinguishes genuine erasure from obfuscation by targeting internal activation signatures rather than surface outputs. Our approach combines dynamic suppression of subject-specific representations with parameter-efficient adaptation, enabling durable unlearning without full model retraining. KIF achieves near-oracle erasure (FQ approx 0.99 vs. 1.00) while preserving utility at oracle levels (MU = 0.62), effectively breaking the stability-erasure tradeoff that has constrained all prior work. We evaluate both standard foundation models (Llama and Mistral) and reasoning-prior models (Qwen and DeepSeek) across 3B to 14B parameters. Our observation shows that standard models exhibit scale-independent true erasure (<3% utility drift), while reasoning-prior models reveal fundamental architectural divergence. Our comprehensive dual-metric evaluation protocol, combining surface-level leakage with latent trace persistence, operationalizes the obfuscation - erasure distinction and enables the first systematic diagnosis of mechanism-level forgetting behavior across model families and scales.",
      "authors": [
        "Syed Naveed Mahmood",
        "Md. Rezaur Rahman Bhuiyan",
        "Tasfia Zaman",
        "Jareen Tasneem Khondaker",
        "Md. Sameer Sakib",
        "Nazia Tasnim",
        "Farig Sadeque"
      ],
      "published": "2026-01-15T16:28:14+00:00",
      "updated": "2026-01-15T16:28:14+00:00",
      "category": "cs.LG",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.4,
      "fetched_at": "2026-01-18T15:37:11.930072",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10714v1",
      "title": "Alterbute: Editing Intrinsic Attributes of Objects in Images",
      "url": "http://arxiv.org/abs/2601.10714v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10714v1",
      "summary": "We introduce Alterbute, a diffusion-based method for editing an object's intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., ''Porsche 911 Carrera'') that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing.",
      "authors": [
        "Tal Reiss",
        "Daniel Winter",
        "Matan Cohen",
        "Alex Rav-Acha",
        "Yael Pritch",
        "Ariel Shamir",
        "Yedid Hoshen"
      ],
      "published": "2026-01-15T18:59:53+00:00",
      "updated": "2026-01-15T18:59:53+00:00",
      "category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.4,
      "fetched_at": "2026-01-18T15:37:18.614157",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10632v1",
      "title": "CoMoVi: Co-Generation of 3D Human Motions and Realistic Videos",
      "url": "http://arxiv.org/abs/2601.10632v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10632v1",
      "summary": "In this paper, we find that the generation of 3D human motions and 2D human videos is intrinsically coupled. 3D motions provide the structural prior for plausibility and consistency in videos, while pre-trained video models offer strong generalization capabilities for motions, which necessitate coupling their generation processes. Based on this, we present CoMoVi, a co-generative framework that couples two video diffusion models (VDMs) to generate 3D human motions and videos synchronously within a single diffusion denoising loop. To achieve this, we first propose an effective 2D human motion representation that can inherit the powerful prior of pre-trained VDMs. Then, we design a dual-branch diffusion model to couple human motion and video generation process with mutual feature interaction and 3D-2D cross attentions. Moreover, we curate CoMoVi Dataset, a large-scale real-world human video dataset with text and motion annotations, covering diverse and challenging human motions. Extensive experiments demonstrate the effectiveness of our method in both 3D human motion and video generation tasks.",
      "authors": [
        "Chengfeng Zhao",
        "Jiazhi Shu",
        "Yubo Zhao",
        "Tianyu Huang",
        "Jiahao Lu",
        "Zekai Gu",
        "Chengwei Ren",
        "Zhiyang Dou",
        "Qing Shuai",
        "Yuan Liu"
      ],
      "published": "2026-01-15T17:52:29+00:00",
      "updated": "2026-01-15T17:52:29+00:00",
      "category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.4,
      "fetched_at": "2026-01-18T15:37:18.614267",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10607v1",
      "title": "Multi-Objective Pareto-Front Optimization for Efficient Adaptive VVC Streaming",
      "url": "http://arxiv.org/abs/2601.10607v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10607v1",
      "summary": "Adaptive video streaming has facilitated improved video streaming over the past years. A balance among coding performance objectives such as bitrate, video quality, and decoding complexity is required to achieve efficient, content- and codec-dependent, adaptive video streaming. This paper proposes a multi-objective Pareto-front (PF) optimization framework to construct quality-monotonic, content-adaptive bitrate ladders Versatile Video Coding (VVC) streaming that jointly optimize video quality, bitrate, and decoding time, which is used as a practical proxy for decoding energy. Two strategies are introduced: the Joint Rate-Quality-Time Pareto Front (JRQT-PF) and the Joint Quality-Time Pareto Front (JQT-PF), each exploring different tradeoff formulations and objective prioritizations. The ladders are constructed under quality monotonicity constraints during adaptive streaming to ensure a consistent Quality of Experience (QoE). Experiments are conducted on a large-scale UHD dataset (Inter-4K), with quality assessed using PSNR, VMAF, and XPSNR, and complexity measured via decoding time and energy consumption. The JQT-PF method achieves 11.76% average bitrate savings while reducing average decoding time by 0.29% to maintain the same XPSNR, compared to a widely-used fixed ladder. More aggressive configurations yield up to 27.88% bitrate savings at the cost of increased complexity. The JRQT-PF strategy, on the other hand, offers more controlled tradeoffs, achieving 6.38 % bitrate savings and 6.17 % decoding time reduction. This framework outperforms existing methods, including fixed ladders, VMAF- and XPSNR-based dynamic resolution selection, and complexity-aware benchmarks. The results confirm that PF optimization with decoding time constraints enables sustainable, high-quality streaming tailored to network and device capabilities.",
      "authors": [
        "Angeliki Katsenou",
        "Vignesh V. Menon",
        "Guoda Laurinaviciute",
        "Benjamin Bross",
        "Detlev Marpe"
      ],
      "published": "2026-01-15T17:23:39+00:00",
      "updated": "2026-01-15T17:23:39+00:00",
      "category": "cs.CV",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.4,
      "fetched_at": "2026-01-18T15:37:18.614324",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10553v1",
      "title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models",
      "url": "http://arxiv.org/abs/2601.10553v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10553v1",
      "summary": "State-of-the-art video generative models produce promising visual content yet often violate basic physics principles, limiting their utility. While some attribute this deficiency to insufficient physics understanding from pre-training, we find that the shortfall in physics plausibility also stems from suboptimal inference strategies. We therefore introduce WMReward and treat improving physics plausibility of video generation as an inference-time alignment problem. In particular, we leverage the strong physics prior of a latent world model (here, VJEPA-2) as a reward to search and steer multiple candidate denoising trajectories, enabling scaling test-time compute for better generation performance. Empirically, our approach substantially improves physics plausibility across image-conditioned, multiframe-conditioned, and text-conditioned generation settings, with validation from human preference study. Notably, in the ICCV 2025 Perception Test PhysicsIQ Challenge, we achieve a final score of 62.64%, winning first place and outperforming the previous state of the art by 7.42%. Our work demonstrates the viability of using latent world models to improve physics plausibility of video generation, beyond this specific instantiation or parameterization.",
      "authors": [
        "Jianhao Yuan",
        "Xiaofeng Zhang",
        "Felix Friedrich",
        "Nicolas Beltran-Velez",
        "Melissa Hall",
        "Reyhane Askari-Hemmat",
        "Xiaochuang Han",
        "Nicolas Ballas",
        "Michal Drozdzal",
        "Adriana Romero-Soriano"
      ],
      "published": "2026-01-15T16:18:00+00:00",
      "updated": "2026-01-15T16:18:00+00:00",
      "category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.4,
      "fetched_at": "2026-01-18T15:37:18.614487",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10460v1",
      "title": "Contextual StereoSet: Stress-Testing Bias Alignment Robustness in Large Language Models",
      "url": "http://arxiv.org/abs/2601.10460v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10460v1",
      "summary": "A model that avoids stereotypes in a lab benchmark may not avoid them in deployment. We show that measured bias shifts dramatically when prompts mention different places, times, or audiences -- no adversarial prompting required.\n  We introduce Contextual StereoSet, a benchmark that holds stereotype content fixed while systematically varying contextual framing. Testing 13 models across two protocols, we find striking patterns: anchoring to 1990 (vs. 2030) raises stereotype selection in all models tested on this contrast (p<0.05); gossip framing raises it in 5 of 6 full-grid models; out-group observer framing shifts it by up to 13 percentage points. These effects replicate in hiring, lending, and help-seeking vignettes.\n  We propose Context Sensitivity Fingerprints (CSF): a compact profile of per-dimension dispersion and paired contrasts with bootstrap CIs and FDR correction. Two evaluation tracks support different use cases -- a 360-context diagnostic grid for deep analysis and a budgeted protocol covering 4,229 items for production screening.\n  The implication is methodological: bias scores from fixed-condition tests may not generalize.This is not a claim about ground-truth bias rates; it is a stress test of evaluation robustness. CSF forces evaluators to ask, \"Under what conditions does bias appear?\" rather than \"Is this model biased?\" We release our benchmark, code, and results.",
      "authors": [
        "Abhinaba Basu",
        "Pavan Chakraborty"
      ],
      "published": "2026-01-15T14:50:49+00:00",
      "updated": "2026-01-15T14:50:49+00:00",
      "category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.1,
      "fetched_at": "2026-01-18T15:37:15.265596",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10455v1",
      "title": "SurgGoal: Rethinking Surgical Planning Evaluation via Goal-Satisfiability",
      "url": "http://arxiv.org/abs/2601.10455v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10455v1",
      "summary": "Surgical planning integrates visual perception, long-horizon reasoning, and procedural knowledge, yet it remains unclear whether current evaluation protocols reliably assess vision-language models (VLMs) in safety-critical settings. Motivated by a goal-oriented view of surgical planning, we define planning correctness via phase-goal satisfiability, where plan validity is determined by expert-defined surgical rules. Based on this definition, we introduce a multicentric meta-evaluation benchmark with valid procedural variations and invalid plans containing order and content errors. Using this benchmark, we show that sequence similarity metrics systematically misjudge planning quality, penalizing valid plans while failing to identify invalid ones. We therefore adopt a rule-based goal-satisfiability metric as a high-precision meta-evaluation reference to assess Video-LLMs under progressively constrained settings, revealing failures due to perception errors and under-constrained reasoning. Structural knowledge consistently improves performance, whereas semantic guidance alone is unreliable and benefits larger models only when combined with structural constraints.",
      "authors": [
        "Ruochen Li",
        "Kun Yuan",
        "Yufei Xia",
        "Yue Zhou",
        "Qingyu Lu",
        "Weihang Li",
        "Youxiang Zhu",
        "Nassir Navab"
      ],
      "published": "2026-01-15T14:47:26+00:00",
      "updated": "2026-01-15T14:47:26+00:00",
      "category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.RO"
      ],
      "primary_category": "cs.CL",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.1,
      "fetched_at": "2026-01-18T15:37:15.265621",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10421v1",
      "title": "Are Language Models Models?",
      "url": "http://arxiv.org/abs/2601.10421v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10421v1",
      "summary": "Futrell and Mahowald claim LMs \"serve as model systems\", but an assessment at each of Marr's three levels suggests the claim is clearly not true at the implementation level, poorly motivated at the algorithmic-representational level, and problematic at the computational theory level. LMs are good candidates as tools; calling them cognitive models overstates the case and unnecessarily feeds LLM hype.",
      "authors": [
        "Philip Resnik"
      ],
      "published": "2026-01-15T14:13:01+00:00",
      "updated": "2026-01-15T14:13:01+00:00",
      "category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.1,
      "fetched_at": "2026-01-18T15:37:15.265641",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10387v1",
      "title": "The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models",
      "url": "http://arxiv.org/abs/2601.10387v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10387v1",
      "summary": "Large language models can represent a variety of personas but typically default to a helpful Assistant identity cultivated during post-training. We investigate the structure of the space of model personas by extracting activation directions corresponding to diverse character archetypes. Across several different models, we find that the leading component of this persona space is an \"Assistant Axis,\" which captures the extent to which a model is operating in its default Assistant mode. Steering towards the Assistant direction reinforces helpful and harmless behavior; steering away increases the model's tendency to identify as other entities. Moreover, steering away with more extreme values often induces a mystical, theatrical speaking style. We find this axis is also present in pre-trained models, where it primarily promotes helpful human archetypes like consultants and coaches and inhibits spiritual ones. Measuring deviations along the Assistant Axis predicts \"persona drift,\" a phenomenon where models slip into exhibiting harmful or bizarre behaviors that are uncharacteristic of their typical persona. We find that persona drift is often driven by conversations demanding meta-reflection on the model's processes or featuring emotionally vulnerable users. We show that restricting activations to a fixed region along the Assistant Axis can stabilize model behavior in these scenarios -- and also in the face of adversarial persona-based jailbreaks. Our results suggest that post-training steers models toward a particular region of persona space but only loosely tethers them to it, motivating work on training and steering strategies that more deeply anchor models to a coherent persona.",
      "authors": [
        "Christina Lu",
        "Jack Gallagher",
        "Jonathan Michala",
        "Kyle Fish",
        "Jack Lindsey"
      ],
      "published": "2026-01-15T13:40:06+00:00",
      "updated": "2026-01-15T13:40:06+00:00",
      "category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.1,
      "fetched_at": "2026-01-18T15:37:15.265715",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10715v1",
      "title": "DInf-Grid: A Neural Differential Equation Solver with Differentiable Feature Grids",
      "url": "http://arxiv.org/abs/2601.10715v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10715v1",
      "summary": "We present a novel differentiable grid-based representation for efficiently solving differential equations (DEs). Widely used architectures for neural solvers, such as sinusoidal neural networks, are coordinate-based MLPs that are both computationally intensive and slow to train. Although grid-based alternatives for implicit representations (e.g., Instant-NGP and K-Planes) train faster by exploiting signal structure, their reliance on linear interpolation restricts their ability to compute higher-order derivatives, rendering them unsuitable for solving DEs. Our approach overcomes these limitations by combining the efficiency of feature grids with radial basis function interpolation, which is infinitely differentiable. To effectively capture high-frequency solutions and enable stable and faster computation of global gradients, we introduce a multi-resolution decomposition with co-located grids. Our proposed representation, DInf-Grid, is trained implicitly using the differential equations as loss functions, enabling accurate modelling of physical fields. We validate DInf-Grid on a variety of tasks, including the Poisson equation for image reconstruction, the Helmholtz equation for wave fields, and the Kirchhoff-Love boundary value problem for cloth simulation. Our results demonstrate a 5-20x speed-up over coordinate-based MLP-based methods, solving differential equations in seconds or minutes while maintaining comparable accuracy and compactness.",
      "authors": [
        "Navami Kairanda",
        "Shanthika Naik",
        "Marc Habermann",
        "Avinash Sharma",
        "Christian Theobalt",
        "Vladislav Golyanik"
      ],
      "published": "2026-01-15T18:59:57+00:00",
      "updated": "2026-01-15T18:59:57+00:00",
      "category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.0,
      "fetched_at": "2026-01-18T15:37:11.929595",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    },
    {
      "id": "2601.10537v1",
      "title": "Enhancing the quality of gauge images captured in smoke and haze scenes through deep learning",
      "url": "http://arxiv.org/abs/2601.10537v1",
      "pdf_url": "https://arxiv.org/pdf/2601.10537v1",
      "summary": "Images captured in hazy and smoky environments suffer from reduced visibility, posing a challenge when monitoring infrastructures and hindering emergency services during critical situations. The proposed work investigates the use of the deep learning models to enhance the automatic, machine-based readability of gauge in smoky environments, with accurate gauge data interpretation serving as a valuable tool for first responders. The study utilizes two deep learning architectures, FFA-Net and AECR-Net, to improve the visibility of gauge images, corrupted with light up to dense haze and smoke. Since benchmark datasets of analog gauge images are unavailable, a new synthetic dataset, containing over 14,000 images, was generated using the Unreal Engine. The models were trained with an 80\\% train, 10\\% validation, and 10\\% test split for the haze and smoke dataset, respectively. For the synthetic haze dataset, the SSIM and PSNR metrics are about 0.98 and 43\\,dB, respectively, comparing well to state-of-the art results. Additionally, more robust results are retrieved from the AECR-Net, when compared to the FFA-Net. Although the results from the synthetic smoke dataset are poorer, the trained models achieve interesting results. In general, imaging in the presence of smoke are more difficult to enhance given the inhomogeneity and high density. Secondly, FFA-Net and AECR-Net are implemented to dehaze and not to desmoke images. This work shows that use of deep learning architectures can improve the quality of analog gauge images captured in smoke and haze scenes immensely. Finally, the enhanced output images can be successfully post-processed for automatic autonomous reading of gauges",
      "authors": [
        "Oscar H. Ram\u00edrez-Agudelo",
        "Akshay N. Shewatkar",
        "Edoardo Milana",
        "Roland C. Aydin",
        "Kai Franke"
      ],
      "published": "2026-01-15T15:59:12+00:00",
      "updated": "2026-01-15T15:59:12+00:00",
      "category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "source": "arxiv",
      "source_priority": "high",
      "score": 7.0,
      "fetched_at": "2026-01-18T15:37:18.614534",
      "created_at": "2026-01-18 10:07:18",
      "stars": 0,
      "forks": 0,
      "watchers": 0,
      "open_issues": 0,
      "language": null,
      "topics": null,
      "license": null,
      "languages": null,
      "contributors_count": 0,
      "owner_type": null,
      "stars_per_day": 0.0,
      "forks_per_day": 0.0,
      "activity_score": 0.0,
      "days_since_creation": 0,
      "days_since_update": 0,
      "is_recently_active": 0,
      "has_wiki": 0,
      "has_pages": 0,
      "has_discussions": 0
    }
  ],
  "content": [
    {
      "id": 5,
      "paper_id": "2601.10712v1",
      "content_type": "linkedin",
      "content": "MatchTIR introduces a reinforcement learning framework for Tool-Integrated Reasoning that uses bipartite matching to assign turn-level rewards, addressing uniform credit assignment in existing methods.\n\n- Reformulates credit assignment as bipartite matching between predicted and ground-truth tool traces, with hard (Hungarian algorithm) and soft (optimal transport) strategies.\n- Employs dual-level advantage estimation combining turn-level and trajectory-level signals for balanced local and global optimization.\n- A 4B model outperforms most 8B baselines on three benchmarks, with gains scaling on long-horizon tasks and reduced tool invocation failures.\n\nPractical takeaway: Fine-grained supervision enables smaller models to match larger ones, lowering deployment costs for agentic systems.\n\n\ud83d\udcce Read more: http://arxiv.org/abs/2601.10712v1\nvia arxiv\n\n#AI #DeepLearning #MachineLearning #Research",
      "analysis": {
        "item_id": "2601.10712v1",
        "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
        "url": "http://arxiv.org/abs/2601.10712v1",
        "source": "arxiv",
        "fact_extraction": "## Core Contribution\n\nMatchTIR introduces a **fine-grained reinforcement learning framework for Tool-Integrated Reasoning (TIR)** that replaces uniform credit assignment with turn-level reward assignment via bipartite matching[1][2]. The core innovation addresses a fundamental limitation in existing RL methods: they assign identical advantages to all steps in a trajectory, failing to distinguish effective tool calls from redundant or erroneous ones in long-horizon tasks[1].\n\n## Technical Details\n\n**Methods:**\n- **Bipartite matching formulation:** Credit assignment reformulated as an optimal alignment problem between predicted and ground-truth tool call traces[1][2]\n- **Two matching strategies:** Hard Assignment (Kuhn-Munkres algorithm) for strict one-to-one mapping and Soft Assignment (Optimal Transport) for probabilistic alignment[1]\n- **Matching matrix construction:** Scoring system with three components comparing predicted tools against ground-truth tools[2]\n- **Dual-level advantage estimation:** Integrates turn-level rewards (individual step contribution via discounted rewards) with trajectory-level signals (overall task quality) to balance local precision and global success[1][2]\n\n**Models and Scale:**\n- 4B parameter model evaluated as primary contribution[1][2]\n- Compared against 8B competitors[1][2]\n\n**Datasets/Benchmarks:**\n- Three benchmarks evaluated (specific names not detailed in abstracts, though FTRL mentioned as in-domain benchmark)[2]\n- In-domain and out-of-domain evaluation[2]\n- \"Hard subset\" mentioned for complexity scaling analysis[2]\n\n**Metrics:**\n- Task-level performance improvements[2]\n- Tool invocation frequency reduction[2]\n- Failure rate reduction[2]\n\n## Explicit Claims\n\n- 4B model **surpasses the majority of 8B competitors**, particularly on long-horizon and multi-turn tasks[1][2]\n- Performance gains **scale with task complexity**, with most significant improvements on hard scenarios[2]\n- Fine-grained supervision reduces tool invocation frequency and failure rates, leading to more selective and accurate tool usage[2]\n- Dual-level advantage estimation consistently outperforms single-level approaches (turn-level or trajectory-level alone)[2]\n- Method demonstrates **effectiveness and robustness** across in-domain and out-of-domain benchmarks[2]\n- Code, datasets, and model checkpoints are fully open-sourced[1]\n\n## Open Questions / Limitations\n\n**Not explicitly claimed or unclear:**\n- Specific benchmark names and their characteristics (only FTRL explicitly named)[2]\n- Exact performance metrics (absolute numbers, percentage improvements not provided in abstracts)\n- Computational cost comparison versus baseline methods\n- Scalability to larger model sizes (only 4B vs 8B comparison shown)\n- Generalization to tool domains beyond those in evaluation\n- Sensitivity analysis for matching strategy selection (hard vs. soft assignment trade-offs)\n- Wall-clock training time or convergence speed improvements\n- Ablation study details beyond the statement that dual-level advantage estimation outperforms single-level approaches[2]\n- Whether the method requires ground-truth traces during inference or only training\n- Performance on out-of-domain tasks relative to in-domain (mentioned but not quantified)[2]",
        "engineer_summary": "**Problem:** Existing RL methods for Tool-Integrated Reasoning assign uniform advantages across entire trajectories, failing to distinguish effective tool calls from redundant or erroneous ones in long-horizon scenarios.[1]\n\n**Solution:** MatchTIR formulates credit assignment as a bipartite matching problem between predicted and ground-truth tool traces.[1] It employs two matching strategies\u2014hard assignment (Hungarian algorithm) and soft assignment (optimal transport)\u2014to generate dense turn-level rewards.[2] A dual-level advantage estimation scheme integrates turn-level and trajectory-level signals, balancing local step precision with global task success.[1]\n\n**Evidence:** Evaluated on three benchmarks, a 4B MatchTIR model outperforms most 8B baselines, with pronounced gains on long-horizon and multi-turn tasks.[1] The method reduces tool invocation frequency and failure rates, indicating more selective and accurate tool usage.[2] Ablations confirm that combining turn-level and trajectory-level advantages yields superior performance compared to either signal alone.[2]",
        "impact_analysis": "## Immediate implications:\n\n**Model efficiency gains**: MatchTIR enables smaller models (4B parameters) to outperform larger competitors (8B) on tool-use tasks[1][2]. This directly reduces computational costs and deployment overhead for organizations building agentic systems, since fewer parameters mean lower inference latency and memory requirements while maintaining or exceeding performance.\n\n**Reduced tool invocation failures**: The framework significantly lowers failure rates and reduces overall tool invocation frequency[1][3]. In production systems, this translates to fewer erroneous API calls, reduced latency from failed interactions, and lower costs when tools charge per invocation (e.g., external APIs, database queries).\n\n**Better performance on complex reasoning**: The method shows pronounced improvements on long-horizon, multi-turn tasks[1][2]. For practical applications like autonomous research agents, code generation systems, or multi-step planning tasks, this addresses a critical bottleneck where existing methods degrade substantially.\n\n## Long-term implications:\n\n**Scalable agentic AI**: Fine-grained credit assignment could become foundational for training more capable autonomous agents. As tool ecosystems expand (APIs, databases, specialized services), the ability to precisely optimize which tools to invoke and when becomes increasingly valuable for building reliable, cost-effective systems.\n\n**Transfer to other domains**: The bipartite matching formulation for credit assignment is domain-agnostic. Beyond tool-use, this approach could generalize to other sequential decision-making problems where distinguishing high-value actions from noise is critical\u2014e.g., dialogue systems, planning, or multi-agent coordination.\n\n## Practical constraints:\n\n**Requires ground-truth traces**: MatchTIR depends on access to optimal or expert tool-call sequences during training[1][4]. In domains where ground-truth reasoning paths are expensive to obtain or ambiguous (e.g., open-ended research tasks), this supervision requirement becomes a bottleneck. The method cannot easily adapt to scenarios where multiple valid solution paths exist.\n\n**Computational overhead during training**: Constructing bipartite matching matrices and computing dual-level advantages adds training-time complexity compared to simpler outcome-based rewards. While inference efficiency improves, the training cost trade-off is not quantified in the available results.",
        "application_mapping": "## Application Scenario 1: Automated Financial Compliance Auditing\n\n**Why this work helps:**\nMatchTIR addresses a critical bottleneck in deploying tool-integrated reasoning for compliance: distinguishing between necessary tool calls (database queries, regulatory lookups, calculations) and redundant or incorrect ones in multi-step audit workflows[1][4]. Traditional reinforcement learning assigns uniform credit across all steps, but compliance audits involve long sequences where a single erroneous tool invocation (e.g., querying the wrong regulatory database) can invalidate downstream reasoning. MatchTIR's fine-grained, turn-level reward assignment enables the model to learn which specific tool interactions contribute to correct compliance determinations, improving reliability in high-stakes financial environments[1].\n\n**Assumptions / prerequisites:**\n- Access to labeled audit traces with ground-truth tool sequences (which tools should be called, in what order)\n- Defined tool set (regulatory databases, calculation engines, document retrieval systems)\n- Tolerance for model training on domain-specific compliance data before deployment\n- Human oversight of initial model outputs during rollout\n\n## Application Scenario 2: Multi-Step Scientific Research Validation\n\n**Why this work helps:**\nScientific reasoning often requires extended sequences of tool interactions\u2014running simulations, querying databases, performing calculations, then validating results[1][2]. MatchTIR's dual-level advantage estimation (turn-level + trajectory-level) is particularly suited here: it can credit individual tool calls that contribute to intermediate correctness while still optimizing for final experimental validity. This prevents the model from learning spurious tool-calling patterns that happen to produce correct final answers by chance, a critical distinction in reproducible research[1][5].\n\n**Assumptions / prerequisites:**\n- Curated dataset of correct multi-turn scientific workflows with annotated tool sequences\n- Integration with domain-specific tools (Python computation, theorem provers, scientific databases)\n- Ability to validate intermediate reasoning steps, not just final outputs\n- Computational resources for training on benchmark datasets (GPQA or similar) before domain application",
        "blog_synthesis": "# MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching\n\n## Context and Background\n\nTool-Integrated Reasoning (TIR) has emerged as a critical capability for large language models, enabling them to solve complex tasks by interleaving reasoning steps with external tool interactions\u2014querying databases, calling APIs, performing calculations, and retrieving information[1]. This paradigm powers autonomous agents, research systems, and multi-step planning applications.\n\nHowever, existing reinforcement learning methods for TIR rely on coarse-grained credit assignment mechanisms. When training these systems, methods like Group Relative Policy Optimization (GRPO) assign uniform advantages to all steps within a reasoning trajectory based on outcome-level or trajectory-level rewards[4]. This \"one-size-fits-all\" approach fails to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon, multi-turn scenarios where a single incorrect tool invocation can propagate errors downstream[1][4]. The result is inefficient policy optimization and suboptimal tool-use strategies that waste computational resources and reduce reliability.\n\n## What Is New in This Work\n\nMatchTIR introduces a **fine-grained reinforcement learning framework that replaces uniform credit assignment with turn-level reward assignment via bipartite matching**[1][2]. Rather than assigning identical advantages to all steps in a trajectory, the framework formulates credit assignment as an optimal alignment problem between predicted tool calls and ground-truth tool sequences[1].\n\nThe core innovation addresses a fundamental bottleneck: existing methods cannot precisely attribute which individual tool interactions contribute to task success. MatchTIR solves this by treating credit assignment as a structured matching problem, enabling the model to learn which specific tool calls are valuable versus redundant or harmful[1][4].\n\n## Technical Explanation\n\n### Bipartite Matching Formulation\n\nMatchTIR reformulates turn-level credit assignment as a bipartite matching problem between two sets: a set of *m* predicted tool calls \\(\\mathcal{P} = \\{p_1, \\dots, p_m\\}\\) from a rollout trajectory and a set of *n* ground-truth tool calls \\(\\mathcal{G} = \\{g_1, \\dots, g_n\\}\\)[1][4]. The framework constructs a weighted bipartite graph where each edge represents the similarity between a predicted tool call and a ground-truth reference.\n\nThe matching matrix \\(S \\in \\mathbb{R}^{m \\times n}\\) quantifies alignment between predicted and ground-truth tools using three components[4]:\n\n- **Tool Name Matching:** An indicator function returning 1 if tool names match, 0 otherwise\n- **Parameter Name Matching:** Similarity scoring for parameter alignment\n- **Parameter Content Matching:** Semantic similarity of parameter values\n\n### Two Assignment Strategies\n\nMatchTIR employs two strategies to convert the similarity matrix into dense turn-level rewards[1][4]:\n\n**Hard Credit Assignment (Hungarian Algorithm).** This strategy enforces strict one-to-one alignment, solving a maximum weight bipartite matching problem to find binary assignments \\(x_{ij} \\in \\{0, 1\\}\\) that maximize total matching score while ensuring each predicted and ground-truth tool is matched at most once[4]. This approach is deterministic and interpretable.\n\n**Soft Credit Assignment (Optimal Transport).** This strategy allows probabilistic alignment, enabling partial credit assignment when multiple valid tool sequences exist. This is particularly useful in domains where multiple reasoning paths are equally valid[1].\n\n### Dual-Level Advantage Estimation\n\nA critical component of MatchTIR is its **dual-level advantage estimation mechanism**, which balances local step precision with global task success[1][4]. Rather than relying solely on turn-level rewards (which may optimize for locally correct steps that don't contribute to task completion) or trajectory-level rewards (which fail to distinguish individual step quality), the framework integrates both signals[1].\n\nFor each rollout in a group of rollouts, a trajectory-level reward is defined as the sum of turn-level rewards, with the final turn's reward being the outcome-level reward reflecting overall task success[4]. This dual-level design ensures that the model learns to execute correct intermediate steps while still optimizing for final task completion.\n\n## Practical Relevance\n\n### Model Efficiency\n\nA 4-parameter billion MatchTIR model **surpasses the majority of 8-parameter billion competitors**, particularly on long-horizon and multi-turn tasks[1][2]. This efficiency gain directly reduces computational costs for organizations deploying agentic systems, since smaller models require lower inference latency and memory overhead while maintaining or exceeding performance.\n\n### Reduced Tool Invocation Failures\n\nThe framework significantly reduces tool invocation frequency and failure rates[1][4]. In production systems, this translates to fewer erroneous API calls, reduced latency from failed interactions, and lower operational costs when tools charge per invocation\u2014critical considerations for systems relying on expensive external services.\n\n### Scaling with Task Complexity\n\nPerformance improvements become more pronounced on harder tasks requiring more tool invocations[4]. This addresses a critical bottleneck: existing methods degrade substantially as task complexity increases, but MatchTIR maintains effectiveness across difficulty levels.\n\n### Structured Tool Environments\n\nMatchTIR exploits the structured nature of tool interactions\u2014tool names, parameter names, and parameter contents are verifiable and unambiguous[1]. This makes the approach particularly suited to domains with well-defined tool APIs (databases, calculation engines, specialized services) rather than open-ended reasoning where multiple valid solution paths exist.\n\n## Limitations and Open Questions\n\n**Ground-Truth Supervision Requirement.** MatchTIR depends on access to optimal or expert tool-call sequences during training[1][4]. In domains where ground-truth reasoning paths are expensive to obtain or inherently ambiguous\u2014such as open-ended research or creative problem-solving\u2014this supervision requirement becomes a practical bottleneck. The method cannot easily adapt to scenarios where multiple valid solution paths exist with equal validity.\n\n**Training Computational Overhead.** Constructing bipartite matching matrices and computing dual-level advantages adds training-time complexity compared to simpler outcome-based rewards. The available results do not quantify this training cost trade-off or provide wall-clock time comparisons against baseline methods.\n\n**Scalability to Larger Models.** Evaluation focuses on 4-parameter billion models compared against 8-parameter billion baselines. Scalability to larger model sizes (13B, 70B+) and whether the approach maintains effectiveness at different scales remains unclear.\n\n**Out-of-Domain Generalization.** While the paper mentions evaluation on out-of-domain benchmarks, specific performance metrics comparing in-domain versus out-of-domain results are not provided in available abstracts[2]. The extent to which fine-grained supervision learned on one tool domain transfers to unseen tools is an open question.\n\n**Inference-Time Requirements.** It is unclear whether the method requires ground-truth traces only during training or also during inference for certain operations, which would impact deployment feasibility.\n\n## Conclusion\n\nMatchTIR addresses a fundamental limitation in reinforcement learning for tool-integrated reasoning: the inability to distinguish effective tool calls from redundant or erroneous ones in long-horizon tasks. By formulating credit assignment as a bipartite matching problem and introducing dual-level advantage estimation, the framework enables fine-grained, turn-level supervision that scales with task complexity[1][2].\n\nThe practical implications are significant: smaller models achieve superior performance on tool-use tasks, tool invocation failures decrease, and reasoning quality improves on complex multi-turn scenarios[1][4]. For engineers building autonomous systems, this work provides a concrete mechanism to improve both efficiency and reliability.\n\nHowever, the approach's dependence on ground-truth tool sequences and its applicability primarily to structured tool environments define its scope. The framework is most valuable in domains with well-defined APIs and available expert demonstrations\u2014financial compliance, scientific workflows, database-driven reasoning\u2014rather than open-ended reasoning tasks. Future work addressing out-of-domain generalization, training efficiency, and scalability to larger models would strengthen the practical impact of this contribution.",
        "linkedin_formatting": "**MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning**\n\nResearchers have introduced MatchTIR, a framework that improves how language models learn to use tools by assigning precise credit to individual steps rather than treating entire reasoning sequences uniformly.\n\nKey advances:\n- Formulates credit assignment as a bipartite matching problem between predicted and ground-truth tool calls\n- Introduces dual-level advantage estimation combining turn-level and trajectory-level signals\n- 4B model outperforms most 8B baselines on long-horizon tasks with fewer tool invocations\n\nThe practical takeaway: smaller, more efficient models can now match larger competitors on complex multi-step reasoning by learning which specific tool interactions actually matter.",
        "credibility_check": "Error: 'generated_output'"
      },
      "file_path": "data/drafts/linkedin/2601.10712v1.txt",
      "status": "drafted",
      "published_url": null,
      "created_at": "2026-01-18 16:34:31",
      "published_at": null,
      "paper_title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
      "paper_url": "http://arxiv.org/abs/2601.10712v1",
      "paper_source": "arxiv"
    },
    {
      "id": 4,
      "paper_id": "2601.10681v1",
      "content_type": "linkedin",
      "content": "# LinkedIn Post\n\n**Researchers at Bravada Group and Eye Dream Pty Ltd have introduced \"context bubbles,\" a framework that improves how retrieval-augmented generation systems assemble information from enterprise documents.**\n\n**Key advances:**\n\u2022 Preserves document structure (sections, rows) to eliminate fragmentation and redundancy in traditional top-k retrieval\n\u2022 Balances relevance, coverage, and diversity under strict token budgets, reducing wasted context window space\n\u2022 Produces auditable retrieval traces\u2014critical for regulated environments where answers must be traceable\n\n**The result:** Experiments on enterprise documents show improved answer quality, citation faithfulness, and better coverage of secondary query facets\u2014all within tighter token constraints.\n\n**Takeaway:** As LLM context windows remain a bottleneck, intelligent context assembly matters more than raw retrieval ranking.\n\n#RAG #LLM #EnterpriseAI #InformationRetrieval #StructuredData\n\n\ud83d\udcce Read more: http://arxiv.org/abs/2601.10681v1\nvia arxiv\n\n#AI #DeepLearning #MachineLearning #Research",
      "analysis": {
        "item_id": "2601.10681v1",
        "title": "Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems",
        "url": "http://arxiv.org/abs/2601.10681v1",
        "source": "arxiv",
        "fact_extraction": "# Core Contribution\n\nThe paper introduces **context bubble construction**, a framework for assembling coherent, auditable context packs for retrieval-augmented generation (RAG) systems that addresses fragmentation, redundancy, and incomplete coverage in traditional top-k passage retrieval[1][3]. The method combines document structure awareness with explicit diversity constraints under strict token budgets.\n\n# Technical Details\n\n**Method:**\n- Starts from high-relevance anchor spans and expands through constrained selection[1][3]\n- Balances query relevance, marginal coverage, and redundancy penalties[1][3]\n- Organizes multi-granular spans (sections, rows) using task-conditioned structural priors[1]\n- Implements diversity through overlap-aware selection ensuring complementary information[1]\n- Enforces strict token budgets and per-section budget constraints[1]\n\n**Selection Process:**\n- Explicit gating mechanisms based on relevancy, structural role, redundancy, and budget constraints[1]\n- Full retrieval trace emitted documenting scoring and selection choices[1][3]\n\n**Evaluation:**\n- Tested on real-world enterprise documents (multi-sheet Excel workbooks, job quote documents)[1]\n- Metrics: token usage reduction, structural coverage, answer correctness, citation faithfulness[1][2]\n- Ablation studies comparing against flat top-k retrieval and diversity-only baselines[1]\n\n**Key Finding:**\nBoth structural priors and diversity constraint selection are necessary; removing either degrades coverage and increases redundancy[1]\n\n# Explicit Claims\n\n- Context bubbles **significantly reduce redundant context** compared to top-k retrieval[1][2]\n- Better coverage of **secondary facets and 2nd/3rd order query aspects**[1][2]\n- **Improved answer quality and citation faithfulness** within limited context windows[1][2]\n- Provides **full auditability** with transparent, inspectable reasons for inclusion/exclusion of chunks[1]\n- Enables **deterministic tuning** through complete selection trace[1][3]\n- Achieves **improved efficiency** and answer quality over standard RAG baselines in enterprise settings[1]\n\n# Open Questions / Limitations\n\n**Not explicitly addressed:**\n- Specific model architectures or LLM versions tested[1]\n- Quantitative metrics (e.g., exact token reduction percentages, answer quality scores)[1]\n- Computational overhead of the constrained selection pipeline versus standard retrieval[1]\n- Generalization beyond enterprise documents to other domains[1]\n- Comparison against other structure-aware or diversity-based retrieval methods beyond top-k and diversity-only baselines[1]\n- How task-conditioned structural priors are learned or specified[1]\n- Scale of experiments (number of documents, queries, or document sizes)[1]\n- Whether the approach handles documents with inconsistent or irregular structure[1]",
        "engineer_summary": "**Context bubble construction** addresses RAG limitations in enterprise settings: information graph fragmentation, content duplication, over-retrieval, and poor coverage of 2nd/3rd-order query facets from structured documents (e.g., multi-sheet Excel).[1][3]\n\nUnlike top-k passage ranking, it preserves document structure via multi-granular spans (sections, rows) and task-conditioned priors, building from high-relevance anchors through explicit gating on relevance, marginal coverage, lexical overlap redundancy penalties, per-section budgets, and token limits. Outputs auditable traces of scoring/selection for deterministic tuning.[1][2][3]\n\nEnterprise document experiments show reduced redundancy, improved secondary facet coverage, answer quality, and citation faithfulness. Ablations confirm necessity of structural priors and diversity constraints; removing either degrades coverage and increases incompleteness.[1][3]\n\n(98 words)",
        "impact_analysis": "## Immediate implications:\n\n**Token efficiency and cost reduction in production RAG systems.** Traditional top-k retrieval wastes context window space through redundancy and fragmentation, forcing either smaller token budgets or fewer retrieved documents. Context Bubbles directly addresses this by reducing redundant context while maintaining coverage, allowing enterprises to process longer documents or more complex queries within fixed LLM context limits\u2014a tangible operational constraint given that context window size directly correlates with inference cost.[1][2]\n\n**Improved answer quality and citation accuracy.** Experiments demonstrate that the method achieves better answer correctness and citation faithfulness compared to flat top-k retrieval.[1][2] For enterprise applications where answers must be traceable to source documents (legal discovery, compliance, financial analysis), this directly reduces the risk of hallucinated or poorly grounded responses.\n\n**Auditability for regulated environments.** The framework emits a full retrieval trace documenting scoring and selection choices, enabling deterministic tuning and transparency.[1][2] This is critical for enterprises operating under regulatory scrutiny where decision-making systems must be explainable and reproducible.\n\n## Long-term implications:\n\n**Scalability to complex document ecosystems.** The method's ability to handle multi-granular document structures (sections, rows, sheets) suggests potential application beyond simple text corpora to semi-structured enterprise data\u2014spreadsheets, databases, and hierarchical document systems that are common in real organizations.[2] As enterprises accumulate larger document collections, structure-aware retrieval becomes increasingly valuable for maintaining coherence.\n\n**Foundation for more reliable LLM applications in knowledge-intensive domains.** The authors frame this as \"a pathway towards more reliable and accurate LLM applications in complex domains, where comprehensive and well-structured information is critical for generating trustworthy outputs.\"[1] This positions context construction as a fundamental lever for improving LLM reliability beyond prompt engineering or model scaling.\n\n## Practical constraints:\n\n**Limited evaluation scope.** The authors acknowledge that experiments used \"specific document types\" (enterprise Excel workbooks and job scope documents) and suggest \"future work could explore the application of this framework to a wider range of corpora and tasks.\"[1] Generalization to other document structures, languages, or domains remains unvalidated.\n\n**Structural priors require domain specification.** The method relies on \"task-conditioned structural priors\" to guide selection.[2] This means the system must be configured with knowledge of document structure for each new document type or domain\u2014it is not a plug-and-play solution. Organizations with heterogeneous document formats may face implementation complexity.\n\n**Trade-off between coverage and token budget.** While the method balances these constraints, the underlying tension remains: achieving comprehensive coverage of secondary query facets may still require larger token allocations for certain complex queries, limiting the universality of the efficiency gains.[2]",
        "application_mapping": "### Application scenario 1: Enterprise knowledge management for querying structured technical reports\nEmployees query internal documents (e.g., PDFs with sections, tables) for information on company policies or technical specs, using the context bubble to retrieve coherent bundles of spans like sections and rows.\n\n**Why this work helps:**  \nIt preserves document structure to avoid fragmentation and duplication from top-k retrieval, enforces diversity constraints for better coverage of secondary facets (e.g., 2nd/3rd order details), reduces redundant context under token budgets, and improves answer quality with auditable citations, as shown in experiments on enterprise documents[1][3][4].\n\n**Assumptions / prerequisites:**  \n- Documents have inherent structure (e.g., sections, tables) parseable into multi-granular spans.  \n- Task-conditioned structural priors are predefined for the domain.  \n- Strict token budget enforced by the LLM deployment.  \n- Integration with existing RAG pipelines for anchor span retrieval.\n\n### Application scenario 2: Legal or financial document review in compliance workflows\nAnalysts query regulatory guidelines or contracts structured as reports, retrieving context bubbles that bundle relevant spans with traceability for decision-making.\n\n**Why this work helps:**  \nConstrained selection balances relevance, coverage, and redundancy, producing compact contexts that cover facets better than top-k methods, with full audit trails for scoring/selection, enhancing citation faithfulness in limited windows\u2014directly addressing enterprise needs like scattered docs[1][3][4].\n\n**Assumptions / prerequisites:**  \n- Documents exhibit hierarchical structure exploitable by the framework.  \n- High-relevance anchors identifiable via initial retriever.  \n- Diversity and budget constraints tuned deterministically.  \n- Deployment in environments requiring auditable, non-hallucinated outputs (e.g., compliance tools).",
        "blog_synthesis": "# Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems\n\n## 1. Context and Background\n\nRetrieval-augmented generation (RAG) systems integrate large language models (LLMs) with external knowledge retrieval to mitigate hallucinations and improve response grounding in knowledge-intensive tasks.[1][2][5] Traditional RAG pipelines retrieve and rank passages using lexical or dense similarity, selecting the top-k for injection into the LLM context window.[2][3] This approach assumes higher-ranked passages directly translate to better generation quality, but it falters on structured enterprise documents such as multi-sheet Excel workbooks or job quote reports.[1][2][3]\n\nKey limitations include fragmentation of information graphs across document structures, over-retrieval leading to token waste, content duplication, and insufficient coverage of secondary (2nd/3rd-order) query facets.[1][2][3] For instance, top-k retrieval may pull disjoint spans from related sections or rows, disrupting coherence and exceeding strict token budgets imposed by LLM context limits.[1][2] These issues are pronounced in enterprise settings, where documents exhibit hierarchical structures (e.g., sections, sheets, tables) and applications demand auditable, traceable outputs for compliance or decision support.[1][3]\n\nThe paper addresses this by reframing context assembly as an optimization problem: constructing compact, coherent \"context bubbles\" that preserve structure, enforce diversity, and fit token constraints while enabling full auditability.[1][2][3]\n\n## 2. What is New in This Work\n\nThe core innovation is **context bubble construction**, a structure-informed, diversity-constrained framework that assembles citable bundles of multi-granular spans (e.g., sections, rows) from high-relevance anchors.[1][2][3] Unlike flat top-k retrieval, it explicitly balances query relevance, marginal coverage gains, redundancy penalties (via lexical overlap), per-section budgets, and global token limits through overt gating mechanisms.[1][2]\n\nNovel elements include:\n- **Task-conditioned structural priors** to organize spans hierarchically, exploiting document geometry for coherent expansion.[1][2]\n- **Constrained selection pipeline** starting from anchor spans, with diversity enforcement to cover complementary facets and avoid duplication.[1][3]\n- **Full retrieval trace emission**, logging all scoring and gating decisions for deterministic tuning and inspection\u2014critical for enterprise trust.[1][2][3]\n\nAblation studies on enterprise documents confirm the synergy: removing structural priors increases fragmentation, while omitting diversity constraints boosts redundancy; both degrade coverage and answer quality.[1][3]\n\n## 3. Technical Explanation (High-Level, Accurate)\n\nThe system is a modular pipeline transforming raw enterprise documents into auditable context packs.[2] It comprises five stages: document parsing into multi-granular spans, initial anchor retrieval, candidate scoring, constrained selection, and trace generation.[1][2]\n\n**Anchor Retrieval and Expansion.** High-relevance spans are identified via standard retrievers (lexical/dense), serving as seeds.[1][2][3] Expansion draws from structurally related candidates (e.g., parent sections, sibling rows) guided by task-conditioned priors\u2014predefined rules encoding document roles like \"summary sections prioritize over appendices.\"[1][2]\n\n**Scoring and Gating.** Candidates receive scores for:\n- Query relevance (similarity to anchors).[1]\n- Marginal coverage (novel information gain).[1][3]\n- Redundancy penalty (lexical overlap with selected spans).[1][2]\n\nOvert gates then filter:\n- **Token budget**: Global and per-section limits prevent overflow.[2]\n- **Diversity**: Overlap thresholds exclude near-duplicates.[1][3]\n- **Structural fit**: Priors penalize misaligned granularities.[1]\n\nThe result is a compact context bubble: a ranked, citable span bundle fitting the LLM window, with provenance traces (e.g., \"Span X excluded: 80% overlap with Y, exceeds row budget\").[1][2][3]\n\nPseudocode outline:\n\n```\ndef construct_context_bubble(query, doc_spans, token_budget):\n    anchors = retrieve_anchors(query, doc_spans)  # High-relevance seeds\n    candidates = expand_structurally(anchors, priors)  # Multi-granular relatives\n    bubble = []\n    trace = []\n    for cand in rank_candidates(candidates, query):  # Relevance + coverage scores\n        if passes_gates(cand, bubble, token_budget, overlap_threshold):\n            bubble.append(cand)\n            trace.append(f\"Added {cand.id}: score={cand.score}, reason=relevant\")\n        else:\n            trace.append(f\"Rejected {cand.id}: {gate_reason}\")\n    return bubble, trace\n```\n\nThis ensures deterministic outputs, unlike probabilistic top-k sampling.[1][2]\n\nEvaluations used enterprise corpora (Excel workbooks, job quotes), measuring token efficiency, structural coverage, answer correctness, and citation faithfulness. Context bubbles reduced redundancy, improved secondary facet coverage, and boosted quality over baselines.[1][2][3]\n\n## 4. Practical Relevance\n\nFor software engineers building RAG pipelines, context bubbles offer plug-in efficiency gains under fixed context windows, directly cutting inference costs (tokens correlate with compute).[1][2] In enterprise knowledge management\u2014querying technical reports or policies\u2014the method bundles coherent sections/tables, avoiding fragmented top-k outputs and enhancing LLM attention focus.[1][3]\n\n**Scenario 1: Structured Report QA.** Employees query multi-section PDFs; bubbles retrieve anchor paragraphs plus covering rows/sections, yielding precise, cited answers with 2nd-order details (e.g., exceptions, cross-references).[1][3]\n\n**Scenario 2: Compliance Workflows.** Legal/financial analysts search contracts; auditable traces justify inclusions/exclusions, supporting traceable decisions in regulated environments.[1][2]\n\nIntegration requires: span parser for document types, prior definitions per domain, and retriever hooks. Ablations validate gains over top-k and diversity-only baselines, positioning it for production where structure is parseable.[1][3]\n\n## 5. Limitations and Open Questions\n\nThe paper evaluates on specific enterprise formats (Excel, job quotes), leaving generalization to irregular structures, other languages, or domains untested.[1][2] Quantitative details\u2014exact token reductions, F1 scores, dataset scales (queries/documents)\u2014are absent, hindering reproducibility.[1]\n\nUnspecified aspects include:\n- LLM/backbone models used.[1]\n- Computational overhead of gating vs. top-k.[1]\n- Prior learning/specification (manual? learned?).[1][2]\n- Handling paraphrasing (lexical overlap may miss semantic redundancy).[1]\n- Comparisons to advanced baselines (e.g., other structure-aware retrievers).[1]\n\nTask-conditioned priors demand domain tuning, complicating heterogeneous deployments.[1][2] Non-deterministic chunking risks inconsistency; future multimodal/hybrid retrieval is suggested.[1]\n\n## 6. Conclusion\n\nContext bubble construction advances RAG by systematically addressing structure fragmentation and redundancy in enterprise documents, delivering efficient, auditable contexts that enhance answer quality and faithfulness.[1][2][3] Engineers can leverage its explicit gates for tunable pipelines, though domain priors and eval breadth limit immediate universality. This framework underscores structure and diversity as essential signals, paving structured paths to reliable LLM applications in complex domains.[1][3]\n\n(Word count: 912)",
        "linkedin_formatting": "Introduced structure- and diversity-aware **context bubble construction** for RAG systems, addressing fragmentation, redundancy, and incomplete coverage in enterprise documents.[1][2][3]\n\nKey advances:\n- Builds from high-relevance anchors via constrained selection balancing relevance, marginal coverage, and redundancy penalties.[1][2]\n- Preserves multi-granular structure (sections, rows) with task-conditioned priors and strict token budgets.[1][3]\n- Emits full audit traces for deterministic tuning and citation faithfulness.[2][3]\n\nEnterprise experiments show reduced redundancy, better secondary facet coverage, and improved answer quality over top-k retrieval.[1][2]\n\n**Takeaway:** Auditability enables reliable tuning in regulated domains.\n\n#RAG #RetrievalAugmentedGeneration #ContextConstruction #EnterpriseAI #LLM",
        "credibility_check": "Error: 'generated_output'"
      },
      "file_path": null,
      "status": "drafted",
      "published_url": null,
      "created_at": "2026-01-18 10:11:46",
      "published_at": null,
      "paper_title": "Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems",
      "paper_url": "http://arxiv.org/abs/2601.10681v1",
      "paper_source": "arxiv"
    },
    {
      "id": 3,
      "paper_id": "2601.10681v1",
      "content_type": "blog",
      "content": "---\nlayout: post\ntitle: \"Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems\"\ndate: 2026-01-18\ncategories: [AI, Research]\ntags:\n  - AI\n  - Machine Learning\n  - Research\nauthor: AI Research Publisher\nsource: arxiv\nsource_url: http://arxiv.org/abs/2601.10681v1\n---\n\n# Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems\n\n## Context and Background\n\nRetrieval-augmented generation (RAG) has become a standard architectural pattern for grounding large language models in external knowledge sources[4][7]. The approach works by retrieving relevant passages from a document collection and injecting them into the LLM's context window to condition generation. However, traditional RAG systems rely on simple top-k passage ranking, which treats documents as flat collections of independent passages without regard for document structure or information coherence[1][2].\n\nThis limitation becomes acute when working with structured enterprise documents\u2014multi-sheet spreadsheets, hierarchical reports, or documents with sections and tables. Top-k retrieval often fragments information across document boundaries, duplicates content, and misses secondary query facets that require understanding relationships between document sections[1][2]. For organizations deploying RAG in production, these issues translate directly into wasted context window space, reduced answer quality, and poor citation faithfulness\u2014all critical constraints in regulated environments where decisions must be traceable to source material.\n\n## What Is New in This Work\n\nResearchers from the Bravada Group and Eye Dream Pty Ltd introduce **context bubble construction**, a framework that reframes retrieval as an assembly problem rather than a ranking problem[1]. Instead of selecting the top-k most relevant passages independently, the method builds coherent, auditable bundles of text spans that preserve document structure while explicitly controlling redundancy and enforcing strict token budgets.\n\nThe core innovation is combining three previously separate concerns into a unified selection process: **document structure awareness**, **diversity constraints**, and **token efficiency**[1][2]. The framework moves beyond flat passage ranking by organizing multi-granular spans (sections, rows, sheets) and using task-conditioned structural priors to guide retrieval. Critically, it produces a full retrieval trace documenting scoring and selection choices, enabling deterministic tuning and complete auditability\u2014a requirement for enterprise applications where explainability is non-negotiable[1][2].\n\n## Technical Explanation\n\nThe context bubble construction pipeline operates in five stages, with the core selection mechanism working as follows[2]:\n\n**Anchor-based expansion:** The process begins by identifying high-relevance anchor spans through standard retrieval. Rather than stopping there, the system expands outward through the document structure\u2014moving to parent sections, related rows, or adjacent content\u2014to capture contextual information that would be fragmented by top-k selection[1][2].\n\n**Constrained selection with explicit gating:** Starting from anchor spans, the algorithm performs constrained selection that balances four competing objectives: query relevance, marginal coverage (information gain from adding a span), redundancy penalties (lexical overlap with already-selected content), and strict token budgets[1][2]. This is implemented through explicit gating mechanisms that enforce:\n\n- Strict token limits on total context size\n- Per-section budget constraints to prevent over-representation of single document sections\n- Lexical overlap thresholds to control redundancy\n- Diversity requirements ensuring selected chunks contribute complementary information[2]\n\nUnlike probabilistic approaches (e.g., determinantal point processes) or learned attention-based methods, the selection process is fully transparent and deterministic[2]. Each inclusion or exclusion decision is logged with its scoring rationale, producing an auditable trace that engineers can inspect and tune.\n\n**Diversity implementation:** Diversity is enforced through overlap-aware selection rather than implicit sampling[1]. When considering whether to add a new span to the context bubble, the system explicitly measures lexical overlap with already-selected content and applies penalties proportional to redundancy. This ensures that the final context pack contains complementary information rather than repeated statements of the same fact[1][2].\n\n**Empirical validation:** Experiments on real-world enterprise documents (multi-sheet Excel workbooks, job scope documents) demonstrate that context bubbles achieve better coverage of secondary query facets, reduced redundancy, improved answer correctness, and enhanced citation faithfulness compared to flat top-k retrieval[1][2]. Ablation studies confirm that both structural priors and diversity constraints are necessary\u2014removing either component degrades coverage and increases incompleteness[1].\n\n## Practical Relevance\n\nThe framework addresses three concrete pain points in production RAG systems:\n\n**Token efficiency and cost reduction:** Enterprise LLMs operate under fixed context window constraints. Traditional top-k retrieval wastes this limited space through redundancy and fragmentation, forcing operators to either reduce the number of retrieved passages or accept incomplete coverage. Context bubbles directly reduce redundant context while maintaining coverage, allowing organizations to process longer documents or more complex queries within fixed token budgets\u2014a tangible operational constraint given that context window size correlates with inference cost[1][2].\n\n**Answer quality in complex domains:** For applications requiring high-fidelity answers (legal discovery, compliance analysis, financial decision support), the method's demonstrated improvements in answer correctness and citation faithfulness are material. By ensuring that secondary query facets are covered and that retrieved context is auditable, the framework reduces hallucination risk and improves grounding[1][2].\n\n**Auditability for regulated environments:** The full retrieval trace enables deterministic tuning and complete transparency into why specific content was selected or excluded[1][2]. This is essential for enterprises operating under regulatory scrutiny where decision-making systems must be explainable and reproducible.\n\n## Limitations and Open Questions\n\nThe research makes clear contributions but leaves several questions unresolved:\n\n**Generalization scope:** Experiments focus on specific enterprise document types (spreadsheets, structured reports). The authors acknowledge that \"future work could explore the application of this framework to a wider range of corpora and tasks.\"[1] Generalization to unstructured text, documents with irregular structure, or non-English languages remains unvalidated.\n\n**Structural prior specification:** The method relies on \"task-conditioned structural priors\" to guide retrieval[2]. This means organizations must configure the system with domain-specific knowledge of document structure for each new document type. It is not a plug-and-play solution; heterogeneous document formats may require significant implementation effort[1].\n\n**Computational overhead:** The paper does not quantify the computational cost of the constrained selection pipeline relative to standard top-k retrieval. For high-throughput deployments, this overhead could be material.\n\n**Comparison against alternatives:** Ablations compare against flat top-k and diversity-only baselines, but the paper lacks direct comparison against other structure-aware or diversity-based retrieval methods, making it difficult to assess relative performance.\n\n**Quantitative metrics:** While the paper reports qualitative improvements, specific metrics (exact token reduction percentages, answer quality scores, citation accuracy rates) are not provided in the available summaries, limiting reproducibility assessment[1][2].\n\n## Conclusion\n\nContext bubble construction represents a meaningful refinement to RAG architecture for enterprise settings. By explicitly combining document structure awareness, diversity constraints, and token efficiency into a unified, auditable selection process, the framework addresses real limitations of flat top-k retrieval. The demonstrated improvements in coverage, redundancy reduction, and citation faithfulness are relevant to organizations deploying RAG in production.\n\nHowever, the approach is not universally applicable. It requires structured documents with parseable hierarchies and domain-specific configuration of structural priors. Organizations with homogeneous, well-structured document collections\u2014common in enterprise settings\u2014are the primary beneficiaries. For teams considering adoption, the key implementation challenge is specifying task-conditioned structural priors for their document types; the algorithmic contribution is solid, but operational integration requires domain expertise.\n\nThe work positions context construction as a fundamental lever for improving LLM reliability in knowledge-intensive domains, complementary to advances in model architecture or prompting techniques. As enterprises continue scaling RAG deployments, methods that improve both efficiency and answer quality under real-world constraints will become increasingly valuable.\n\n---\n\n## Source\n\n**Original Publication:** [arxiv](http://arxiv.org/abs/2601.10681v1)\n**Authors:** Amir Khurshid, Abhishek Sehgal\n**Published:** 2026-01-15T18:43:19+00:00\n\n*This article was automatically generated using AI analysis. Please refer to the original source for complete details.*",
      "analysis": {
        "item_id": "2601.10681v1",
        "title": "Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems",
        "url": "http://arxiv.org/abs/2601.10681v1",
        "source": "arxiv",
        "fact_extraction": "# Core Contribution\n\nThe paper introduces **context bubble construction**, a framework for assembling coherent, auditable context packs for retrieval-augmented generation (RAG) systems that addresses fragmentation, redundancy, and incomplete coverage in traditional top-k passage retrieval[1][3]. The method combines document structure awareness with explicit diversity constraints under strict token budgets.\n\n# Technical Details\n\n**Method:**\n- Starts from high-relevance anchor spans and expands through constrained selection[1][3]\n- Balances query relevance, marginal coverage, and redundancy penalties[1][3]\n- Organizes multi-granular spans (sections, rows) using task-conditioned structural priors[1]\n- Implements diversity through overlap-aware selection ensuring complementary information[1]\n- Enforces strict token budgets and per-section budget constraints[1]\n\n**Selection Process:**\n- Explicit gating mechanisms based on relevancy, structural role, redundancy, and budget constraints[1]\n- Full retrieval trace emitted documenting scoring and selection choices[1][3]\n\n**Evaluation:**\n- Tested on real-world enterprise documents (multi-sheet Excel workbooks, job quote documents)[1]\n- Metrics: token usage reduction, structural coverage, answer correctness, citation faithfulness[1][2]\n- Ablation studies comparing against flat top-k retrieval and diversity-only baselines[1]\n\n**Key Finding:**\nBoth structural priors and diversity constraint selection are necessary; removing either degrades coverage and increases redundancy[1]\n\n# Explicit Claims\n\n- Context bubbles **significantly reduce redundant context** compared to top-k retrieval[1][2]\n- Better coverage of **secondary facets and 2nd/3rd order query aspects**[1][2]\n- **Improved answer quality and citation faithfulness** within limited context windows[1][2]\n- Provides **full auditability** with transparent, inspectable reasons for inclusion/exclusion of chunks[1]\n- Enables **deterministic tuning** through complete selection trace[1][3]\n- Achieves **improved efficiency** and answer quality over standard RAG baselines in enterprise settings[1]\n\n# Open Questions / Limitations\n\n**Not explicitly addressed:**\n- Specific model architectures or LLM versions tested[1]\n- Quantitative metrics (e.g., exact token reduction percentages, answer quality scores)[1]\n- Computational overhead of the constrained selection pipeline versus standard retrieval[1]\n- Generalization beyond enterprise documents to other domains[1]\n- Comparison against other structure-aware or diversity-based retrieval methods beyond top-k and diversity-only baselines[1]\n- How task-conditioned structural priors are learned or specified[1]\n- Scale of experiments (number of documents, queries, or document sizes)[1]\n- Whether the approach handles documents with inconsistent or irregular structure[1]",
        "engineer_summary": "**Context bubble construction** addresses RAG limitations in enterprise settings: information graph fragmentation, content duplication, over-retrieval, and poor coverage of 2nd/3rd-order query facets from structured documents (e.g., multi-sheet Excel).[1][3]\n\nUnlike top-k passage ranking, it preserves document structure via multi-granular spans (sections, rows) and task-conditioned priors, building from high-relevance anchors through explicit gating on relevance, marginal coverage, lexical overlap redundancy penalties, per-section budgets, and token limits. Outputs auditable traces of scoring/selection for deterministic tuning.[1][2][3]\n\nEnterprise document experiments show reduced redundancy, improved secondary facet coverage, answer quality, and citation faithfulness. Ablations confirm necessity of structural priors and diversity constraints; removing either degrades coverage and increases incompleteness.[1][3]\n\n(98 words)",
        "impact_analysis": "## Immediate implications:\n\n**Token efficiency and cost reduction in production RAG systems.** Traditional top-k retrieval wastes context window space through redundancy and fragmentation, forcing either smaller token budgets or fewer retrieved documents. Context Bubbles directly addresses this by reducing redundant context while maintaining coverage, allowing enterprises to process longer documents or more complex queries within fixed LLM context limits\u2014a tangible operational constraint given that context window size directly correlates with inference cost.[1][2]\n\n**Improved answer quality and citation accuracy.** Experiments demonstrate that the method achieves better answer correctness and citation faithfulness compared to flat top-k retrieval.[1][2] For enterprise applications where answers must be traceable to source documents (legal discovery, compliance, financial analysis), this directly reduces the risk of hallucinated or poorly grounded responses.\n\n**Auditability for regulated environments.** The framework emits a full retrieval trace documenting scoring and selection choices, enabling deterministic tuning and transparency.[1][2] This is critical for enterprises operating under regulatory scrutiny where decision-making systems must be explainable and reproducible.\n\n## Long-term implications:\n\n**Scalability to complex document ecosystems.** The method's ability to handle multi-granular document structures (sections, rows, sheets) suggests potential application beyond simple text corpora to semi-structured enterprise data\u2014spreadsheets, databases, and hierarchical document systems that are common in real organizations.[2] As enterprises accumulate larger document collections, structure-aware retrieval becomes increasingly valuable for maintaining coherence.\n\n**Foundation for more reliable LLM applications in knowledge-intensive domains.** The authors frame this as \"a pathway towards more reliable and accurate LLM applications in complex domains, where comprehensive and well-structured information is critical for generating trustworthy outputs.\"[1] This positions context construction as a fundamental lever for improving LLM reliability beyond prompt engineering or model scaling.\n\n## Practical constraints:\n\n**Limited evaluation scope.** The authors acknowledge that experiments used \"specific document types\" (enterprise Excel workbooks and job scope documents) and suggest \"future work could explore the application of this framework to a wider range of corpora and tasks.\"[1] Generalization to other document structures, languages, or domains remains unvalidated.\n\n**Structural priors require domain specification.** The method relies on \"task-conditioned structural priors\" to guide selection.[2] This means the system must be configured with knowledge of document structure for each new document type or domain\u2014it is not a plug-and-play solution. Organizations with heterogeneous document formats may face implementation complexity.\n\n**Trade-off between coverage and token budget.** While the method balances these constraints, the underlying tension remains: achieving comprehensive coverage of secondary query facets may still require larger token allocations for certain complex queries, limiting the universality of the efficiency gains.[2]",
        "application_mapping": "### Application scenario 1: Enterprise knowledge management for querying structured technical reports\nEmployees query internal documents (e.g., PDFs with sections, tables) for information on company policies or technical specs, using the context bubble to retrieve coherent bundles of spans like sections and rows.\n\n**Why this work helps:**  \nIt preserves document structure to avoid fragmentation and duplication from top-k retrieval, enforces diversity constraints for better coverage of secondary facets (e.g., 2nd/3rd order details), reduces redundant context under token budgets, and improves answer quality with auditable citations, as shown in experiments on enterprise documents[1][3][4].\n\n**Assumptions / prerequisites:**  \n- Documents have inherent structure (e.g., sections, tables) parseable into multi-granular spans.  \n- Task-conditioned structural priors are predefined for the domain.  \n- Strict token budget enforced by the LLM deployment.  \n- Integration with existing RAG pipelines for anchor span retrieval.\n\n### Application scenario 2: Legal or financial document review in compliance workflows\nAnalysts query regulatory guidelines or contracts structured as reports, retrieving context bubbles that bundle relevant spans with traceability for decision-making.\n\n**Why this work helps:**  \nConstrained selection balances relevance, coverage, and redundancy, producing compact contexts that cover facets better than top-k methods, with full audit trails for scoring/selection, enhancing citation faithfulness in limited windows\u2014directly addressing enterprise needs like scattered docs[1][3][4].\n\n**Assumptions / prerequisites:**  \n- Documents exhibit hierarchical structure exploitable by the framework.  \n- High-relevance anchors identifiable via initial retriever.  \n- Diversity and budget constraints tuned deterministically.  \n- Deployment in environments requiring auditable, non-hallucinated outputs (e.g., compliance tools).",
        "blog_synthesis": "# Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems\n\n## 1. Context and Background\n\nRetrieval-augmented generation (RAG) systems integrate large language models (LLMs) with external knowledge retrieval to mitigate hallucinations and improve response grounding in knowledge-intensive tasks.[1][2][5] Traditional RAG pipelines retrieve and rank passages using lexical or dense similarity, selecting the top-k for injection into the LLM context window.[2][3] This approach assumes higher-ranked passages directly translate to better generation quality, but it falters on structured enterprise documents such as multi-sheet Excel workbooks or job quote reports.[1][2][3]\n\nKey limitations include fragmentation of information graphs across document structures, over-retrieval leading to token waste, content duplication, and insufficient coverage of secondary (2nd/3rd-order) query facets.[1][2][3] For instance, top-k retrieval may pull disjoint spans from related sections or rows, disrupting coherence and exceeding strict token budgets imposed by LLM context limits.[1][2] These issues are pronounced in enterprise settings, where documents exhibit hierarchical structures (e.g., sections, sheets, tables) and applications demand auditable, traceable outputs for compliance or decision support.[1][3]\n\nThe paper addresses this by reframing context assembly as an optimization problem: constructing compact, coherent \"context bubbles\" that preserve structure, enforce diversity, and fit token constraints while enabling full auditability.[1][2][3]\n\n## 2. What is New in This Work\n\nThe core innovation is **context bubble construction**, a structure-informed, diversity-constrained framework that assembles citable bundles of multi-granular spans (e.g., sections, rows) from high-relevance anchors.[1][2][3] Unlike flat top-k retrieval, it explicitly balances query relevance, marginal coverage gains, redundancy penalties (via lexical overlap), per-section budgets, and global token limits through overt gating mechanisms.[1][2]\n\nNovel elements include:\n- **Task-conditioned structural priors** to organize spans hierarchically, exploiting document geometry for coherent expansion.[1][2]\n- **Constrained selection pipeline** starting from anchor spans, with diversity enforcement to cover complementary facets and avoid duplication.[1][3]\n- **Full retrieval trace emission**, logging all scoring and gating decisions for deterministic tuning and inspection\u2014critical for enterprise trust.[1][2][3]\n\nAblation studies on enterprise documents confirm the synergy: removing structural priors increases fragmentation, while omitting diversity constraints boosts redundancy; both degrade coverage and answer quality.[1][3]\n\n## 3. Technical Explanation (High-Level, Accurate)\n\nThe system is a modular pipeline transforming raw enterprise documents into auditable context packs.[2] It comprises five stages: document parsing into multi-granular spans, initial anchor retrieval, candidate scoring, constrained selection, and trace generation.[1][2]\n\n**Anchor Retrieval and Expansion.** High-relevance spans are identified via standard retrievers (lexical/dense), serving as seeds.[1][2][3] Expansion draws from structurally related candidates (e.g., parent sections, sibling rows) guided by task-conditioned priors\u2014predefined rules encoding document roles like \"summary sections prioritize over appendices.\"[1][2]\n\n**Scoring and Gating.** Candidates receive scores for:\n- Query relevance (similarity to anchors).[1]\n- Marginal coverage (novel information gain).[1][3]\n- Redundancy penalty (lexical overlap with selected spans).[1][2]\n\nOvert gates then filter:\n- **Token budget**: Global and per-section limits prevent overflow.[2]\n- **Diversity**: Overlap thresholds exclude near-duplicates.[1][3]\n- **Structural fit**: Priors penalize misaligned granularities.[1]\n\nThe result is a compact context bubble: a ranked, citable span bundle fitting the LLM window, with provenance traces (e.g., \"Span X excluded: 80% overlap with Y, exceeds row budget\").[1][2][3]\n\nPseudocode outline:\n\n```\ndef construct_context_bubble(query, doc_spans, token_budget):\n    anchors = retrieve_anchors(query, doc_spans)  # High-relevance seeds\n    candidates = expand_structurally(anchors, priors)  # Multi-granular relatives\n    bubble = []\n    trace = []\n    for cand in rank_candidates(candidates, query):  # Relevance + coverage scores\n        if passes_gates(cand, bubble, token_budget, overlap_threshold):\n            bubble.append(cand)\n            trace.append(f\"Added {cand.id}: score={cand.score}, reason=relevant\")\n        else:\n            trace.append(f\"Rejected {cand.id}: {gate_reason}\")\n    return bubble, trace\n```\n\nThis ensures deterministic outputs, unlike probabilistic top-k sampling.[1][2]\n\nEvaluations used enterprise corpora (Excel workbooks, job quotes), measuring token efficiency, structural coverage, answer correctness, and citation faithfulness. Context bubbles reduced redundancy, improved secondary facet coverage, and boosted quality over baselines.[1][2][3]\n\n## 4. Practical Relevance\n\nFor software engineers building RAG pipelines, context bubbles offer plug-in efficiency gains under fixed context windows, directly cutting inference costs (tokens correlate with compute).[1][2] In enterprise knowledge management\u2014querying technical reports or policies\u2014the method bundles coherent sections/tables, avoiding fragmented top-k outputs and enhancing LLM attention focus.[1][3]\n\n**Scenario 1: Structured Report QA.** Employees query multi-section PDFs; bubbles retrieve anchor paragraphs plus covering rows/sections, yielding precise, cited answers with 2nd-order details (e.g., exceptions, cross-references).[1][3]\n\n**Scenario 2: Compliance Workflows.** Legal/financial analysts search contracts; auditable traces justify inclusions/exclusions, supporting traceable decisions in regulated environments.[1][2]\n\nIntegration requires: span parser for document types, prior definitions per domain, and retriever hooks. Ablations validate gains over top-k and diversity-only baselines, positioning it for production where structure is parseable.[1][3]\n\n## 5. Limitations and Open Questions\n\nThe paper evaluates on specific enterprise formats (Excel, job quotes), leaving generalization to irregular structures, other languages, or domains untested.[1][2] Quantitative details\u2014exact token reductions, F1 scores, dataset scales (queries/documents)\u2014are absent, hindering reproducibility.[1]\n\nUnspecified aspects include:\n- LLM/backbone models used.[1]\n- Computational overhead of gating vs. top-k.[1]\n- Prior learning/specification (manual? learned?).[1][2]\n- Handling paraphrasing (lexical overlap may miss semantic redundancy).[1]\n- Comparisons to advanced baselines (e.g., other structure-aware retrievers).[1]\n\nTask-conditioned priors demand domain tuning, complicating heterogeneous deployments.[1][2] Non-deterministic chunking risks inconsistency; future multimodal/hybrid retrieval is suggested.[1]\n\n## 6. Conclusion\n\nContext bubble construction advances RAG by systematically addressing structure fragmentation and redundancy in enterprise documents, delivering efficient, auditable contexts that enhance answer quality and faithfulness.[1][2][3] Engineers can leverage its explicit gates for tunable pipelines, though domain priors and eval breadth limit immediate universality. This framework underscores structure and diversity as essential signals, paving structured paths to reliable LLM applications in complex domains.[1][3]\n\n(Word count: 912)",
        "linkedin_formatting": "Introduced structure- and diversity-aware **context bubble construction** for RAG systems, addressing fragmentation, redundancy, and incomplete coverage in enterprise documents.[1][2][3]\n\nKey advances:\n- Builds from high-relevance anchors via constrained selection balancing relevance, marginal coverage, and redundancy penalties.[1][2]\n- Preserves multi-granular structure (sections, rows) with task-conditioned priors and strict token budgets.[1][3]\n- Emits full audit traces for deterministic tuning and citation faithfulness.[2][3]\n\nEnterprise experiments show reduced redundancy, better secondary facet coverage, and improved answer quality over top-k retrieval.[1][2]\n\n**Takeaway:** Auditability enables reliable tuning in regulated domains.\n\n#RAG #RetrievalAugmentedGeneration #ContextConstruction #EnterpriseAI #LLM",
        "credibility_check": "Error: 'generated_output'"
      },
      "file_path": null,
      "status": "drafted",
      "published_url": null,
      "created_at": "2026-01-18 10:11:42",
      "published_at": null,
      "paper_title": "Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems",
      "paper_url": "http://arxiv.org/abs/2601.10681v1",
      "paper_source": "arxiv"
    },
    {
      "id": 2,
      "paper_id": "2601.10712v1",
      "content_type": "linkedin",
      "content": "MatchTIR introduces fine-grained reinforcement learning for Tool-Integrated Reasoning (TIR) by assigning turn-level rewards via bipartite matching between predicted and ground-truth tool calls.[1][2]\n\n- Formulates credit assignment as bipartite matching with hard (Kuhn-Munkres) and soft (Optimal Transport) strategies to distinguish effective from redundant tool calls.[1][3]\n- Uses dual-level advantage estimation to balance turn-level precision with trajectory success, optimized via GRPO.[1][4]\n- 4B model outperforms most 8B competitors on three benchmarks, especially long-horizon multi-turn tasks; code open-sourced.[1][2]\n\nTakeaway: Fine-grained supervision enables efficient TIR training for smaller models in complex agentic systems.\n\n#MatchTIR #ToolIntegratedReasoning #ReinforcementLearning #BipartiteMatching #LLMReasoning\n\n\ud83d\udcce Read more: http://arxiv.org/abs/2601.10712v1\nvia arxiv\n\n#AI #DeepLearning #MachineLearning #Research",
      "analysis": {
        "item_id": "2601.10712v1",
        "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
        "url": "http://arxiv.org/abs/2601.10712v1",
        "source": "arxiv",
        "fact_extraction": "## Core Contribution\n\nMatchTIR addresses **coarse-grained credit assignment in tool-integrated reasoning** by introducing a reinforcement learning framework that assigns fine-grained, turn-level rewards to individual tool interactions rather than uniform rewards across entire trajectories[1][2]. The framework formulates credit assignment as a bipartite matching problem between predicted and ground-truth tool call sequences, enabling the model to distinguish effective tool calls from redundant or erroneous ones in multi-turn scenarios[1][2].\n\n## Technical Details\n\n**Methods:**\n- **Bipartite matching-based reward assignment**: Two assignment strategies are employed\u2014Hard Assignment (Kuhn-Munkres algorithm) for strict one-to-one mapping and Soft Assignment (Optimal Transport) for probabilistic alignment[1]\n- **Dual-level advantage estimation**: Integrates turn-level rewards (individual step contribution via discounted rewards) with trajectory-level signals (overall task success) to balance local precision and global performance[1][3]\n- **Optimization objective**: GRPO (likely Group Relative Policy Optimization) with integrated dual-level advantages[4]\n\n**Model and Scale:**\n- 4B parameter model tested as primary contribution[1][2]\n- Compared against 8B parameter competitors[1][2]\n\n**Evaluation:**\n- Tested on three benchmarks (specific benchmark names not detailed in provided excerpts)[1][2]\n- Evaluated on both in-domain and out-of-domain scenarios[3]\n- Particular focus on long-horizon and multi-turn interaction tasks[1][2][3]\n\n**Artifacts:**\n- Code, datasets, and model checkpoints are open-sourced on GitHub[1]\n\n## Explicit Claims\n\n- A 4B model using MatchTIR **surpasses the majority of 8B competitors**, particularly in long-horizon and multi-turn tasks[1][2]\n- The framework provides **dense and verifiable supervision signals** through turn-level reward design[1]\n- MatchTIR demonstrates **effectiveness and robustness across various model scales**[3]\n- The method effectively **distinguishes high-quality tool calls from redundant or uninformative ones**[3]\n\n## Open Questions / Limitations\n\n**Not explicitly claimed or unclear:**\n- Specific benchmark names and datasets used for evaluation\n- Quantitative performance metrics (e.g., exact improvement percentages, absolute scores)\n- Computational overhead or training efficiency compared to baseline methods\n- Whether the framework generalizes to tool sets beyond those in the benchmarks\n- Ablation study results isolating the contribution of hard vs. soft assignment strategies\n- Performance on out-of-domain tasks relative to in-domain performance\n- Scalability to models larger than 8B parameters\n- Comparison against other recent fine-grained credit assignment methods in TIR",
        "engineer_summary": "**Problem:** Existing RL methods for Tool-Integrated Reasoning assign uniform advantages across trajectory steps, failing to distinguish effective tool calls from redundant or erroneous ones in long-horizon scenarios[1][2].\n\n**Solution:** MatchTIR formulates turn-level credit assignment as a bipartite matching problem between predicted and ground-truth tool calls[1]. It employs two strategies: hard assignment (Kuhn-Munkres algorithm) for strict one-to-one mapping and soft assignment (optimal transport) for probabilistic alignment[3]. A dual-level advantage estimation scheme integrates turn-level rewards with trajectory-level signals for balanced optimization[1].\n\n**Evidence:** Experiments across three benchmarks demonstrate effectiveness, with a 4B model outperforming most 8B competitors on long-horizon and multi-turn tasks[2]. The framework shows robust generalization across task complexity levels[1].\n\n**Novelty:** Prior work relied on coarse-grained outcome or trajectory rewards. MatchTIR introduces dense, turn-level supervision via structured matching of tool interactions (names, parameters, contents), enabling precise credit assignment to individual steps[1][4].",
        "impact_analysis": "**Immediate implications:**\nMatchTIR enables more efficient training of smaller LLMs for **Tool-Integrated Reasoning (TIR)** tasks by providing fine-grained, turn-level rewards via bipartite matching, distinguishing effective tool calls from redundant or erroneous ones in multi-turn scenarios[1][2][4]. Evidence from benchmarks shows a **4B-parameter model outperforming most 8B competitors**, particularly in long-horizon tasks, allowing parameter-efficient optimization without relying on coarse trajectory-level rewards[1][2].\n\n**Long-term implications:**\nFine-grained supervision could improve scalability of TIR in complex, real-world applications like multi-step planning or agentic systems by better aligning local tool precision with global task success through dual-level advantage estimation; this is a potential extension supported by the method's robustness across in-domain and out-of-domain benchmarks, though untested beyond experiments[2][4].\n\n**Practical constraints:**\nRequires ground-truth traces for bipartite matching, limiting applicability to datasets with detailed annotations and increasing preprocessing costs for long trajectories[1][2][4].",
        "application_mapping": "### Application scenario 1: Scientific research automation for complex calculations\nMatchTIR enhances LLMs in verifying physics or chemistry simulations involving multi-turn tool calls, such as computing atomic ratios under varying conditions using Python interpreters.\n\n**Why this work helps:**  \nFine-grained supervision via bipartite matching assigns precise turn-level rewards, distinguishing effective tool calls (e.g., accurate computations) from redundant ones in long-horizon tasks, improving performance on benchmarks like GPQA where a 4B MatchTIR model outperforms larger competitors.[Content][1]\n\n**Assumptions / prerequisites:**  \n- Access to external tools like Python interpreters or databases for computation.  \n- Ground-truth traces available for training via bipartite matching.  \n- Deployment on LLMs with TIR capabilities, tested on multi-turn benchmarks.\n\n### Application scenario 2: AI coding assistants for debugging multi-step code generation\nMatchTIR supports LLMs in iterative code writing, testing, and refinement, interleaving reasoning with tool interactions like code executors in extended sessions.\n\n**Why this work helps:**  \nDual-level advantage estimation balances local turn precision (e.g., rewarding correct tool usage per step) with global success, reducing errors in long trajectories compared to coarse-grained RL methods.[Content][4]\n\n**Assumptions / prerequisites:**  \n- Integration with code execution tools and APIs.  \n- Training data with paired predicted/ground-truth traces for reward derivation.  \n- Hardware for fine-tuning 4B+ parameter models on TIR benchmarks.",
        "blog_synthesis": "# MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching\n\n## Context and Background\n\nTool-Integrated Reasoning (TIR) represents a significant capability for large language models, enabling them to tackle complex tasks by interleaving reasoning steps with external tool interactions[1][2]. This paradigm has proven valuable for applications ranging from scientific computation to code generation, where models must make sequential decisions about which tools to invoke and how to use them.\n\nHowever, training these systems presents a fundamental challenge: how to assign credit for success or failure across multi-turn interactions. Current reinforcement learning approaches typically rely on **outcome-level or trajectory-level rewards**, assigning uniform advantages to all steps within a reasoning trajectory[1][2]. This coarse-grained approach treats every tool invocation equally, regardless of whether it was essential, redundant, or erroneous. In long-horizon scenarios with many interaction turns, this inability to distinguish between effective and ineffective tool calls creates a significant bottleneck for efficient model optimization[1][2].\n\n## What Is New in This Work\n\nMatchTIR introduces a **fine-grained credit assignment framework** that moves beyond uniform trajectory-level rewards to assign **distinct advantages to individual interaction turns**[1][3]. The core innovation reformulates the credit assignment problem as a bipartite matching problem between predicted tool call sequences and ground-truth traces[1][2].\n\nThis represents a departure from prior work in two ways. First, it leverages the **structured nature of tool interactions**\u2014tool names, parameter names, and parameter contents\u2014to enable explicit evaluation of correctness at each turn[4]. Second, it combines turn-level precision with trajectory-level success through a dual-level advantage estimation scheme, ensuring the model optimizes for both local accuracy and global task completion[1][3].\n\nThe practical outcome is notable: a 4B-parameter model trained with MatchTIR **surpasses the majority of 8B-parameter competitors**, particularly in long-horizon and multi-turn tasks[1][2]. This efficiency gain suggests that fine-grained supervision enables more effective learning from the same amount of data.\n\n## Technical Explanation\n\n### Bipartite Matching for Reward Assignment\n\nMatchTIR formulates turn-level reward assignment as a bipartite matching problem. Given a predicted sequence of tool calls and a ground-truth sequence, the framework constructs a matching matrix and applies one of two assignment strategies[1]:\n\n- **Hard Assignment (Kuhn-Munkres Algorithm)**: Enforces strict one-to-one mapping between predicted and ground-truth tool calls, producing deterministic turn-level rewards[1]\n- **Soft Assignment (Optimal Transport)**: Enables probabilistic alignment, allowing for flexible matching when exact correspondence is ambiguous[1]\n\nBoth strategies produce **dense, turn-level rewards** that expose which specific tool interactions contributed to task success or failure[1][4]. This is fundamentally different from outcome-only rewards, which provide no signal about intermediate steps.\n\n### Dual-Level Advantage Estimation\n\nTo balance local step precision with global task success, MatchTIR integrates two reward signals[1][3]:\n\n- **Turn-level advantages**: Derived from discounted rewards at individual interaction steps, capturing the contribution of each tool call\n- **Trajectory-level signals**: Reflecting overall task quality and final outcome\n\nThis dual-level scheme ensures the model learns both to execute individual tool calls correctly and to select the right sequence of tools for the overall task[1][3]. The combined advantages are then used with a policy optimization objective (GRPO) to update the model[4].\n\n### Evaluation Scope\n\nExperiments span three benchmarks with evaluation on both in-domain and out-of-domain scenarios[3]. The framework shows particular strength in long-horizon and multi-turn interaction tasks[1][2][3], suggesting that the benefits of fine-grained supervision scale with task complexity.\n\n## Practical Relevance\n\nFor software engineers and AI practitioners, MatchTIR addresses a concrete problem: **training smaller models to perform complex tool-integrated tasks efficiently**. The 4B-to-8B performance gap has immediate implications for deployment, where model size directly affects latency, memory consumption, and inference cost.\n\nThe framework is particularly relevant for applications requiring extended reasoning chains:\n\n- **Scientific computation workflows**: Multi-step calculations where intermediate tool calls must be precise\n- **Code generation and debugging**: Iterative refinement where each execution step provides structured feedback\n- **Information retrieval systems**: Sequential queries where redundant lookups waste computational resources\n\nThe open-sourced code, datasets, and model checkpoints enable practitioners to apply MatchTIR to domain-specific TIR tasks without reimplementing the core framework[1].\n\n## Limitations and Open Questions\n\nSeveral aspects warrant careful consideration:\n\n**Ground-truth trace requirement**: MatchTIR requires annotated ground-truth tool call sequences for bipartite matching during training[1][2][4]. This limits applicability to datasets with detailed trajectory annotations and increases preprocessing overhead for long-horizon tasks.\n\n**Benchmark specificity**: While experiments cover three benchmarks, the specific datasets and their characteristics are not detailed in available sources. Generalization to tool sets or domains beyond those benchmarks remains unclear.\n\n**Scalability beyond 8B**: Experiments focus on 4B and 8B models. Performance on larger models (13B, 70B+) and whether the efficiency gains persist at scale is unexplored.\n\n**Computational overhead**: The cost of computing bipartite matchings for each training trajectory\u2014particularly for long horizons\u2014is not discussed. This could impact training efficiency despite improved sample efficiency.\n\n**Ablation analysis**: The relative contribution of hard versus soft assignment strategies, and the impact of the dual-level advantage scheme versus simpler alternatives, is not explicitly quantified in available excerpts.\n\n## Conclusion\n\nMatchTIR addresses a genuine limitation in current reinforcement learning approaches for Tool-Integrated Reasoning by introducing structured, turn-level credit assignment via bipartite matching. The framework's ability to distinguish effective tool calls from redundant or erroneous ones in multi-turn scenarios represents a meaningful advance in training efficiency, demonstrated by smaller models achieving competitive performance with larger baselines.\n\nThe work is technically sound and practically motivated, with open-sourced artifacts enabling adoption. However, practitioners should be aware of the ground-truth annotation requirement and the need to validate performance on their specific tool sets and domains. For teams building agentic systems or complex reasoning pipelines, MatchTIR provides a principled approach to improving model efficiency without sacrificing task performance.",
        "linkedin_formatting": "MatchTIR introduces fine-grained supervision for Tool-Integrated Reasoning (TIR) via bipartite matching between predicted and ground-truth tool calls.[1][2]\n\nKey advances:\n- Hard (Kuhn-Munkres) and soft (Optimal Transport) assignment strategies for turn-level rewards, distinguishing effective from redundant tool calls.[1][3]\n- Dual-level advantage estimation balancing local precision with global task success.[1][4]\n- 4B model outperforms most 8B competitors on long-horizon, multi-turn benchmarks; code open-sourced.[1][2]\n\nTakeaway: Fine-grained credit assignment enables efficient TIR training for smaller models in complex agentic systems.\n\n#MatchTIR #ToolIntegratedReasoning #ReinforcementLearning #BipartiteMatching #LLMReasoning",
        "credibility_check": "Error: 'generated_output'"
      },
      "file_path": "data/drafts/linkedin/2601.10712v1.txt",
      "status": "published",
      "published_url": "https://www.linkedin.com/feed/update/urn:li:activity:urn:li:share:7418684645233438720/",
      "created_at": "2026-01-18 10:10:08",
      "published_at": "2026-01-18T21:34:12.242992",
      "paper_title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
      "paper_url": "http://arxiv.org/abs/2601.10712v1",
      "paper_source": "arxiv"
    },
    {
      "id": 1,
      "paper_id": "2601.10712v1",
      "content_type": "blog",
      "content": "---\nlayout: post\ntitle: \"MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching\"\ndate: 2026-01-18\ncategories: [AI, Research]\ntags:\n  - AI\n  - Machine Learning\n  - Research\nauthor: AI Research Publisher\nsource: arxiv\nsource_url: http://arxiv.org/abs/2601.10712v1\n---\n\n# MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching\n\n## Context and Background\n\nTool-Integrated Reasoning (TIR) represents a critical capability for large language models, enabling them to tackle complex tasks by interleaving reasoning steps with external tool interactions[1][2]. This paradigm has proven valuable for applications ranging from scientific computation to code generation, where models must decide not only what to reason about but also when and how to invoke external resources.\n\nHowever, training LLMs for effective tool use presents a significant challenge. Current reinforcement learning approaches typically rely on **outcome-level or trajectory-level rewards**, assigning uniform advantages to all steps within a reasoning trajectory[1][2]. This coarse-grained credit assignment creates a fundamental problem: the model cannot distinguish between effective tool calls, redundant invocations, and erroneous ones\u2014particularly in long-horizon, multi-turn scenarios where dozens of interactions may occur before reaching a final answer[1][2].\n\nConsider a multi-step reasoning task where an LLM makes ten tool calls to solve a problem. Traditional RL methods reward or penalize all ten calls equally based on the final outcome. If nine calls were essential and one was redundant, the model receives no signal about which call was unnecessary. This inefficiency compounds in longer trajectories, making it difficult for smaller models to learn effective tool-use policies.\n\n## What Is New in This Work\n\nMatchTIR introduces a **fine-grained reinforcement learning framework** that assigns distinct, precise advantages to individual interaction turns rather than uniform rewards across trajectories[1][2]. The core innovation reformulates credit assignment as a **bipartite matching problem** between predicted tool call sequences and ground-truth traces[1].\n\nThe key insight is that tool interactions expose structured, verifiable signals\u2014tool names, parameter names, and parameter contents\u2014which enable explicit evaluation of correctness at each turn[4]. By matching predicted calls against ground-truth calls, the framework can assign dense, turn-level rewards that reflect the actual contribution of each interaction step.\n\nThis represents a departure from prior work in two ways. First, it moves beyond coarse-grained outcome rewards to fine-grained turn-level supervision. Second, it leverages the inherent structure of tool calls (which are discrete, named operations with explicit parameters) rather than treating reasoning trajectories as opaque sequences.\n\n## Technical Explanation\n\n### Bipartite Matching for Reward Assignment\n\nMatchTIR employs two complementary assignment strategies to derive turn-level rewards[1]:\n\n**Hard Assignment** uses the Kuhn-Munkres algorithm to establish strict one-to-one mappings between predicted and ground-truth tool calls[1]. This approach is deterministic and produces unambiguous assignments, useful when tool call sequences are well-structured and ground-truth traces are precise.\n\n**Soft Assignment** applies Optimal Transport to enable probabilistic alignment[1]. Rather than forcing one-to-one mappings, soft assignment allows partial credit allocation across multiple possible matches, accommodating scenarios where multiple valid tool call sequences could solve a problem or where predicted calls partially align with ground-truth calls.\n\nBoth strategies produce dense supervision signals\u2014every predicted tool call receives a reward signal based on its alignment with ground-truth calls, rather than waiting for final task completion.\n\n### Dual-Level Advantage Estimation\n\nA critical design choice in MatchTIR is balancing **local step precision** with **global task success**[1][3]. The framework integrates two levels of advantage signals:\n\n- **Turn-level advantages** reflect individual step contribution via discounted rewards, capturing whether a specific tool call was effective or redundant[1]\n- **Trajectory-level signals** capture overall task success, ensuring the model optimizes for solving the complete problem[1]\n\nThis dual-level scheme prevents a pathological scenario: a model could learn to make individually \"correct\" tool calls that collectively fail to solve the task, or conversely, make locally suboptimal calls that happen to lead to correct final answers. By combining both signals, the framework encourages locally precise and globally successful behavior.\n\n### Optimization and Implementation\n\nThe framework optimizes using GRPO (Group Relative Policy Optimization) with integrated dual-level advantages[4]. The approach does not require a learned value function, simplifying the training pipeline and reducing computational overhead compared to actor-critic methods.\n\n## Practical Relevance\n\nThe empirical results demonstrate concrete practical value. A **4-billion parameter MatchTIR model surpasses the majority of 8-billion parameter competitors**, particularly on long-horizon and multi-turn tasks[1][2]. This efficiency gain is significant: achieving comparable performance with half the parameters reduces inference latency, memory requirements, and computational costs\u2014critical factors for production deployment.\n\nThe framework shows effectiveness across both in-domain and out-of-domain benchmarks[3], suggesting the learned policies generalize beyond the specific tasks used for training. This robustness is essential for real-world applications where test distributions may differ from training data.\n\nFor software engineers building agentic systems, MatchTIR offers a concrete method to improve tool-use behavior without scaling to larger models. For researchers, it provides a principled approach to credit assignment in multi-turn reasoning that could extend beyond tool use to other sequential decision-making problems in language models.\n\n## Limitations and Open Questions\n\nSeveral important limitations warrant acknowledgment:\n\n**Ground-truth trace requirement**: The framework requires ground-truth tool call sequences for bipartite matching during training[1][2][4]. This necessitates detailed annotations of correct reasoning paths, increasing dataset preparation costs and limiting applicability to domains where such annotations are expensive or ambiguous.\n\n**Scalability to larger models**: Experiments focus on 4B and 8B parameter models[1][2]. It remains unclear whether the approach scales effectively to 70B+ parameter models or whether the efficiency gains diminish at larger scales.\n\n**Benchmark specificity**: While three benchmarks are mentioned, the search results do not specify which benchmarks or provide detailed quantitative metrics (exact improvement percentages, absolute performance scores)[1][2]. This limits assessment of performance magnitude and generalization breadth.\n\n**Computational overhead**: The cost of bipartite matching on long trajectories is not discussed. For trajectories with hundreds of tool calls, computing optimal assignments could introduce non-trivial overhead.\n\n**Ablation analysis**: The relative contributions of hard versus soft assignment strategies, and the impact of different weighting schemes for turn-level versus trajectory-level signals, are not detailed in available excerpts.\n\n## Conclusion\n\nMatchTIR addresses a genuine bottleneck in training LLMs for tool-integrated reasoning: the inability of existing methods to assign credit at the granularity of individual tool interactions. By reformulating credit assignment as a bipartite matching problem and introducing dual-level advantage estimation, the framework enables smaller models to achieve competitive performance with larger baselines.\n\nThe work is technically sound, grounded in the structure of tool calls themselves, and demonstrates practical efficiency gains. For practitioners building agentic systems or reasoning-heavy applications, MatchTIR provides a concrete method to improve tool-use behavior without scaling model size. The open-sourced code and model checkpoints lower barriers to adoption[1].\n\nThe primary constraint\u2014requiring ground-truth traces\u2014is not insurmountable for many applications but does limit applicability to domains with sparse or expensive annotations. Future work addressing this constraint, extending to larger model scales, and providing detailed ablation studies would strengthen the contribution further.\n\n---\n\n## Source\n\n**Original Publication:** [arxiv](http://arxiv.org/abs/2601.10712v1)\n**Authors:** Changle Qu, Sunhao Dai, Hengyi Cai, Jun Xu, Shuaiqiang Wang, Dawei Yin\n**Published:** 2026-01-15T18:59:23+00:00\n\n*This article was automatically generated using AI analysis. Please refer to the original source for complete details.*",
      "analysis": {
        "item_id": "2601.10712v1",
        "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
        "url": "http://arxiv.org/abs/2601.10712v1",
        "source": "arxiv",
        "fact_extraction": "## Core Contribution\n\nMatchTIR addresses **coarse-grained credit assignment in tool-integrated reasoning** by introducing a reinforcement learning framework that assigns fine-grained, turn-level rewards to individual tool interactions rather than uniform rewards across entire trajectories[1][2]. The framework formulates credit assignment as a bipartite matching problem between predicted and ground-truth tool call sequences, enabling the model to distinguish effective tool calls from redundant or erroneous ones in multi-turn scenarios[1][2].\n\n## Technical Details\n\n**Methods:**\n- **Bipartite matching-based reward assignment**: Two assignment strategies are employed\u2014Hard Assignment (Kuhn-Munkres algorithm) for strict one-to-one mapping and Soft Assignment (Optimal Transport) for probabilistic alignment[1]\n- **Dual-level advantage estimation**: Integrates turn-level rewards (individual step contribution via discounted rewards) with trajectory-level signals (overall task success) to balance local precision and global performance[1][3]\n- **Optimization objective**: GRPO (likely Group Relative Policy Optimization) with integrated dual-level advantages[4]\n\n**Model and Scale:**\n- 4B parameter model tested as primary contribution[1][2]\n- Compared against 8B parameter competitors[1][2]\n\n**Evaluation:**\n- Tested on three benchmarks (specific benchmark names not detailed in provided excerpts)[1][2]\n- Evaluated on both in-domain and out-of-domain scenarios[3]\n- Particular focus on long-horizon and multi-turn interaction tasks[1][2][3]\n\n**Artifacts:**\n- Code, datasets, and model checkpoints are open-sourced on GitHub[1]\n\n## Explicit Claims\n\n- A 4B model using MatchTIR **surpasses the majority of 8B competitors**, particularly in long-horizon and multi-turn tasks[1][2]\n- The framework provides **dense and verifiable supervision signals** through turn-level reward design[1]\n- MatchTIR demonstrates **effectiveness and robustness across various model scales**[3]\n- The method effectively **distinguishes high-quality tool calls from redundant or uninformative ones**[3]\n\n## Open Questions / Limitations\n\n**Not explicitly claimed or unclear:**\n- Specific benchmark names and datasets used for evaluation\n- Quantitative performance metrics (e.g., exact improvement percentages, absolute scores)\n- Computational overhead or training efficiency compared to baseline methods\n- Whether the framework generalizes to tool sets beyond those in the benchmarks\n- Ablation study results isolating the contribution of hard vs. soft assignment strategies\n- Performance on out-of-domain tasks relative to in-domain performance\n- Scalability to models larger than 8B parameters\n- Comparison against other recent fine-grained credit assignment methods in TIR",
        "engineer_summary": "**Problem:** Existing RL methods for Tool-Integrated Reasoning assign uniform advantages across trajectory steps, failing to distinguish effective tool calls from redundant or erroneous ones in long-horizon scenarios[1][2].\n\n**Solution:** MatchTIR formulates turn-level credit assignment as a bipartite matching problem between predicted and ground-truth tool calls[1]. It employs two strategies: hard assignment (Kuhn-Munkres algorithm) for strict one-to-one mapping and soft assignment (optimal transport) for probabilistic alignment[3]. A dual-level advantage estimation scheme integrates turn-level rewards with trajectory-level signals for balanced optimization[1].\n\n**Evidence:** Experiments across three benchmarks demonstrate effectiveness, with a 4B model outperforming most 8B competitors on long-horizon and multi-turn tasks[2]. The framework shows robust generalization across task complexity levels[1].\n\n**Novelty:** Prior work relied on coarse-grained outcome or trajectory rewards. MatchTIR introduces dense, turn-level supervision via structured matching of tool interactions (names, parameters, contents), enabling precise credit assignment to individual steps[1][4].",
        "impact_analysis": "**Immediate implications:**\nMatchTIR enables more efficient training of smaller LLMs for **Tool-Integrated Reasoning (TIR)** tasks by providing fine-grained, turn-level rewards via bipartite matching, distinguishing effective tool calls from redundant or erroneous ones in multi-turn scenarios[1][2][4]. Evidence from benchmarks shows a **4B-parameter model outperforming most 8B competitors**, particularly in long-horizon tasks, allowing parameter-efficient optimization without relying on coarse trajectory-level rewards[1][2].\n\n**Long-term implications:**\nFine-grained supervision could improve scalability of TIR in complex, real-world applications like multi-step planning or agentic systems by better aligning local tool precision with global task success through dual-level advantage estimation; this is a potential extension supported by the method's robustness across in-domain and out-of-domain benchmarks, though untested beyond experiments[2][4].\n\n**Practical constraints:**\nRequires ground-truth traces for bipartite matching, limiting applicability to datasets with detailed annotations and increasing preprocessing costs for long trajectories[1][2][4].",
        "application_mapping": "### Application scenario 1: Scientific research automation for complex calculations\nMatchTIR enhances LLMs in verifying physics or chemistry simulations involving multi-turn tool calls, such as computing atomic ratios under varying conditions using Python interpreters.\n\n**Why this work helps:**  \nFine-grained supervision via bipartite matching assigns precise turn-level rewards, distinguishing effective tool calls (e.g., accurate computations) from redundant ones in long-horizon tasks, improving performance on benchmarks like GPQA where a 4B MatchTIR model outperforms larger competitors.[Content][1]\n\n**Assumptions / prerequisites:**  \n- Access to external tools like Python interpreters or databases for computation.  \n- Ground-truth traces available for training via bipartite matching.  \n- Deployment on LLMs with TIR capabilities, tested on multi-turn benchmarks.\n\n### Application scenario 2: AI coding assistants for debugging multi-step code generation\nMatchTIR supports LLMs in iterative code writing, testing, and refinement, interleaving reasoning with tool interactions like code executors in extended sessions.\n\n**Why this work helps:**  \nDual-level advantage estimation balances local turn precision (e.g., rewarding correct tool usage per step) with global success, reducing errors in long trajectories compared to coarse-grained RL methods.[Content][4]\n\n**Assumptions / prerequisites:**  \n- Integration with code execution tools and APIs.  \n- Training data with paired predicted/ground-truth traces for reward derivation.  \n- Hardware for fine-tuning 4B+ parameter models on TIR benchmarks.",
        "blog_synthesis": "# MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching\n\n## Context and Background\n\nTool-Integrated Reasoning (TIR) represents a significant capability for large language models, enabling them to tackle complex tasks by interleaving reasoning steps with external tool interactions[1][2]. This paradigm has proven valuable for applications ranging from scientific computation to code generation, where models must make sequential decisions about which tools to invoke and how to use them.\n\nHowever, training these systems presents a fundamental challenge: how to assign credit for success or failure across multi-turn interactions. Current reinforcement learning approaches typically rely on **outcome-level or trajectory-level rewards**, assigning uniform advantages to all steps within a reasoning trajectory[1][2]. This coarse-grained approach treats every tool invocation equally, regardless of whether it was essential, redundant, or erroneous. In long-horizon scenarios with many interaction turns, this inability to distinguish between effective and ineffective tool calls creates a significant bottleneck for efficient model optimization[1][2].\n\n## What Is New in This Work\n\nMatchTIR introduces a **fine-grained credit assignment framework** that moves beyond uniform trajectory-level rewards to assign **distinct advantages to individual interaction turns**[1][3]. The core innovation reformulates the credit assignment problem as a bipartite matching problem between predicted tool call sequences and ground-truth traces[1][2].\n\nThis represents a departure from prior work in two ways. First, it leverages the **structured nature of tool interactions**\u2014tool names, parameter names, and parameter contents\u2014to enable explicit evaluation of correctness at each turn[4]. Second, it combines turn-level precision with trajectory-level success through a dual-level advantage estimation scheme, ensuring the model optimizes for both local accuracy and global task completion[1][3].\n\nThe practical outcome is notable: a 4B-parameter model trained with MatchTIR **surpasses the majority of 8B-parameter competitors**, particularly in long-horizon and multi-turn tasks[1][2]. This efficiency gain suggests that fine-grained supervision enables more effective learning from the same amount of data.\n\n## Technical Explanation\n\n### Bipartite Matching for Reward Assignment\n\nMatchTIR formulates turn-level reward assignment as a bipartite matching problem. Given a predicted sequence of tool calls and a ground-truth sequence, the framework constructs a matching matrix and applies one of two assignment strategies[1]:\n\n- **Hard Assignment (Kuhn-Munkres Algorithm)**: Enforces strict one-to-one mapping between predicted and ground-truth tool calls, producing deterministic turn-level rewards[1]\n- **Soft Assignment (Optimal Transport)**: Enables probabilistic alignment, allowing for flexible matching when exact correspondence is ambiguous[1]\n\nBoth strategies produce **dense, turn-level rewards** that expose which specific tool interactions contributed to task success or failure[1][4]. This is fundamentally different from outcome-only rewards, which provide no signal about intermediate steps.\n\n### Dual-Level Advantage Estimation\n\nTo balance local step precision with global task success, MatchTIR integrates two reward signals[1][3]:\n\n- **Turn-level advantages**: Derived from discounted rewards at individual interaction steps, capturing the contribution of each tool call\n- **Trajectory-level signals**: Reflecting overall task quality and final outcome\n\nThis dual-level scheme ensures the model learns both to execute individual tool calls correctly and to select the right sequence of tools for the overall task[1][3]. The combined advantages are then used with a policy optimization objective (GRPO) to update the model[4].\n\n### Evaluation Scope\n\nExperiments span three benchmarks with evaluation on both in-domain and out-of-domain scenarios[3]. The framework shows particular strength in long-horizon and multi-turn interaction tasks[1][2][3], suggesting that the benefits of fine-grained supervision scale with task complexity.\n\n## Practical Relevance\n\nFor software engineers and AI practitioners, MatchTIR addresses a concrete problem: **training smaller models to perform complex tool-integrated tasks efficiently**. The 4B-to-8B performance gap has immediate implications for deployment, where model size directly affects latency, memory consumption, and inference cost.\n\nThe framework is particularly relevant for applications requiring extended reasoning chains:\n\n- **Scientific computation workflows**: Multi-step calculations where intermediate tool calls must be precise\n- **Code generation and debugging**: Iterative refinement where each execution step provides structured feedback\n- **Information retrieval systems**: Sequential queries where redundant lookups waste computational resources\n\nThe open-sourced code, datasets, and model checkpoints enable practitioners to apply MatchTIR to domain-specific TIR tasks without reimplementing the core framework[1].\n\n## Limitations and Open Questions\n\nSeveral aspects warrant careful consideration:\n\n**Ground-truth trace requirement**: MatchTIR requires annotated ground-truth tool call sequences for bipartite matching during training[1][2][4]. This limits applicability to datasets with detailed trajectory annotations and increases preprocessing overhead for long-horizon tasks.\n\n**Benchmark specificity**: While experiments cover three benchmarks, the specific datasets and their characteristics are not detailed in available sources. Generalization to tool sets or domains beyond those benchmarks remains unclear.\n\n**Scalability beyond 8B**: Experiments focus on 4B and 8B models. Performance on larger models (13B, 70B+) and whether the efficiency gains persist at scale is unexplored.\n\n**Computational overhead**: The cost of computing bipartite matchings for each training trajectory\u2014particularly for long horizons\u2014is not discussed. This could impact training efficiency despite improved sample efficiency.\n\n**Ablation analysis**: The relative contribution of hard versus soft assignment strategies, and the impact of the dual-level advantage scheme versus simpler alternatives, is not explicitly quantified in available excerpts.\n\n## Conclusion\n\nMatchTIR addresses a genuine limitation in current reinforcement learning approaches for Tool-Integrated Reasoning by introducing structured, turn-level credit assignment via bipartite matching. The framework's ability to distinguish effective tool calls from redundant or erroneous ones in multi-turn scenarios represents a meaningful advance in training efficiency, demonstrated by smaller models achieving competitive performance with larger baselines.\n\nThe work is technically sound and practically motivated, with open-sourced artifacts enabling adoption. However, practitioners should be aware of the ground-truth annotation requirement and the need to validate performance on their specific tool sets and domains. For teams building agentic systems or complex reasoning pipelines, MatchTIR provides a principled approach to improving model efficiency without sacrificing task performance.",
        "linkedin_formatting": "MatchTIR introduces fine-grained supervision for Tool-Integrated Reasoning (TIR) via bipartite matching between predicted and ground-truth tool calls.[1][2]\n\nKey advances:\n- Hard (Kuhn-Munkres) and soft (Optimal Transport) assignment strategies for turn-level rewards, distinguishing effective from redundant tool calls.[1][3]\n- Dual-level advantage estimation balancing local precision with global task success.[1][4]\n- 4B model outperforms most 8B competitors on long-horizon, multi-turn benchmarks; code open-sourced.[1][2]\n\nTakeaway: Fine-grained credit assignment enables efficient TIR training for smaller models in complex agentic systems.\n\n#MatchTIR #ToolIntegratedReasoning #ReinforcementLearning #BipartiteMatching #LLMReasoning",
        "credibility_check": "Error: 'generated_output'"
      },
      "file_path": null,
      "status": "drafted",
      "published_url": null,
      "created_at": "2026-01-18 10:10:04",
      "published_at": null,
      "paper_title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
      "paper_url": "http://arxiv.org/abs/2601.10712v1",
      "paper_source": "arxiv"
    }
  ]
}