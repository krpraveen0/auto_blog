[
  {
    "id": "github_test_12345",
    "title": "test-user/awesome-ml-project",
    "url": "https://github.com/test-user/awesome-ml-project",
    "pdf_url": null,
    "summary": "An amazing machine learning project that makes AI accessible to everyone",
    "authors": [],
    "published": "2025-12-01T00:00:00Z",
    "updated": "2026-01-18T00:00:00Z",
    "category": null,
    "categories": [],
    "primary_category": null,
    "source": "github",
    "source_priority": "medium",
    "score": 0.0,
    "fetched_at": "2026-01-19T16:00:00Z",
    "created_at": "2026-01-19 16:41:46",
    "stars": 5432,
    "forks": 876,
    "watchers": 234,
    "open_issues": 45,
    "language": "Python",
    "topics": [
      "machine-learning",
      "deep-learning",
      "neural-networks"
    ],
    "license": "MIT",
    "languages": {
      "Python": 85000,
      "JavaScript": 12000,
      "HTML": 3000
    },
    "contributors_count": 42,
    "owner_type": "Organization",
    "stars_per_day": 110.24,
    "forks_per_day": 17.76,
    "activity_score": 1191.2,
    "days_since_creation": 49,
    "days_since_update": 1,
    "is_recently_active": 1,
    "has_wiki": 1,
    "has_pages": 1,
    "has_discussions": 0
  },
  {
    "id": "2601.10712v1",
    "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
    "url": "http://arxiv.org/abs/2601.10712v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10712v1",
    "summary": "Tool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by interleaving reasoning steps with external tool interactions. However, existing reinforcement learning methods typically rely on outcome- or trajectory-level rewards, assigning uniform advantages to all steps within a trajectory. This coarse-grained credit assignment fails to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios. To address this, we propose MatchTIR, a framework that introduces fine-grained supervision via bipartite matching-based turn-level reward assignment and dual-level advantage estimation. Specifically, we formulate credit assignment as a bipartite matching problem between predicted and ground-truth traces, utilizing two assignment strategies to derive dense turn-level rewards. Furthermore, to balance local step precision with global task success, we introduce a dual-level advantage estimation scheme that integrates turn-level and trajectory-level signals, assigning distinct advantage values to individual interaction turns. Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model surpasses the majority of 8B competitors, particularly in long-horizon and multi-turn tasks. Our codes are available at https://github.com/quchangle1/MatchTIR.",
    "authors": [
      "Changle Qu",
      "Sunhao Dai",
      "Hengyi Cai",
      "Jun Xu",
      "Shuaiqiang Wang",
      "Dawei Yin"
    ],
    "published": "2026-01-15T18:59:23+00:00",
    "updated": "2026-01-15T18:59:23+00:00",
    "category": "cs.AI",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "source": "arxiv",
    "source_priority": "high",
    "score": 9.0,
    "fetched_at": "2026-01-18T15:37:08.020359",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10681v1",
    "title": "Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems",
    "url": "http://arxiv.org/abs/2601.10681v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10681v1",
    "summary": "Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.",
    "authors": [
      "Amir Khurshid",
      "Abhishek Sehgal"
    ],
    "published": "2026-01-15T18:43:19+00:00",
    "updated": "2026-01-15T18:43:19+00:00",
    "category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "source": "arxiv",
    "source_priority": "high",
    "score": 9.0,
    "fetched_at": "2026-01-18T15:37:08.020489",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10581v1",
    "title": "From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA",
    "url": "http://arxiv.org/abs/2601.10581v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10581v1",
    "summary": "Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.",
    "authors": [
      "Kimia Abedini",
      "Farzad Shami",
      "Gianmaria Silvello"
    ],
    "published": "2026-01-15T16:54:11+00:00",
    "updated": "2026-01-15T16:54:11+00:00",
    "category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "source": "arxiv",
    "source_priority": "high",
    "score": 9.0,
    "fetched_at": "2026-01-18T15:37:08.020647",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10567v1",
    "title": "Generative AI collective behavior needs an interactionist paradigm",
    "url": "http://arxiv.org/abs/2601.10567v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10567v1",
    "summary": "In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.",
    "authors": [
      "Laura Ferrarotti",
      "Gian Maria Campedelli",
      "Roberto Dess\u00ec",
      "Andrea Baronchelli",
      "Giovanni Iacca",
      "Kathleen M. Carley",
      "Alex Pentland",
      "Joel Z. Leibo",
      "James Evans",
      "Bruno Lepri"
    ],
    "published": "2026-01-15T16:29:23+00:00",
    "updated": "2026-01-15T16:29:23+00:00",
    "category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "source": "arxiv",
    "source_priority": "high",
    "score": 9.0,
    "fetched_at": "2026-01-18T15:37:08.020678",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10543v1",
    "title": "Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing",
    "url": "http://arxiv.org/abs/2601.10543v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10543v1",
    "summary": "Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: https://github.com/zyz13590/SafeProbing.",
    "authors": [
      "Yinzhi Zhao",
      "Ming Wang",
      "Shi Feng",
      "Xiaocui Yang",
      "Daling Wang",
      "Yifei Zhang"
    ],
    "published": "2026-01-15T16:09:10+00:00",
    "updated": "2026-01-15T16:09:10+00:00",
    "category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "source": "arxiv",
    "source_priority": "high",
    "score": 9.0,
    "fetched_at": "2026-01-18T15:37:08.020738",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10527v1",
    "title": "A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5",
    "url": "http://arxiv.org/abs/2601.10527v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10527v1",
    "summary": "The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.",
    "authors": [
      "Xingjun Ma",
      "Yixu Wang",
      "Hengyuan Xu",
      "Yutao Wu",
      "Yifan Ding",
      "Yunhan Zhao",
      "Zilong Wang",
      "Jiabin Hua",
      "Ming Wen",
      "Jianan Liu",
      "Ranjie Duan",
      "Yifeng Gao",
      "Yingshui Tan",
      "Yunhao Chen",
      "Hui Xue",
      "Xin Wang",
      "Wei Cheng",
      "Jingjing Chen",
      "Zuxuan Wu",
      "Bo Li",
      "Yu-Gang Jiang"
    ],
    "published": "2026-01-15T15:52:52+00:00",
    "updated": "2026-01-15T15:52:52+00:00",
    "category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "source": "arxiv",
    "source_priority": "high",
    "score": 9.0,
    "fetched_at": "2026-01-18T15:37:08.020769",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10524v1",
    "title": "Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection",
    "url": "http://arxiv.org/abs/2601.10524v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10524v1",
    "summary": "The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover the root causes of their generalization failures. Our investigation reveals three critical findings: (1) Generalization is driven by a powerful synergy between architecture and data diversity. The Gemma 2 9B model achieves state-of-the-art performance (>91\\% F1), but only when trained on a stylistically diverse ``generalist'' dataset. (2) Generalization is highly architecture-dependent. We diagnose a specific failure mode in Llama 3.1 8B, which performs well on a narrow domain but cannot integrate diverse data, leading to a significant performance drop. (3) Some architectures are inherently more generalizable. The Mistral model proves to be a consistent and resilient performer across multiple training paradigms. By pinpointing the flawed heuristics responsible for these failures, our work provides a concrete methodology for diagnosing and understanding generalization failures, underscoring that reliable AI requires deep validation of the interplay between architecture, data, and training strategy.",
    "authors": [
      "Frank Bobe",
      "Gregory D. Vetaw",
      "Chase Pavlick",
      "Darshan Bryner",
      "Matthew Cook",
      "Jose Salas-Vernis"
    ],
    "published": "2026-01-15T15:51:24+00:00",
    "updated": "2026-01-15T15:51:24+00:00",
    "category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "source": "arxiv",
    "source_priority": "high",
    "score": 9.0,
    "fetched_at": "2026-01-18T15:37:08.020790",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10702v1",
    "title": "Grounding Agent Memory in Contextual Intent",
    "url": "http://arxiv.org/abs/2601.10702v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10702v1",
    "summary": "Deploying large language models in long-horizon, goal-oriented interactions remains challenging because similar entities and facts recur under different latent goals and constraints, causing memory systems to retrieve context-mismatched evidence. We propose STITCH (Structured Intent Tracking in Contextual History), an agentic memory system that indexes each trajectory step with a structured retrieval cue, contextual intent, and retrieves history by matching the current step's intent. Contextual intent provides compact signals that disambiguate repeated mentions and reduce interference: (1) the current latent goal defining a thematic segment, (2) the action type, and (3) the salient entity types anchoring which attributes matter. During inference, STITCH filters and prioritizes memory snippets by intent compatibility, suppressing semantically similar but context-incompatible history.\n  For evaluation, we introduce CAME-Bench, a benchmark for context-aware retrieval in realistic, dynamic, goal-oriented trajectories. Across CAME-Bench and LongMemEval, STITCH achieves state-of-the-art performance, outperforming the strongest baseline by 35.6%, with the largest gains as trajectory length increases. Our analysis shows that intent indexing substantially reduces retrieval noise, supporting intent-aware memory for robust long-horizon reasoning.",
    "authors": [
      "Ruozhen Yang",
      "Yucheng Jiang",
      "Yueqi Jiang",
      "Priyanka Kargupta",
      "Yunyi Zhang",
      "Jiawei Han"
    ],
    "published": "2026-01-15T18:55:13+00:00",
    "updated": "2026-01-15T18:55:13+00:00",
    "category": "cs.AI",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "source": "arxiv",
    "source_priority": "high",
    "score": 8.6,
    "fetched_at": "2026-01-18T15:37:08.020399",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10700v1",
    "title": "LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals",
    "url": "http://arxiv.org/abs/2601.10700v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10700v1",
    "summary": "Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods.",
    "authors": [
      "Gilat Toker",
      "Nitay Calderon",
      "Ohad Amosy",
      "Roi Reichart"
    ],
    "published": "2026-01-15T18:54:50+00:00",
    "updated": "2026-01-15T18:54:50+00:00",
    "category": "cs.AI",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "source": "arxiv",
    "source_priority": "high",
    "score": 8.6,
    "fetched_at": "2026-01-18T15:37:08.020423",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10684v1",
    "title": "On the origin of neural scaling laws: from random graphs to natural language",
    "url": "http://arxiv.org/abs/2601.10684v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10684v1",
    "summary": "Scaling laws have played a major role in the modern AI revolution, providing practitioners predictive power over how the model performance will improve with increasing data, compute, and number of model parameters. This has spurred an intense interest in the origin of neural scaling laws, with a common suggestion being that they arise from power law structure already present in the data. In this paper we study scaling laws for transformers trained to predict random walks (bigrams) on graphs with tunable complexity. We demonstrate that this simplified setting already gives rise to neural scaling laws even in the absence of power law structure in the data correlations. We further consider dialing down the complexity of natural language systematically, by training on sequences sampled from increasingly simplified generative language models, from 4,2,1-layer transformer language models down to language bigrams, revealing a monotonic evolution of the scaling exponents. Our results also include scaling laws obtained from training on random walks on random graphs drawn from Erd\u00f6s-Renyi and scale-free Barab\u00e1si-Albert ensembles. Finally, we revisit conventional scaling laws for language modeling, demonstrating that several essential results can be reproduced using 2 layer transformers with context length of 50, provide a critical analysis of various fits used in prior literature, demonstrate an alternative method for obtaining compute optimal curves as compared with current practice in published literature, and provide preliminary evidence that maximal update parameterization may be more parameter efficient than standard parameterization.",
    "authors": [
      "Maissam Barkeshli",
      "Alberto Alfarano",
      "Andrey Gromov"
    ],
    "published": "2026-01-15T18:46:09+00:00",
    "updated": "2026-01-15T18:46:09+00:00",
    "category": "cs.AI",
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "source": "arxiv",
    "source_priority": "high",
    "score": 8.6,
    "fetched_at": "2026-01-18T15:37:08.020468",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10679v1",
    "title": "Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models",
    "url": "http://arxiv.org/abs/2601.10679v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10679v1",
    "summary": "Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) \"Grokking\" dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM \"guesses\" the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be \"guessing\" instead of \"reasoning\". Leveraging this \"guessing\" picture, we propose three strategies to scale HRM's guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models \"reason\".",
    "authors": [
      "Zirui Ren",
      "Ziming Liu"
    ],
    "published": "2026-01-15T18:42:50+00:00",
    "updated": "2026-01-15T18:42:50+00:00",
    "category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "source": "arxiv",
    "source_priority": "high",
    "score": 8.6,
    "fetched_at": "2026-01-18T15:37:08.020510",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10587v1",
    "title": "Adversarial Evasion Attacks on Computer Vision using SHAP Values",
    "url": "http://arxiv.org/abs/2601.10587v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10587v1",
    "summary": "The paper introduces a white-box attack on computer vision models using SHAP values. It demonstrates how adversarial evasion attacks can compromise the performance of deep learning models by reducing output confidence or inducing misclassifications. Such attacks are particularly insidious as they can deceive the perception of an algorithm while eluding human perception due to their imperceptibility to the human eye. The proposed attack leverages SHAP values to quantify the significance of individual inputs to the output at the inference stage. A comparison is drawn between the SHAP attack and the well-known Fast Gradient Sign Method. We find evidence that SHAP attacks are more robust in generating misclassifications particularly in gradient hiding scenarios.",
    "authors": [
      "Frank Mollard",
      "Marcus Becker",
      "Florian Roehrbein"
    ],
    "published": "2026-01-15T16:58:55+00:00",
    "updated": "2026-01-15T16:58:55+00:00",
    "category": "cs.AI",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "source": "arxiv",
    "source_priority": "high",
    "score": 8.6,
    "fetched_at": "2026-01-18T15:37:08.020625",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10520v1",
    "title": "Breaking Up with Normatively Monolithic Agency with GRACE: A Reason-Based Neuro-Symbolic Architecture for Safe and Ethical AI Alignment",
    "url": "http://arxiv.org/abs/2601.10520v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10520v1",
    "summary": "As AI agents become increasingly autonomous, widely deployed in consequential contexts, and efficacious in bringing about real-world impacts, ensuring that their decisions are not only instrumentally effective but also normatively aligned has become critical. We introduce a neuro-symbolic reason-based containment architecture, Governor for Reason-Aligned ContainmEnt (GRACE), that decouples normative reasoning from instrumental decision-making and can contain AI agents of virtually any design. GRACE restructures decision-making into three modules: a Moral Module (MM) that determines permissible macro actions via deontic logic-based reasoning; a Decision-Making Module (DMM) that encapsulates the target agent while selecting instrumentally optimal primitive actions in accordance with derived macro actions; and a Guard that monitors and enforces moral compliance. The MM uses a reason-based formalism providing a semantic foundation for deontic logic, enabling interpretability, contestability, and justifiability. Its symbolic representation enriches the DMM's informational context and supports formal verification and statistical guarantees of alignment enforced by the Guard. We demonstrate GRACE on an example of a LLM therapy assistant, showing how it enables stakeholders to understand, contest, and refine agent behavior.",
    "authors": [
      "Felix Jahn",
      "Yannic Muskalla",
      "Lisa Dargasz",
      "Patrick Schramowski",
      "Kevin Baum"
    ],
    "published": "2026-01-15T15:47:38+00:00",
    "updated": "2026-01-15T15:47:38+00:00",
    "category": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "source": "arxiv",
    "source_priority": "high",
    "score": 8.6,
    "fetched_at": "2026-01-18T15:37:08.020814",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10673v1",
    "title": "Single-Stage Huffman Encoder for ML Compression",
    "url": "http://arxiv.org/abs/2601.10673v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10673v1",
    "summary": "Training and serving Large Language Models (LLMs) require partitioning data across multiple accelerators, where collective operations are frequently bottlenecked by network bandwidth. Lossless compression using Huffman codes is an effective way to alleviate the issue, however, its three-stage design requiring on-the-fly frequency analysis, codebook generation and transmission of codebook along with data introduces computational, latency and data overheads which are prohibitive for latency-sensitive scenarios such as die-to-die communication. This paper proposes a single-stage Huffman encoder that eliminates these overheads by using fixed codebooks derived from the average probability distribution of previous data batches. Through our analysis of the Gemma 2B model, we demonstrate that tensors exhibit high statistical similarity across layers and shards. Using this approach we achieve compression within 0.5% of per-shard Huffman coding and within 1% of the ideal Shannon compressibility, enabling efficient on-the-fly compression.",
    "authors": [
      "Aditya Agrawal",
      "Albert Magyar",
      "Hiteshwar Eswaraiah",
      "Patrick Sheridan",
      "Pradeep Janedula",
      "Ravi Krishnan Venkatesan",
      "Krishna Nair",
      "Ravi Iyer"
    ],
    "published": "2026-01-15T18:37:56+00:00",
    "updated": "2026-01-15T18:37:56+00:00",
    "category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "source": "arxiv",
    "source_priority": "high",
    "score": 8.6,
    "fetched_at": "2026-01-18T15:37:11.929793",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10657v1",
    "title": "PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution",
    "url": "http://arxiv.org/abs/2601.10657v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10657v1",
    "summary": "Large Language Models (LLMs) have emerged as powerful operators for evolutionary search, yet the design of efficient search scaffolds remains ad hoc. While promising, current LLM-in-the-loop systems lack a systematic approach to managing the evolutionary process. We identify three distinct failure modes: Context Pollution, where experiment history biases future candidate generation; Mode Collapse, where agents stagnate in local minima due to poor exploration-exploitation balance; and Weak Collaboration, where rigid crossover strategies fail to leverage parallel search trajectories effectively. We introduce Progress-Aware Consistent Evolution (PACEvolve), a framework designed to robustly govern the agent's context and search dynamics, to address these challenges. PACEvolve combines hierarchical context management (HCM) with pruning to address context pollution; momentum-based backtracking (MBB) to escape local minima; and a self-adaptive sampling policy that unifies backtracking and crossover for dynamic search coordination (CE), allowing agents to balance internal refinement with cross-trajectory collaboration. We demonstrate that PACEvolve provides a systematic path to consistent, long-horizon self-improvement, achieving state-of-the-art results on LLM-SR and KernelBench, while discovering solutions surpassing the record on Modded NanoGPT.",
    "authors": [
      "Minghao Yan",
      "Bo Peng",
      "Benjamin Coleman",
      "Ziqi Chen",
      "Zhouhang Xie",
      "Zhankui He",
      "Noveen Sachdeva",
      "Isabella Ye",
      "Weili Wang",
      "Chi Wang",
      "Ed H. Chi",
      "Wang-Cheng Kang",
      "Derek Zhiyuan Cheng",
      "Beidou Wang"
    ],
    "published": "2026-01-15T18:25:23+00:00",
    "updated": "2026-01-15T18:25:23+00:00",
    "category": "cs.LG",
    "categories": [
      "cs.NE",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "source": "arxiv",
    "source_priority": "high",
    "score": 8.6,
    "fetched_at": "2026-01-18T15:37:11.929822",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10639v1",
    "title": "STEM: Scaling Transformers with Embedding Modules",
    "url": "http://arxiv.org/abs/2601.10639v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10639v1",
    "summary": "Fine-grained sparsity promises higher parametric capacity without proportional per-token compute, but often suffers from training instability, load balancing, and communication overhead. We introduce STEM (Scaling Transformers with Embedding Modules), a static, token-indexed approach that replaces the FFN up-projection with a layer-local embedding lookup while keeping the gate and down-projection dense. This removes runtime routing, enables CPU offload with asynchronous prefetch, and decouples capacity from both per-token FLOPs and cross-device communication. Empirically, STEM trains stably despite extreme sparsity. It improves downstream performance over dense baselines while reducing per-token FLOPs and parameter accesses (eliminating roughly one-third of FFN parameters). STEM learns embedding spaces with large angular spread which enhances its knowledge storage capacity. More interestingly, this enhanced knowledge capacity comes with better interpretability. The token-indexed nature of STEM embeddings allows simple ways to perform knowledge editing and knowledge injection in an interpretable manner without any intervention in the input text or additional computation. In addition, STEM strengthens long-context performance: as sequence length grows, more distinct parameters are activated, yielding practical test-time capacity scaling. Across 350M and 1B model scales, STEM delivers up to ~3--4% accuracy improvements overall, with notable gains on knowledge and reasoning-heavy benchmarks (ARC-Challenge, OpenBookQA, GSM8K, MMLU). Overall, STEM is an effective way of scaling parametric memory while providing better interpretability, better training stability and improved efficiency.",
    "authors": [
      "Ranajoy Sadhukhan",
      "Sheng Cao",
      "Harry Dong",
      "Changsheng Zhao",
      "Attiano Purpura-Pontoniere",
      "Yuandong Tian",
      "Zechun Liu",
      "Beidi Chen"
    ],
    "published": "2026-01-15T18:00:27+00:00",
    "updated": "2026-01-15T18:00:27+00:00",
    "category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "source": "arxiv",
    "source_priority": "high",
    "score": 8.6,
    "fetched_at": "2026-01-18T15:37:11.929871",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10660v1",
    "title": "Detecting Winning Arguments with Large Language Models and Persuasion Strategies",
    "url": "http://arxiv.org/abs/2601.10660v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10660v1",
    "summary": "Detecting persuasion in argumentative text is a challenging task with important implications for understanding human communication. This work investigates the role of persuasion strategies - such as Attack on reputation, Distraction, and Manipulative wording - in determining the persuasiveness of a text. We conduct experiments on three annotated argument datasets: Winning Arguments (built from the Change My View subreddit), Anthropic/Persuasion, and Persuasion for Good. Our approach leverages large language models (LLMs) with a Multi-Strategy Persuasion Scoring approach that guides reasoning over six persuasion strategies. Results show that strategy-guided reasoning improves the prediction of persuasiveness. To better understand the influence of content, we organize the Winning Argument dataset into broad discussion topics and analyze performance across them. We publicly release this topic-annotated version of the dataset to facilitate future research. Overall, our methodology demonstrates the value of structured, strategy-aware prompting for enhancing interpretability and robustness in argument quality assessment.",
    "authors": [
      "Tiziano Labruna",
      "Arkadiusz Modzelewski",
      "Giorgio Satta",
      "Giovanni Da San Martino"
    ],
    "published": "2026-01-15T18:30:15+00:00",
    "updated": "2026-01-15T18:30:15+00:00",
    "category": "cs.CL",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "source": "arxiv",
    "source_priority": "high",
    "score": 8.6,
    "fetched_at": "2026-01-18T15:37:15.265322",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10589v1",
    "title": "Be Your Own Red Teamer: Safety Alignment via Self-Play and Reflective Experience Replay",
    "url": "http://arxiv.org/abs/2601.10589v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10589v1",
    "summary": "Large Language Models (LLMs) have achieved remarkable capabilities but remain vulnerable to adversarial ``jailbreak'' attacks designed to bypass safety guardrails. Current safety alignment methods depend heavily on static external red teaming, utilizing fixed defense prompts or pre-collected adversarial datasets. This leads to a rigid defense that overfits known patterns and fails to generalize to novel, sophisticated threats. To address this critical limitation, we propose empowering the model to be its own red teamer, capable of achieving autonomous and evolving adversarial attacks. Specifically, we introduce Safety Self- Play (SSP), a system that utilizes a single LLM to act concurrently as both the Attacker (generating jailbreaks) and the Defender (refusing harmful requests) within a unified Reinforcement Learning (RL) loop, dynamically evolving attack strategies to uncover vulnerabilities while simultaneously strengthening defense mechanisms. To ensure the Defender effectively addresses critical safety issues during the self-play, we introduce an advanced Reflective Experience Replay Mechanism, which uses an experience pool accumulated throughout the process. The mechanism employs a Upper Confidence Bound (UCB) sampling strategy to focus on failure cases with low rewards, helping the model learn from past hard mistakes while balancing exploration and exploitation. Extensive experiments demonstrate that our SSP approach autonomously evolves robust defense capabilities, significantly outperforming baselines trained on static adversarial datasets and establishing a new benchmark for proactive safety alignment.",
    "authors": [
      "Hao Wang",
      "Yanting Wang",
      "Hao Li",
      "Rui Li",
      "Lei Sha"
    ],
    "published": "2026-01-15T17:00:16+00:00",
    "updated": "2026-01-15T17:00:16+00:00",
    "category": "cs.CL",
    "categories": [
      "cs.CR",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "source": "arxiv",
    "source_priority": "high",
    "score": 8.6,
    "fetched_at": "2026-01-18T15:37:15.265370",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10532v1",
    "title": "PERM: Psychology-grounded Empathetic Reward Modeling for Large Language Models",
    "url": "http://arxiv.org/abs/2601.10532v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10532v1",
    "summary": "Large Language Models (LLMs) are increasingly deployed in human-centric applications, yet they often fail to provide substantive emotional support. While Reinforcement Learning (RL) has been utilized to enhance empathy of LLMs, existing reward models typically evaluate empathy from a single perspective, overlooking the inherently bidirectional interaction nature of empathy between the supporter and seeker as defined by Empathy Cycle theory. To address this limitation, we propose Psychology-grounded Empathetic Reward Modeling (PERM). PERM operationalizes empathy evaluation through a bidirectional decomposition: 1) Supporter perspective, assessing internal resonation and communicative expression; 2) Seeker perspective, evaluating emotional reception. Additionally, it incorporates a bystander perspective to monitor overall interaction quality. Extensive experiments on a widely-used emotional intelligence benchmark and an industrial daily conversation dataset demonstrate that PERM outperforms state-of-the-art baselines by over 10\\%. Furthermore, a blinded user study reveals a 70\\% preference for our approach, highlighting its efficacy in generating more empathetic responses. Our code, dataset, and models are available at https://github.com/ZhengWwwq/PERM.",
    "authors": [
      "Chengbing Wang",
      "Wuqiang Zheng",
      "Yang Zhang",
      "Fengbin Zhu",
      "Junyi Cheng",
      "Yi Xie",
      "Wenjie Wang",
      "Fuli Feng"
    ],
    "published": "2026-01-15T15:56:55+00:00",
    "updated": "2026-01-15T15:56:55+00:00",
    "category": "cs.CL",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "source": "arxiv",
    "source_priority": "high",
    "score": 8.6,
    "fetched_at": "2026-01-18T15:37:15.265489",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10710v1",
    "title": "From One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion",
    "url": "http://arxiv.org/abs/2601.10710v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10710v1",
    "summary": "Vision-Language Models (VLMs) create a severe visual feature bottleneck by using a crude, asymmetric connection that links only the output of the vision encoder to the input of the large language model (LLM). This static architecture fundamentally limits the ability of LLMs to achieve comprehensive alignment with hierarchical visual knowledge, compromising their capacity to accurately integrate local details with global semantics into coherent reasoning. To resolve this, we introduce Cross-Layer Injection (CLI), a novel and lightweight framework that forges a dynamic many-to-many bridge between the two modalities. CLI consists of two synergistic, parameter-efficient components: an Adaptive Multi-Projection (AMP) module that harmonizes features from diverse vision layers, and an Adaptive Gating Fusion (AGF) mechanism that empowers the LLM to selectively inject the most relevant visual information based on its real-time decoding context. We validate the effectiveness and versatility of CLI by integrating it into LLaVA-OneVision and LLaVA-1.5. Extensive experiments on 18 diverse benchmarks demonstrate significant performance improvements, establishing CLI as a scalable paradigm that unlocks deeper multimodal understanding by granting LLMs on-demand access to the full visual hierarchy.",
    "authors": [
      "Cheng Chen",
      "Yuyu Guo",
      "Pengpeng Zeng",
      "Jingkuan Song",
      "Peng Di",
      "Hang Yu",
      "Lianli Gao"
    ],
    "published": "2026-01-15T18:59:10+00:00",
    "updated": "2026-01-15T18:59:10+00:00",
    "category": "cs.CV",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "source": "arxiv",
    "source_priority": "high",
    "score": 8.6,
    "fetched_at": "2026-01-18T15:37:18.614184",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10551v1",
    "title": "Unleashing the Capabilities of Large Vision-Language Models for Intelligent Perception of Roadside Infrastructure",
    "url": "http://arxiv.org/abs/2601.10551v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10551v1",
    "summary": "Automated perception of urban roadside infrastructure is crucial for smart city management, yet general-purpose models often struggle to capture the necessary fine-grained attributes and domain rules. While Large Vision Language Models (VLMs) excel at open-world recognition, they often struggle to accurately interpret complex facility states in compliance with engineering standards, leading to unreliable performance in real-world applications. To address this, we propose a domain-adapted framework that transforms VLMs into specialized agents for intelligent infrastructure analysis. Our approach integrates a data-efficient fine-tuning strategy with a knowledge-grounded reasoning mechanism. Specifically, we leverage open-vocabulary fine-tuning on Grounding DINO to robustly localize diverse assets with minimal supervision, followed by LoRA-based adaptation on Qwen-VL for deep semantic attribute reasoning. To mitigate hallucinations and enforce professional compliance, we introduce a dual-modality Retrieval-Augmented Generation (RAG) module that dynamically retrieves authoritative industry standards and visual exemplars during inference. Evaluated on a comprehensive new dataset of urban roadside scenes, our framework achieves a detection performance of 58.9 mAP and an attribute recognition accuracy of 95.5%, demonstrating a robust solution for intelligent infrastructure monitoring.",
    "authors": [
      "Luxuan Fu",
      "Chong Liu",
      "Bisheng Yang",
      "Zhen Dong"
    ],
    "published": "2026-01-15T16:16:34+00:00",
    "updated": "2026-01-15T16:16:34+00:00",
    "category": "cs.CV",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "source": "arxiv",
    "source_priority": "high",
    "score": 8.6,
    "fetched_at": "2026-01-18T15:37:18.614509",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10504v1",
    "title": "DR-Arena: an Automated Evaluation Framework for Deep Research Agents",
    "url": "http://arxiv.org/abs/2601.10504v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10504v1",
    "summary": "As Large Language Models (LLMs) increasingly operate as Deep Research (DR) Agents capable of autonomous investigation and information synthesis, reliable evaluation of their task performance has become a critical bottleneck. Current benchmarks predominantly rely on static datasets, which suffer from several limitations: limited task generality, temporal misalignment, and data contamination. To address these, we introduce DR-Arena, a fully automated evaluation framework that pushes DR agents to their capability limits through dynamic investigation. DR-Arena constructs real-time Information Trees from fresh web trends to ensure the evaluation rubric is synchronized with the live world state, and employs an automated Examiner to generate structured tasks testing two orthogonal capabilities: Deep reasoning and Wide coverage. DR-Arena further adopts Adaptive Evolvement Loop, a state-machine controller that dynamically escalates task complexity based on real-time performance, demanding deeper deduction or wider aggregation until a decisive capability boundary emerges. Experiments with six advanced DR agents demonstrate that DR-Arena achieves a Spearman correlation of 0.94 with the LMSYS Search Arena leaderboard. This represents the state-of-the-art alignment with human preferences without any manual efforts, validating DR-Arena as a reliable alternative for costly human adjudication.",
    "authors": [
      "Yiwen Gao",
      "Ruochen Zhao",
      "Yang Deng",
      "Wenxuan Zhang"
    ],
    "published": "2026-01-15T15:28:21+00:00",
    "updated": "2026-01-15T15:28:21+00:00",
    "category": "cs.CL",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "source": "arxiv",
    "source_priority": "high",
    "score": 8.3,
    "fetched_at": "2026-01-18T15:37:15.265573",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10410v1",
    "title": "TF3-RO-50M: Training Compact Romanian Language Models from Scratch on Synthetic Moral Microfiction",
    "url": "http://arxiv.org/abs/2601.10410v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10410v1",
    "summary": "Recent advances in synthetic data generation have shown that compact language models can be trained effectively when the underlying corpus is structurally controlled and linguistically coherent. However, for morphologically rich and computationally under-resourced languages such as Romanian, there is still no openly documented, end-to-end pipeline that unifies tokenizer design, preprocessing, pretraining, compression, evaluation, and large-scale synthetic data generation in a reproducible framework. Building on TF1, a three-million-story English fable dataset, and TF2, which extends TF1 through high-quality Romanian translations, we introduce TF3-RO, a Romanian-centric language modeling pipeline spanning tokenizer training, from-scratch model development, and Romanian-native dataset generation. TF3-RO constructs Romanian-specific BPE and Unigram tokenizers from a linguistically informed corpus to mitigate token inflation induced by Romanian morphology. Using long-sequence packed training, we pretrain a 51.65M-parameter LLaMA-style Transformer entirely from scratch. The model is subsequently optimized through quantization, structured pruning, and logit-based knowledge distillation, yielding a compact 26.45M-parameter student model with tied embeddings and strong deployment characteristics. Using this distilled model, TF3-RO generates three million Romanian-native synthetic fables via a controlled combinatorial prompting framework. Across all stages, the pipeline integrates a comprehensive evaluation suite combining intrinsic metrics, Romanian agreement probes, entity coherence, rule-based grammar checking, and LLM-based assessment. TF3-RO provides a reproducible and linguistically grounded framework for training compact Romanian language models and producing large-scale synthetic narrative corpora.",
    "authors": [
      "Mihai Dan Nadas",
      "Laura Diosan",
      "Andreea Tomescu",
      "Andrei Piscoran"
    ],
    "published": "2026-01-15T14:02:00+00:00",
    "updated": "2026-01-15T14:02:00+00:00",
    "category": "cs.CL",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "source": "arxiv",
    "source_priority": "high",
    "score": 8.3,
    "fetched_at": "2026-01-18T15:37:15.265665",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10388v1",
    "title": "INDIC DIALECT: A Multi Task Benchmark to Evaluate and Translate in Indian Language Dialects",
    "url": "http://arxiv.org/abs/2601.10388v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10388v1",
    "summary": "Recent NLP advances focus primarily on standardized languages, leaving most low-resource dialects under-served especially in Indian scenarios. In India, the issue is particularly important: despite Hindi being the third most spoken language globally (over 600 million speakers), its numerous dialects remain underrepresented. The situation is similar for Odia, which has around 45 million speakers. While some datasets exist which contain standard Hindi and Odia languages, their regional dialects have almost no web presence. We introduce INDIC-DIALECT, a human-curated parallel corpus of 13k sentence pairs spanning 11 dialects and 2 languages: Hindi and Odia. Using this corpus, we construct a multi-task benchmark with three tasks: dialect classification, multiple-choice question (MCQ) answering, and machine translation (MT). Our experiments show that LLMs like GPT-4o and Gemini 2.5 perform poorly on the classification task. While fine-tuned transformer based models pretrained on Indian languages substantially improve performance e.g., improving F1 from 19.6\\% to 89.8\\% on dialect classification. For dialect to language translation, we find that hybrid AI model achieves highest BLEU score of 61.32 compared to the baseline score of 23.36. Interestingly, due to complexity in generating dialect sentences, we observe that for language to dialect translation the ``rule-based followed by AI\" approach achieves best BLEU score of 48.44 compared to the baseline score of 27.59. INDIC-DIALECT thus is a new benchmark for dialect-aware Indic NLP, and we plan to release it as open source to support further work on low-resource Indian dialects.",
    "authors": [
      "Tarun Sharma",
      "Manikandan Ravikiran",
      "Sourava Kumar Behera",
      "Pramit Bhattacharya",
      "Arnab Bhattacharya",
      "Rohit Saluja"
    ],
    "published": "2026-01-15T13:40:27+00:00",
    "updated": "2026-01-15T13:40:27+00:00",
    "category": "cs.CL",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "source": "arxiv",
    "source_priority": "high",
    "score": 8.3,
    "fetched_at": "2026-01-18T15:37:15.265689",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10562v1",
    "title": "Process-Guided Concept Bottleneck Model",
    "url": "http://arxiv.org/abs/2601.10562v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10562v1",
    "summary": "Concept Bottleneck Models (CBMs) improve the explainability of black-box Deep Learning (DL) by introducing intermediate semantic concepts. However, standard CBMs often overlook domain-specific relationships and causal mechanisms, and their dependence on complete concept labels limits applicability in scientific domains where supervision is sparse but processes are well defined. To address this, we propose the Process-Guided Concept Bottleneck Model (PG-CBM), an extension of CBMs which constrains learning to follow domain-defined causal mechanisms through biophysically meaningful intermediate concepts. Using above ground biomass density estimation from Earth Observation data as a case study, we show that PG-CBM reduces error and bias compared to multiple benchmarks, whilst leveraging multi-source heterogeneous training data and producing interpretable intermediate outputs. Beyond improved accuracy, PG-CBM enhances transparency, enables detection of spurious learning, and provides scientific insights, representing a step toward more trustworthy AI systems in scientific applications.",
    "authors": [
      "Reza M. Asiyabi",
      "SEOSAW Partnership",
      "Steven Hancock",
      "Casey Ryan"
    ],
    "published": "2026-01-15T16:25:55+00:00",
    "updated": "2026-01-15T16:25:55+00:00",
    "category": "cs.AI",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "source": "arxiv",
    "source_priority": "high",
    "score": 8.2,
    "fetched_at": "2026-01-18T15:37:08.020697",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10645v1",
    "title": "Influential Training Data Retrieval for Explaining Verbalized Confidence of LLMs",
    "url": "http://arxiv.org/abs/2601.10645v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10645v1",
    "summary": "Large language models (LLMs) can increase users' perceived trust by verbalizing confidence in their outputs. However, prior work has shown that LLMs are often overconfident, making their stated confidence unreliable since it does not consistently align with factual accuracy. To better understand the sources of this verbalized confidence, we introduce TracVC (\\textbf{Trac}ing \\textbf{V}erbalized \\textbf{C}onfidence), a method that builds on information retrieval and influence estimation to trace generated confidence expressions back to the training data. We evaluate TracVC on OLMo and Llama models in a question answering setting, proposing a new metric, content groundness, which measures the extent to which an LLM grounds its confidence in content-related training examples (relevant to the question and answer) versus in generic examples of confidence verbalization. Our analysis reveals that OLMo2-13B is frequently influenced by confidence-related data that is lexically unrelated to the query, suggesting that it may mimic superficial linguistic expressions of certainty rather than rely on genuine content grounding. These findings point to a fundamental limitation in current training regimes: LLMs may learn how to sound confident without learning when confidence is justified. Our analysis provides a foundation for improving LLMs' trustworthiness in expressing more reliable confidence.",
    "authors": [
      "Yuxi Xia",
      "Loris Schoenegger",
      "Benjamin Roth"
    ],
    "published": "2026-01-15T18:05:42+00:00",
    "updated": "2026-01-15T18:05:42+00:00",
    "category": "cs.CL",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "source": "arxiv",
    "source_priority": "high",
    "score": 8.2,
    "fetched_at": "2026-01-18T15:37:15.265345",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10649v1",
    "title": "CURVE: A Benchmark for Cultural and Multilingual Long Video Reasoning",
    "url": "http://arxiv.org/abs/2601.10649v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10649v1",
    "summary": "Recent advancements in video models have shown tremendous progress, particularly in long video understanding. However, current benchmarks predominantly feature western-centric data and English as the dominant language, introducing significant biases in evaluation. To address this, we introduce CURVE (Cultural Understanding and Reasoning in Video Evaluation), a challenging benchmark for multicultural and multilingual video reasoning. CURVE comprises high-quality, entirely human-generated annotations from diverse, region-specific cultural videos across 18 global locales. Unlike prior work that relies on automatic translations, CURVE provides complex questions, answers, and multi-step reasoning steps, all crafted in native languages. Making progress on CURVE requires a deeply situated understanding of visual cultural context. Furthermore, we leverage CURVE's reasoning traces to construct evidence-based graphs and propose a novel iterative strategy using these graphs to identify fine-grained errors in reasoning. Our evaluations reveal that SoTA Video-LLMs struggle significantly, performing substantially below human-level accuracy, with errors primarily stemming from the visual perception of cultural elements. CURVE will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file\\#minerva-cultural",
    "authors": [
      "Darshan Singh",
      "Arsha Nagrani",
      "Kawshik Manikantan",
      "Harman Singh",
      "Dinesh Tewari",
      "Tobias Weyand",
      "Cordelia Schmid",
      "Anelia Angelova",
      "Shachi Dave"
    ],
    "published": "2026-01-15T18:15:06+00:00",
    "updated": "2026-01-15T18:15:06+00:00",
    "category": "cs.CV",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "source": "arxiv",
    "source_priority": "high",
    "score": 8.2,
    "fetched_at": "2026-01-18T15:37:18.614235",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10535v1",
    "title": "SVII-3D: Advancing Roadside Infrastructure Inventory with Decimeter-level 3D Localization and Comprehension from Sparse Street Imagery",
    "url": "http://arxiv.org/abs/2601.10535v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10535v1",
    "summary": "The automated creation of digital twins and precise asset inventories is a critical task in smart city construction and facility lifecycle management. However, utilizing cost-effective sparse imagery remains challenging due to limited robustness, inaccurate localization, and a lack of fine-grained state understanding. To address these limitations, SVII-3D, a unified framework for holistic asset digitization, is proposed. First, LoRA fine-tuned open-set detection is fused with a spatial-attention matching network to robustly associate observations across sparse views. Second, a geometry-guided refinement mechanism is introduced to resolve structural errors, achieving precise decimeter-level 3D localization. Third, transcending static geometric mapping, a Vision-Language Model agent leveraging multi-modal prompting is incorporated to automatically diagnose fine-grained operational states. Experiments demonstrate that SVII-3D significantly improves identification accuracy and minimizes localization errors. Consequently, this framework offers a scalable, cost-effective solution for high-fidelity infrastructure digitization, effectively bridging the gap between sparse perception and automated intelligent maintenance.",
    "authors": [
      "Chong Liu",
      "Luxuan Fu",
      "Yang Jia",
      "Zhen Dong",
      "Bisheng Yang"
    ],
    "published": "2026-01-15T15:57:18+00:00",
    "updated": "2026-01-15T15:57:18+00:00",
    "category": "cs.CV",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "source": "arxiv",
    "source_priority": "high",
    "score": 8.2,
    "fetched_at": "2026-01-18T15:37:18.614558",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10521v1",
    "title": "BikeActions: An Open Platform and Benchmark for Cyclist-Centric VRU Action Recognition",
    "url": "http://arxiv.org/abs/2601.10521v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10521v1",
    "summary": "Anticipating the intentions of Vulnerable Road Users (VRUs) is a critical challenge for safe autonomous driving (AD) and mobile robotics. While current research predominantly focuses on pedestrian crossing behaviors from a vehicle's perspective, interactions within dense shared spaces remain underexplored. To bridge this gap, we introduce FUSE-Bike, the first fully open perception platform of its kind. Equipped with two LiDARs, a camera, and GNSS, it facilitates high-fidelity, close-range data capture directly from a cyclist's viewpoint. Leveraging this platform, we present BikeActions, a novel multi-modal dataset comprising 852 annotated samples across 5 distinct action classes, specifically tailored to improve VRU behavior modeling. We establish a rigorous benchmark by evaluating state-of-the-art graph convolution and transformer-based models on our publicly released data splits, establishing the first performance baselines for this challenging task. We release the full dataset together with data curation tools, the open hardware design, and the benchmark code to foster future research in VRU action understanding under https://iv.ee.hm.edu/bikeactions/.",
    "authors": [
      "Max A. Buettner",
      "Kanak Mazumder",
      "Luca Koecher",
      "Mario Finkbeiner",
      "Sebastian Niebler",
      "Fabian B. Flohr"
    ],
    "published": "2026-01-15T15:47:46+00:00",
    "updated": "2026-01-15T15:47:46+00:00",
    "category": "cs.CV",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "source": "arxiv",
    "source_priority": "high",
    "score": 8.2,
    "fetched_at": "2026-01-18T15:37:18.614613",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10611v1",
    "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
    "url": "http://arxiv.org/abs/2601.10611v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10611v1",
    "summary": "Today's strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&F on video tracking).",
    "authors": [
      "Christopher Clark",
      "Jieyu Zhang",
      "Zixian Ma",
      "Jae Sung Park",
      "Mohammadreza Salehi",
      "Rohun Tripathi",
      "Sangho Lee",
      "Zhongzheng Ren",
      "Chris Dongjoo Kim",
      "Yinuo Yang",
      "Vincent Shao",
      "Yue Yang",
      "Weikai Huang",
      "Ziqi Gao",
      "Taira Anderson",
      "Jianrui Zhang",
      "Jitesh Jain",
      "George Stoica",
      "Winson Han",
      "Ali Farhadi",
      "Ranjay Krishna"
    ],
    "published": "2026-01-15T17:27:44+00:00",
    "updated": "2026-01-15T17:27:44+00:00",
    "category": "cs.AI",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.8,
    "fetched_at": "2026-01-18T15:37:08.020560",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10600v1",
    "title": "Procedural Fairness in Multi-Agent Bandits",
    "url": "http://arxiv.org/abs/2601.10600v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10600v1",
    "summary": "In the context of multi-agent multi-armed bandits (MA-MAB), fairness is often reduced to outcomes: maximizing welfare, reducing inequality, or balancing utilities. However, evidence in psychology, economics, and Rawlsian theory suggests that fairness is also about process and who gets a say in the decisions being made. We introduce a new fairness objective, procedural fairness, which provides equal decision-making power for all agents, lies in the core, and provides for proportionality in outcomes. Empirical results confirm that fairness notions based on optimizing for outcomes sacrifice equal voice and representation, while the sacrifice in outcome-based fairness objectives (like equality and utilitarianism) is minimal under procedurally fair policies. We further prove that different fairness notions prioritize fundamentally different and incompatible values, highlighting that fairness requires explicit normative choices. This paper argues that procedural legitimacy deserves greater focus as a fairness objective, and provides a framework for putting procedural fairness into practice.",
    "authors": [
      "Joshua Caiata",
      "Carter Blair",
      "Kate Larson"
    ],
    "published": "2026-01-15T17:11:51+00:00",
    "updated": "2026-01-15T17:11:51+00:00",
    "category": "cs.AI",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.GT",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.8,
    "fetched_at": "2026-01-18T15:37:08.020581",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10560v1",
    "title": "Learning Latency-Aware Orchestration for Parallel Multi-Agent Systems",
    "url": "http://arxiv.org/abs/2601.10560v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10560v1",
    "summary": "Multi-agent systems (MAS) enable complex reasoning by coordinating multiple agents, but often incur high inference latency due to multi-step execution and repeated model invocations, severely limiting their scalability and usability in time-sensitive scenarios. Most existing approaches primarily optimize task performance and inference cost, and explicitly or implicitly assume sequential execution, making them less optimal for controlling latency under parallel execution. In this work, we investigate learning-based orchestration of multi-agent systems with explicit latency supervision under parallel execution. We propose Latency-Aware Multi-agent System (LAMaS), a latency-aware multi-agent orchestration framework that enables parallel execution and explicitly optimizes the critical execution path, allowing the controller to construct execution topology graphs with lower latency under parallel execution. Our experiments show that our approach reduces critical path length by 38-46% compared to the state-of-the-art baseline for multi-agent architecture search across multiple benchmarks, while maintaining or even improving task performance. These results highlight the importance of explicitly optimizing latency under parallel execution when designing efficient multi-agent systems. The code is available at https://github.com/xishi404/LAMaS",
    "authors": [
      "Xi Shi",
      "Mengxin Zheng",
      "Qian Lou"
    ],
    "published": "2026-01-15T16:23:53+00:00",
    "updated": "2026-01-15T16:23:53+00:00",
    "category": "cs.AI",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.MA",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.8,
    "fetched_at": "2026-01-18T15:37:08.020716",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10707v1",
    "title": "See Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection",
    "url": "http://arxiv.org/abs/2601.10707v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10707v1",
    "summary": "Recent advances in end-to-end autonomous driving show that policies trained on patch-aligned features extracted from foundation models generalize better to Out-of-Distribution (OOD). We hypothesize that due to the self-attention mechanism, each patch feature implicitly embeds/contains information from all other patches, represented in a different way and intensity, making these descriptors highly redundant. We quantify redundancy in such (BLIP2) features via PCA and cross-patch similarity: $90$% of variance is captured by $17/64$ principal components, and strong inter-token correlations are pervasive. Training on such overlapping information leads the policy to overfit spurious correlations, hurting OOD robustness. We present Stochastic-Patch-Selection (SPS), a simple yet effective approach for learning policies that are more robust, generalizable, and efficient. For every frame, SPS randomly masks a fraction of patch descriptors, not feeding them to the policy model, while preserving the spatial layout of the remaining patches. Thus, the policy is provided with different stochastic but complete views of the (same) scene: every random subset of patches acts like a different, yet still sensible, coherent projection of the world. The policy thus bases its decisions on features that are invariant to which specific tokens survive. Extensive experiments confirm that across all OOD scenarios, our method outperforms the state of the art (SOTA), achieving a $6.2$% average improvement and up to $20.4$% in closed-loop simulations, while being $2.4\\times$ faster. We conduct ablations over masking rates and patch-feature reorganization, training and evaluating 9 systems, with 8 of them surpassing prior SOTA. Finally, we show that the same learned policy transfers to a physical, real-world car without any tuning.",
    "authors": [
      "Amir Mallak",
      "Erfan Aasi",
      "Shiva Sreeram",
      "Tsun-Hsuan Wang",
      "Daniela Rus",
      "Alaa Maalouf"
    ],
    "published": "2026-01-15T18:58:33+00:00",
    "updated": "2026-01-15T18:58:33+00:00",
    "category": "cs.LG",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.8,
    "fetched_at": "2026-01-18T15:37:11.929651",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10583v1",
    "title": "Combinatorial Optimization Augmented Machine Learning",
    "url": "http://arxiv.org/abs/2601.10583v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10583v1",
    "summary": "Combinatorial optimization augmented machine learning (COAML) has recently emerged as a powerful paradigm for integrating predictive models with combinatorial decision-making. By embedding combinatorial optimization oracles into learning pipelines, COAML enables the construction of policies that are both data-driven and feasibility-preserving, bridging the traditions of machine learning, operations research, and stochastic optimization. This paper provides a comprehensive overview of the state of the art in COAML. We introduce a unifying framework for COAML pipelines, describe their methodological building blocks, and formalize their connection to empirical cost minimization. We then develop a taxonomy of problem settings based on the form of uncertainty and decision structure. Using this taxonomy, we review algorithmic approaches for static and dynamic problems, survey applications across domains such as scheduling, vehicle routing, stochastic programming, and reinforcement learning, and synthesize methodological contributions in terms of empirical cost minimization, imitation learning, and reinforcement learning. Finally, we identify key research frontiers. This survey aims to serve both as a tutorial introduction to the field and as a roadmap for future research at the interface of combinatorial optimization and machine learning.",
    "authors": [
      "Maximilian Schiffer",
      "Heiko Hoppe",
      "Yue Su",
      "Louis Bouvier",
      "Axel Parmentier"
    ],
    "published": "2026-01-15T16:55:19+00:00",
    "updated": "2026-01-15T16:55:19+00:00",
    "category": "cs.LG",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.8,
    "fetched_at": "2026-01-18T15:37:11.930020",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10606v1",
    "title": "RSATalker: Realistic Socially-Aware Talking Head Generation for Multi-Turn Conversation",
    "url": "http://arxiv.org/abs/2601.10606v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10606v1",
    "summary": "Talking head generation is increasingly important in virtual reality (VR), especially for social scenarios involving multi-turn conversation. Existing approaches face notable limitations: mesh-based 3D methods can model dual-person dialogue but lack realistic textures, while large-model-based 2D methods produce natural appearances but incur prohibitive computational costs. Recently, 3D Gaussian Splatting (3DGS) based methods achieve efficient and realistic rendering but remain speaker-only and ignore social relationships. We introduce RSATalker, the first framework that leverages 3DGS for realistic and socially-aware talking head generation with support for multi-turn conversation. Our method first drives mesh-based 3D facial motion from speech, then binds 3D Gaussians to mesh facets to render high-fidelity 2D avatar videos. To capture interpersonal dynamics, we propose a socially-aware module that encodes social relationships, including blood and non-blood as well as equal and unequal, into high-level embeddings through a learnable query mechanism. We design a three-stage training paradigm and construct the RSATalker dataset with speech-mesh-image triplets annotated with social relationships. Extensive experiments demonstrate that RSATalker achieves state-of-the-art performance in both realism and social awareness. The code and dataset will be released.",
    "authors": [
      "Peng Chen",
      "Xiaobao Wei",
      "Yi Yang",
      "Naiming Yao",
      "Hui Chen",
      "Feng Tian"
    ],
    "published": "2026-01-15T17:23:19+00:00",
    "updated": "2026-01-15T17:23:19+00:00",
    "category": "cs.CV",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.8,
    "fetched_at": "2026-01-18T15:37:18.614348",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10592v1",
    "title": "Action100M: A Large-scale Video Action Dataset",
    "url": "http://arxiv.org/abs/2601.10592v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10592v1",
    "summary": "Inferring physical actions from visual observations is a fundamental capability for advancing machine intelligence in the physical world. Achieving this requires large-scale, open-vocabulary video action datasets that span broad domains. We introduce Action100M, a large-scale dataset constructed from 1.2M Internet instructional videos (14.6 years of duration), yielding O(100 million) temporally localized segments with open-vocabulary action supervision and rich captions. Action100M is generated by a fully automated pipeline that (i) performs hierarchical temporal segmentation using V-JEPA 2 embeddings, (ii) produces multi-level frame and segment captions organized as a Tree-of-Captions, and (iii) aggregates evidence with a reasoning model (GPT-OSS-120B) under a multi-round Self-Refine procedure to output structured annotations (brief/detailed action, actor, brief/detailed caption). Training VL-JEPA on Action100M demonstrates consistent data-scaling improvements and strong zero-shot performance across diverse action recognition benchmarks, establishing Action100M as a new foundation for scalable research in video understanding and world modeling.",
    "authors": [
      "Delong Chen",
      "Tejaswi Kasarla",
      "Yejin Bang",
      "Mustafa Shukor",
      "Willy Chung",
      "Jade Yu",
      "Allen Bolourchi",
      "Theo Moutakanni",
      "Pascale Fung"
    ],
    "published": "2026-01-15T17:02:27+00:00",
    "updated": "2026-01-15T17:02:27+00:00",
    "category": "cs.CV",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.8,
    "fetched_at": "2026-01-18T15:37:18.614371",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10577v1",
    "title": "Jordan-Segmentable Masks: A Topology-Aware definition for characterizing Binary Image Segmentation",
    "url": "http://arxiv.org/abs/2601.10577v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10577v1",
    "summary": "Image segmentation plays a central role in computer vision. However, widely used evaluation metrics, whether pixel-wise, region-based, or boundary-focused, often struggle to capture the structural and topological coherence of a segmentation. In many practical scenarios, such as medical imaging or object delineation, small inaccuracies in boundary, holes, or fragmented predictions can result in high metric scores, despite the fact that the resulting masks fail to preserve the object global shape or connectivity. This highlights a limitation of conventional metrics: they are unable to assess whether a predicted segmentation partitions the image into meaningful interior and exterior regions.\n  In this work, we introduce a topology-aware notion of segmentation based on the Jordan Curve Theorem, and adapted for use in digital planes. We define the concept of a \\emph{Jordan-segmentatable mask}, which is a binary segmentation whose structure ensures a topological separation of the image domain into two connected components. We analyze segmentation masks through the lens of digital topology and homology theory, extracting a $4$-curve candidate from the mask, verifying its topological validity using Betti numbers. A mask is considered Jordan-segmentatable when this candidate forms a digital 4-curve with $\u03b2_0 = \u03b2_1 = 1$, or equivalently when its complement splits into exactly two $8$-connected components.\n  This framework provides a mathematically rigorous, unsupervised criterion with which to assess the structural coherence of segmentation masks. By combining digital Jordan theory and homological invariants, our approach provides a valuable alternative to standard evaluation metrics, especially in applications where topological correctness must be preserved.",
    "authors": [
      "Serena Grazia De Benedictis",
      "Amedeo Altavilla",
      "Nicoletta Del Buono"
    ],
    "published": "2026-01-15T16:47:53+00:00",
    "updated": "2026-01-15T16:47:53+00:00",
    "category": "cs.CV",
    "categories": [
      "cs.CV",
      "math.AT",
      "math.NA"
    ],
    "primary_category": "cs.CV",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.8,
    "fetched_at": "2026-01-18T15:37:18.614418",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10696v1",
    "title": "The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load",
    "url": "http://arxiv.org/abs/2601.10696v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10696v1",
    "summary": "Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users' prior expertise and interaction strategies through prompting.",
    "authors": [
      "Han Jiang",
      "Yao Xiao",
      "Rachel Hurley",
      "Shichao Liu"
    ],
    "published": "2026-01-15T18:52:59+00:00",
    "updated": "2026-01-15T18:52:59+00:00",
    "category": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.4,
    "fetched_at": "2026-01-18T15:37:08.020445",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10708v1",
    "title": "High-accuracy and dimension-free sampling with diffusions",
    "url": "http://arxiv.org/abs/2601.10708v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10708v1",
    "summary": "Diffusion models have shown remarkable empirical success in sampling from rich multi-modal distributions. Their inference relies on numerically solving a certain differential equation. This differential equation cannot be solved in closed form, and its resolution via discretization typically requires many small iterations to produce \\emph{high-quality} samples.\n  More precisely, prior works have shown that the iteration complexity of discretization methods for diffusion models scales polynomially in the ambient dimension and the inverse accuracy $1/\\varepsilon$. In this work, we propose a new solver for diffusion models relying on a subtle interplay between low-degree approximation and the collocation method (Lee, Song, Vempala 2018), and we prove that its iteration complexity scales \\emph{polylogarithmically} in $1/\\varepsilon$, yielding the first ``high-accuracy'' guarantee for a diffusion-based sampler that only uses (approximate) access to the scores of the data distribution. In addition, our bound does not depend explicitly on the ambient dimension; more precisely, the dimension affects the complexity of our solver through the \\emph{effective radius} of the support of the target distribution only.",
    "authors": [
      "Khashayar Gatmiry",
      "Sitan Chen",
      "Adil Salim"
    ],
    "published": "2026-01-15T18:58:50+00:00",
    "updated": "2026-01-15T18:58:50+00:00",
    "category": "cs.LG",
    "categories": [
      "cs.LG",
      "math.ST"
    ],
    "primary_category": "cs.LG",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.4,
    "fetched_at": "2026-01-18T15:37:11.929624",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10705v1",
    "title": "Distributed Perceptron under Bounded Staleness, Partial Participation, and Noisy Communication",
    "url": "http://arxiv.org/abs/2601.10705v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10705v1",
    "summary": "We study a semi-asynchronous client-server perceptron trained via iterative parameter mixing (IPM-style averaging): clients run local perceptron updates and a server forms a global model by aggregating the updates that arrive in each communication round. The setting captures three system effects in federated and distributed deployments: (i) stale updates due to delayed model delivery and delayed application of client computations (two-sided version lag), (ii) partial participation (intermittent client availability), and (iii) imperfect communication on both downlink and uplink, modeled as effective zero-mean additive noise with bounded second moment. We introduce a server-side aggregation rule called staleness-bucket aggregation with padding that deterministically enforces a prescribed staleness profile over update ages without assuming any stochastic model for delays or participation. Under margin separability and bounded data radius, we prove a finite-horizon expected bound on the cumulative weighted number of perceptron mistakes over a given number of server rounds: the impact of delay appears only through the mean enforced staleness, whereas communication noise contributes an additional term that grows on the order of the square root of the horizon with the total noise energy. In the noiseless case, we show how a finite expected mistake budget yields an explicit finite-round stabilization bound under a mild fresh-participation condition.",
    "authors": [
      "Keval Jain",
      "Anant Raj",
      "Saurav Prakash",
      "Girish Varma"
    ],
    "published": "2026-01-15T18:56:54+00:00",
    "updated": "2026-01-15T18:56:54+00:00",
    "category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.4,
    "fetched_at": "2026-01-18T15:37:11.929675",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10701v1",
    "title": "Communication-Efficient and Privacy-Adaptable Mechanism -- a Federated Learning Scheme with Convergence Analysis",
    "url": "http://arxiv.org/abs/2601.10701v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10701v1",
    "summary": "Federated learning enables multiple parties to jointly train learning models without sharing their own underlying data, offering a practical pathway to privacy-preserving collaboration under data-governance constraints. Continued study of federated learning is essential to address key challenges in it, including communication efficiency and privacy protection between parties. A recent line of work introduced a novel approach called the Communication-Efficient and Privacy-Adaptable Mechanism (CEPAM), which achieves both objectives simultaneously. CEPAM leverages the rejection-sampled universal quantizer (RSUQ), a randomized vector quantizer whose quantization error is equivalent to a prescribed noise, which can be tuned to customize privacy protection between parties. In this work, we theoretically analyze the privacy guarantees and convergence properties of CEPAM. Moreover, we assess CEPAM's utility performance through experimental evaluations, including convergence profiles compared with other baselines, and accuracy-privacy trade-offs between different parties.",
    "authors": [
      "Chun Hei Michael Shiu",
      "Chih Wei Ling"
    ],
    "published": "2026-01-15T18:55:00+00:00",
    "updated": "2026-01-15T18:55:00+00:00",
    "category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.4,
    "fetched_at": "2026-01-18T15:37:11.929697",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10690v1",
    "title": "Data-driven stochastic reduced-order modeling of parametrized dynamical systems",
    "url": "http://arxiv.org/abs/2601.10690v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10690v1",
    "summary": "Modeling complex dynamical systems under varying conditions is computationally intensive, often rendering high-fidelity simulations intractable. Although reduced-order models (ROMs) offer a promising solution, current methods often struggle with stochastic dynamics and fail to quantify prediction uncertainty, limiting their utility in robust decision-making contexts. To address these challenges, we introduce a data-driven framework for learning continuous-time stochastic ROMs that generalize across parameter spaces and forcing conditions. Our approach, based on amortized stochastic variational inference, leverages a reparametrization trick for Markov Gaussian processes to eliminate the need for computationally expensive forward solvers during training. This enables us to jointly learn a probabilistic autoencoder and stochastic differential equations governing the latent dynamics, at a computational cost that is independent of the dataset size and system stiffness. Additionally, our approach offers the flexibility of incorporating physics-informed priors if available. Numerical studies are presented for three challenging test problems, where we demonstrate excellent generalization to unseen parameter combinations and forcings, and significant efficiency gains compared to existing approaches.",
    "authors": [
      "Andrew F. Ilersich",
      "Kevin Course",
      "Prasanth B. Nair"
    ],
    "published": "2026-01-15T18:50:18+00:00",
    "updated": "2026-01-15T18:50:18+00:00",
    "category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.4,
    "fetched_at": "2026-01-18T15:37:11.929720",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10628v1",
    "title": "Parametric RDT approach to computational gap of symmetric binary perceptron",
    "url": "http://arxiv.org/abs/2601.10628v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10628v1",
    "summary": "We study potential presence of statistical-computational gaps (SCG) in symmetric binary perceptrons (SBP) via a parametric utilization of \\emph{fully lifted random duality theory} (fl-RDT) [96]. A structural change from decreasingly to arbitrarily ordered $c$-sequence (a key fl-RDT parametric component) is observed on the second lifting level and associated with \\emph{satisfiability} ($\u03b1_c$) -- \\emph{algorithmic} ($\u03b1_a$) constraints density threshold change thereby suggesting a potential existence of a nonzero computational gap $SCG=\u03b1_c-\u03b1_a$. The second level estimate is shown to match the theoretical $\u03b1_c$ whereas the $r\\rightarrow \\infty$ level one is proposed to correspond to $\u03b1_a$. For example, for the canonical SBP ($\u03ba=1$ margin) we obtain $\u03b1_c\\approx 1.8159$ on the second and $\u03b1_a\\approx 1.6021$ (with converging tendency towards $\\sim 1.59$ range) on the seventh level. Our propositions remarkably well concur with recent literature: (i) in [20] local entropy replica approach predicts $\u03b1_{LE}\\approx 1.58$ as the onset of clustering defragmentation (presumed driving force behind locally improving algorithms failures); (ii) in $\u03b1\\rightarrow 0$ regime we obtain on the third lifting level $\u03ba\\approx 1.2385\\sqrt{\\frac{\u03b1_a}{-\\log\\left ( \u03b1_a \\right ) }}$ which qualitatively matches overlap gap property (OGP) based predictions of [43] and identically matches local entropy based predictions of [24]; (iii) $c$-sequence ordering change phenomenology mirrors the one observed in asymmetric binary perceptron (ABP) in [98] and the negative Hopfield model in [100]; and (iv) as in [98,100], we here design a CLuP based algorithm whose practical performance closely matches proposed theoretical predictions.",
    "authors": [
      "Mihailo Stojnic"
    ],
    "published": "2026-01-15T17:48:58+00:00",
    "updated": "2026-01-15T17:48:58+00:00",
    "category": "cs.LG",
    "categories": [
      "stat.ML",
      "cond-mat.dis-nn",
      "cs.IT",
      "cs.LG",
      "math.PR"
    ],
    "primary_category": "stat.ML",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.4,
    "fetched_at": "2026-01-18T15:37:11.929922",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10566v1",
    "title": "Representation-Aware Unlearning via Activation Signatures: From Suppression to Knowledge-Signature Erasure",
    "url": "http://arxiv.org/abs/2601.10566v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10566v1",
    "summary": "Selective knowledge erasure from LLMs is critical for GDPR compliance and model safety, yet current unlearning methods conflate behavioral suppression with true knowledge removal, allowing latent capabilities to persist beneath surface-level refusals. In this work, we address this challenge by introducing Knowledge Immunization Framework (KIF), a representation-aware architecture that distinguishes genuine erasure from obfuscation by targeting internal activation signatures rather than surface outputs. Our approach combines dynamic suppression of subject-specific representations with parameter-efficient adaptation, enabling durable unlearning without full model retraining. KIF achieves near-oracle erasure (FQ approx 0.99 vs. 1.00) while preserving utility at oracle levels (MU = 0.62), effectively breaking the stability-erasure tradeoff that has constrained all prior work. We evaluate both standard foundation models (Llama and Mistral) and reasoning-prior models (Qwen and DeepSeek) across 3B to 14B parameters. Our observation shows that standard models exhibit scale-independent true erasure (<3% utility drift), while reasoning-prior models reveal fundamental architectural divergence. Our comprehensive dual-metric evaluation protocol, combining surface-level leakage with latent trace persistence, operationalizes the obfuscation - erasure distinction and enables the first systematic diagnosis of mechanism-level forgetting behavior across model families and scales.",
    "authors": [
      "Syed Naveed Mahmood",
      "Md. Rezaur Rahman Bhuiyan",
      "Tasfia Zaman",
      "Jareen Tasneem Khondaker",
      "Md. Sameer Sakib",
      "Nazia Tasnim",
      "Farig Sadeque"
    ],
    "published": "2026-01-15T16:28:14+00:00",
    "updated": "2026-01-15T16:28:14+00:00",
    "category": "cs.LG",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.4,
    "fetched_at": "2026-01-18T15:37:11.930072",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10714v1",
    "title": "Alterbute: Editing Intrinsic Attributes of Objects in Images",
    "url": "http://arxiv.org/abs/2601.10714v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10714v1",
    "summary": "We introduce Alterbute, a diffusion-based method for editing an object's intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., ''Porsche 911 Carrera'') that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing.",
    "authors": [
      "Tal Reiss",
      "Daniel Winter",
      "Matan Cohen",
      "Alex Rav-Acha",
      "Yael Pritch",
      "Ariel Shamir",
      "Yedid Hoshen"
    ],
    "published": "2026-01-15T18:59:53+00:00",
    "updated": "2026-01-15T18:59:53+00:00",
    "category": "cs.CV",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.4,
    "fetched_at": "2026-01-18T15:37:18.614157",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10632v1",
    "title": "CoMoVi: Co-Generation of 3D Human Motions and Realistic Videos",
    "url": "http://arxiv.org/abs/2601.10632v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10632v1",
    "summary": "In this paper, we find that the generation of 3D human motions and 2D human videos is intrinsically coupled. 3D motions provide the structural prior for plausibility and consistency in videos, while pre-trained video models offer strong generalization capabilities for motions, which necessitate coupling their generation processes. Based on this, we present CoMoVi, a co-generative framework that couples two video diffusion models (VDMs) to generate 3D human motions and videos synchronously within a single diffusion denoising loop. To achieve this, we first propose an effective 2D human motion representation that can inherit the powerful prior of pre-trained VDMs. Then, we design a dual-branch diffusion model to couple human motion and video generation process with mutual feature interaction and 3D-2D cross attentions. Moreover, we curate CoMoVi Dataset, a large-scale real-world human video dataset with text and motion annotations, covering diverse and challenging human motions. Extensive experiments demonstrate the effectiveness of our method in both 3D human motion and video generation tasks.",
    "authors": [
      "Chengfeng Zhao",
      "Jiazhi Shu",
      "Yubo Zhao",
      "Tianyu Huang",
      "Jiahao Lu",
      "Zekai Gu",
      "Chengwei Ren",
      "Zhiyang Dou",
      "Qing Shuai",
      "Yuan Liu"
    ],
    "published": "2026-01-15T17:52:29+00:00",
    "updated": "2026-01-15T17:52:29+00:00",
    "category": "cs.CV",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.4,
    "fetched_at": "2026-01-18T15:37:18.614267",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10607v1",
    "title": "Multi-Objective Pareto-Front Optimization for Efficient Adaptive VVC Streaming",
    "url": "http://arxiv.org/abs/2601.10607v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10607v1",
    "summary": "Adaptive video streaming has facilitated improved video streaming over the past years. A balance among coding performance objectives such as bitrate, video quality, and decoding complexity is required to achieve efficient, content- and codec-dependent, adaptive video streaming. This paper proposes a multi-objective Pareto-front (PF) optimization framework to construct quality-monotonic, content-adaptive bitrate ladders Versatile Video Coding (VVC) streaming that jointly optimize video quality, bitrate, and decoding time, which is used as a practical proxy for decoding energy. Two strategies are introduced: the Joint Rate-Quality-Time Pareto Front (JRQT-PF) and the Joint Quality-Time Pareto Front (JQT-PF), each exploring different tradeoff formulations and objective prioritizations. The ladders are constructed under quality monotonicity constraints during adaptive streaming to ensure a consistent Quality of Experience (QoE). Experiments are conducted on a large-scale UHD dataset (Inter-4K), with quality assessed using PSNR, VMAF, and XPSNR, and complexity measured via decoding time and energy consumption. The JQT-PF method achieves 11.76% average bitrate savings while reducing average decoding time by 0.29% to maintain the same XPSNR, compared to a widely-used fixed ladder. More aggressive configurations yield up to 27.88% bitrate savings at the cost of increased complexity. The JRQT-PF strategy, on the other hand, offers more controlled tradeoffs, achieving 6.38 % bitrate savings and 6.17 % decoding time reduction. This framework outperforms existing methods, including fixed ladders, VMAF- and XPSNR-based dynamic resolution selection, and complexity-aware benchmarks. The results confirm that PF optimization with decoding time constraints enables sustainable, high-quality streaming tailored to network and device capabilities.",
    "authors": [
      "Angeliki Katsenou",
      "Vignesh V. Menon",
      "Guoda Laurinaviciute",
      "Benjamin Bross",
      "Detlev Marpe"
    ],
    "published": "2026-01-15T17:23:39+00:00",
    "updated": "2026-01-15T17:23:39+00:00",
    "category": "cs.CV",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.4,
    "fetched_at": "2026-01-18T15:37:18.614324",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10553v1",
    "title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models",
    "url": "http://arxiv.org/abs/2601.10553v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10553v1",
    "summary": "State-of-the-art video generative models produce promising visual content yet often violate basic physics principles, limiting their utility. While some attribute this deficiency to insufficient physics understanding from pre-training, we find that the shortfall in physics plausibility also stems from suboptimal inference strategies. We therefore introduce WMReward and treat improving physics plausibility of video generation as an inference-time alignment problem. In particular, we leverage the strong physics prior of a latent world model (here, VJEPA-2) as a reward to search and steer multiple candidate denoising trajectories, enabling scaling test-time compute for better generation performance. Empirically, our approach substantially improves physics plausibility across image-conditioned, multiframe-conditioned, and text-conditioned generation settings, with validation from human preference study. Notably, in the ICCV 2025 Perception Test PhysicsIQ Challenge, we achieve a final score of 62.64%, winning first place and outperforming the previous state of the art by 7.42%. Our work demonstrates the viability of using latent world models to improve physics plausibility of video generation, beyond this specific instantiation or parameterization.",
    "authors": [
      "Jianhao Yuan",
      "Xiaofeng Zhang",
      "Felix Friedrich",
      "Nicolas Beltran-Velez",
      "Melissa Hall",
      "Reyhane Askari-Hemmat",
      "Xiaochuang Han",
      "Nicolas Ballas",
      "Michal Drozdzal",
      "Adriana Romero-Soriano"
    ],
    "published": "2026-01-15T16:18:00+00:00",
    "updated": "2026-01-15T16:18:00+00:00",
    "category": "cs.CV",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.4,
    "fetched_at": "2026-01-18T15:37:18.614487",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10460v1",
    "title": "Contextual StereoSet: Stress-Testing Bias Alignment Robustness in Large Language Models",
    "url": "http://arxiv.org/abs/2601.10460v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10460v1",
    "summary": "A model that avoids stereotypes in a lab benchmark may not avoid them in deployment. We show that measured bias shifts dramatically when prompts mention different places, times, or audiences -- no adversarial prompting required.\n  We introduce Contextual StereoSet, a benchmark that holds stereotype content fixed while systematically varying contextual framing. Testing 13 models across two protocols, we find striking patterns: anchoring to 1990 (vs. 2030) raises stereotype selection in all models tested on this contrast (p<0.05); gossip framing raises it in 5 of 6 full-grid models; out-group observer framing shifts it by up to 13 percentage points. These effects replicate in hiring, lending, and help-seeking vignettes.\n  We propose Context Sensitivity Fingerprints (CSF): a compact profile of per-dimension dispersion and paired contrasts with bootstrap CIs and FDR correction. Two evaluation tracks support different use cases -- a 360-context diagnostic grid for deep analysis and a budgeted protocol covering 4,229 items for production screening.\n  The implication is methodological: bias scores from fixed-condition tests may not generalize.This is not a claim about ground-truth bias rates; it is a stress test of evaluation robustness. CSF forces evaluators to ask, \"Under what conditions does bias appear?\" rather than \"Is this model biased?\" We release our benchmark, code, and results.",
    "authors": [
      "Abhinaba Basu",
      "Pavan Chakraborty"
    ],
    "published": "2026-01-15T14:50:49+00:00",
    "updated": "2026-01-15T14:50:49+00:00",
    "category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.1,
    "fetched_at": "2026-01-18T15:37:15.265596",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10455v1",
    "title": "SurgGoal: Rethinking Surgical Planning Evaluation via Goal-Satisfiability",
    "url": "http://arxiv.org/abs/2601.10455v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10455v1",
    "summary": "Surgical planning integrates visual perception, long-horizon reasoning, and procedural knowledge, yet it remains unclear whether current evaluation protocols reliably assess vision-language models (VLMs) in safety-critical settings. Motivated by a goal-oriented view of surgical planning, we define planning correctness via phase-goal satisfiability, where plan validity is determined by expert-defined surgical rules. Based on this definition, we introduce a multicentric meta-evaluation benchmark with valid procedural variations and invalid plans containing order and content errors. Using this benchmark, we show that sequence similarity metrics systematically misjudge planning quality, penalizing valid plans while failing to identify invalid ones. We therefore adopt a rule-based goal-satisfiability metric as a high-precision meta-evaluation reference to assess Video-LLMs under progressively constrained settings, revealing failures due to perception errors and under-constrained reasoning. Structural knowledge consistently improves performance, whereas semantic guidance alone is unreliable and benefits larger models only when combined with structural constraints.",
    "authors": [
      "Ruochen Li",
      "Kun Yuan",
      "Yufei Xia",
      "Yue Zhou",
      "Qingyu Lu",
      "Weihang Li",
      "Youxiang Zhu",
      "Nassir Navab"
    ],
    "published": "2026-01-15T14:47:26+00:00",
    "updated": "2026-01-15T14:47:26+00:00",
    "category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.RO"
    ],
    "primary_category": "cs.CL",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.1,
    "fetched_at": "2026-01-18T15:37:15.265621",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10421v1",
    "title": "Are Language Models Models?",
    "url": "http://arxiv.org/abs/2601.10421v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10421v1",
    "summary": "Futrell and Mahowald claim LMs \"serve as model systems\", but an assessment at each of Marr's three levels suggests the claim is clearly not true at the implementation level, poorly motivated at the algorithmic-representational level, and problematic at the computational theory level. LMs are good candidates as tools; calling them cognitive models overstates the case and unnecessarily feeds LLM hype.",
    "authors": [
      "Philip Resnik"
    ],
    "published": "2026-01-15T14:13:01+00:00",
    "updated": "2026-01-15T14:13:01+00:00",
    "category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.1,
    "fetched_at": "2026-01-18T15:37:15.265641",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10387v1",
    "title": "The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models",
    "url": "http://arxiv.org/abs/2601.10387v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10387v1",
    "summary": "Large language models can represent a variety of personas but typically default to a helpful Assistant identity cultivated during post-training. We investigate the structure of the space of model personas by extracting activation directions corresponding to diverse character archetypes. Across several different models, we find that the leading component of this persona space is an \"Assistant Axis,\" which captures the extent to which a model is operating in its default Assistant mode. Steering towards the Assistant direction reinforces helpful and harmless behavior; steering away increases the model's tendency to identify as other entities. Moreover, steering away with more extreme values often induces a mystical, theatrical speaking style. We find this axis is also present in pre-trained models, where it primarily promotes helpful human archetypes like consultants and coaches and inhibits spiritual ones. Measuring deviations along the Assistant Axis predicts \"persona drift,\" a phenomenon where models slip into exhibiting harmful or bizarre behaviors that are uncharacteristic of their typical persona. We find that persona drift is often driven by conversations demanding meta-reflection on the model's processes or featuring emotionally vulnerable users. We show that restricting activations to a fixed region along the Assistant Axis can stabilize model behavior in these scenarios -- and also in the face of adversarial persona-based jailbreaks. Our results suggest that post-training steers models toward a particular region of persona space but only loosely tethers them to it, motivating work on training and steering strategies that more deeply anchor models to a coherent persona.",
    "authors": [
      "Christina Lu",
      "Jack Gallagher",
      "Jonathan Michala",
      "Kyle Fish",
      "Jack Lindsey"
    ],
    "published": "2026-01-15T13:40:06+00:00",
    "updated": "2026-01-15T13:40:06+00:00",
    "category": "cs.CL",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.1,
    "fetched_at": "2026-01-18T15:37:15.265715",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10715v1",
    "title": "DInf-Grid: A Neural Differential Equation Solver with Differentiable Feature Grids",
    "url": "http://arxiv.org/abs/2601.10715v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10715v1",
    "summary": "We present a novel differentiable grid-based representation for efficiently solving differential equations (DEs). Widely used architectures for neural solvers, such as sinusoidal neural networks, are coordinate-based MLPs that are both computationally intensive and slow to train. Although grid-based alternatives for implicit representations (e.g., Instant-NGP and K-Planes) train faster by exploiting signal structure, their reliance on linear interpolation restricts their ability to compute higher-order derivatives, rendering them unsuitable for solving DEs. Our approach overcomes these limitations by combining the efficiency of feature grids with radial basis function interpolation, which is infinitely differentiable. To effectively capture high-frequency solutions and enable stable and faster computation of global gradients, we introduce a multi-resolution decomposition with co-located grids. Our proposed representation, DInf-Grid, is trained implicitly using the differential equations as loss functions, enabling accurate modelling of physical fields. We validate DInf-Grid on a variety of tasks, including the Poisson equation for image reconstruction, the Helmholtz equation for wave fields, and the Kirchhoff-Love boundary value problem for cloth simulation. Our results demonstrate a 5-20x speed-up over coordinate-based MLP-based methods, solving differential equations in seconds or minutes while maintaining comparable accuracy and compactness.",
    "authors": [
      "Navami Kairanda",
      "Shanthika Naik",
      "Marc Habermann",
      "Avinash Sharma",
      "Christian Theobalt",
      "Vladislav Golyanik"
    ],
    "published": "2026-01-15T18:59:57+00:00",
    "updated": "2026-01-15T18:59:57+00:00",
    "category": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.0,
    "fetched_at": "2026-01-18T15:37:11.929595",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  },
  {
    "id": "2601.10537v1",
    "title": "Enhancing the quality of gauge images captured in smoke and haze scenes through deep learning",
    "url": "http://arxiv.org/abs/2601.10537v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10537v1",
    "summary": "Images captured in hazy and smoky environments suffer from reduced visibility, posing a challenge when monitoring infrastructures and hindering emergency services during critical situations. The proposed work investigates the use of the deep learning models to enhance the automatic, machine-based readability of gauge in smoky environments, with accurate gauge data interpretation serving as a valuable tool for first responders. The study utilizes two deep learning architectures, FFA-Net and AECR-Net, to improve the visibility of gauge images, corrupted with light up to dense haze and smoke. Since benchmark datasets of analog gauge images are unavailable, a new synthetic dataset, containing over 14,000 images, was generated using the Unreal Engine. The models were trained with an 80\\% train, 10\\% validation, and 10\\% test split for the haze and smoke dataset, respectively. For the synthetic haze dataset, the SSIM and PSNR metrics are about 0.98 and 43\\,dB, respectively, comparing well to state-of-the art results. Additionally, more robust results are retrieved from the AECR-Net, when compared to the FFA-Net. Although the results from the synthetic smoke dataset are poorer, the trained models achieve interesting results. In general, imaging in the presence of smoke are more difficult to enhance given the inhomogeneity and high density. Secondly, FFA-Net and AECR-Net are implemented to dehaze and not to desmoke images. This work shows that use of deep learning architectures can improve the quality of analog gauge images captured in smoke and haze scenes immensely. Finally, the enhanced output images can be successfully post-processed for automatic autonomous reading of gauges",
    "authors": [
      "Oscar H. Ram\u00edrez-Agudelo",
      "Akshay N. Shewatkar",
      "Edoardo Milana",
      "Roland C. Aydin",
      "Kai Franke"
    ],
    "published": "2026-01-15T15:59:12+00:00",
    "updated": "2026-01-15T15:59:12+00:00",
    "category": "cs.CV",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "source": "arxiv",
    "source_priority": "high",
    "score": 7.0,
    "fetched_at": "2026-01-18T15:37:18.614534",
    "created_at": "2026-01-18 10:07:18",
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "language": null,
    "topics": null,
    "license": null,
    "languages": null,
    "contributors_count": 0,
    "owner_type": null,
    "stars_per_day": 0.0,
    "forks_per_day": 0.0,
    "activity_score": 0.0,
    "days_since_creation": 0,
    "days_since_update": 0,
    "is_recently_active": 0,
    "has_wiki": 0,
    "has_pages": 0,
    "has_discussions": 0
  }
]